{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackeygle/DD2424-Deep-Learning-in-Data-Dcience/blob/main/assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LgQKSEiNrXCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UnDm5mA9Sp3_",
        "outputId": "2cccd717-4890-4ce1-dbc5-085ea6a1460e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  ** New min smooth loss: 2.9187 at iter 4974 **\n",
            "  ** New min smooth loss: 2.9182 at iter 4975 **\n",
            "  ** New min smooth loss: 2.9174 at iter 4976 **\n",
            "  ** New min smooth loss: 2.9166 at iter 4977 **\n",
            "  ** New min smooth loss: 2.9154 at iter 4978 **\n",
            "  ** New min smooth loss: 2.9145 at iter 4979 **\n",
            "  ** New min smooth loss: 2.9144 at iter 4980 **\n",
            "  ** New min smooth loss: 2.9133 at iter 4981 **\n",
            "  ** New min smooth loss: 2.9126 at iter 4982 **\n",
            "  ** New min smooth loss: 2.9117 at iter 4983 **\n",
            "  ** New min smooth loss: 2.9111 at iter 4984 **\n",
            "  ** New min smooth loss: 2.9104 at iter 4985 **\n",
            "  ** New min smooth loss: 2.9098 at iter 4986 **\n",
            "  ** New min smooth loss: 2.9091 at iter 4987 **\n",
            "  ** New min smooth loss: 2.9080 at iter 4988 **\n",
            "  ** New min smooth loss: 2.9068 at iter 4989 **\n",
            "  ** New min smooth loss: 2.9060 at iter 4990 **\n",
            "  ** New min smooth loss: 2.9054 at iter 4991 **\n",
            "  ** New min smooth loss: 2.9045 at iter 4992 **\n",
            "  ** New min smooth loss: 2.9034 at iter 4993 **\n",
            "  ** New min smooth loss: 2.9026 at iter 4994 **\n",
            "  ** New min smooth loss: 2.9016 at iter 4995 **\n",
            "  ** New min smooth loss: 2.9010 at iter 4996 **\n",
            "  ** New min smooth loss: 2.8997 at iter 4997 **\n",
            "  ** New min smooth loss: 2.8986 at iter 4998 **\n",
            "  ** New min smooth loss: 2.8978 at iter 4999 **\n",
            "  ** New min smooth loss: 2.8970 at iter 5000 **\n",
            "Iter: 5000/100000, Smooth Loss: 2.8970, Min Smooth Loss: 2.8970 (at iter 5000), Time: 25.52s\n",
            "  ** New min smooth loss: 2.8959 at iter 5001 **\n",
            "  ** New min smooth loss: 2.8953 at iter 5002 **\n",
            "  ** New min smooth loss: 2.8951 at iter 5003 **\n",
            "  ** New min smooth loss: 2.8947 at iter 5004 **\n",
            "  ** New min smooth loss: 2.8936 at iter 5005 **\n",
            "  ** New min smooth loss: 2.8926 at iter 5006 **\n",
            "  ** New min smooth loss: 2.8916 at iter 5007 **\n",
            "  ** New min smooth loss: 2.8907 at iter 5008 **\n",
            "  ** New min smooth loss: 2.8902 at iter 5009 **\n",
            "  ** New min smooth loss: 2.8896 at iter 5010 **\n",
            "  ** New min smooth loss: 2.8887 at iter 5011 **\n",
            "  ** New min smooth loss: 2.8883 at iter 5012 **\n",
            "  ** New min smooth loss: 2.8873 at iter 5013 **\n",
            "  ** New min smooth loss: 2.8859 at iter 5014 **\n",
            "  ** New min smooth loss: 2.8849 at iter 5015 **\n",
            "  ** New min smooth loss: 2.8836 at iter 5016 **\n",
            "  ** New min smooth loss: 2.8828 at iter 5017 **\n",
            "  ** New min smooth loss: 2.8821 at iter 5019 **\n",
            "  ** New min smooth loss: 2.8817 at iter 5020 **\n",
            "  ** New min smooth loss: 2.8809 at iter 5021 **\n",
            "  ** New min smooth loss: 2.8804 at iter 5022 **\n",
            "  ** New min smooth loss: 2.8801 at iter 5023 **\n",
            "  ** New min smooth loss: 2.8790 at iter 5024 **\n",
            "  ** New min smooth loss: 2.8789 at iter 5025 **\n",
            "  ** New min smooth loss: 2.8781 at iter 5026 **\n",
            "  ** New min smooth loss: 2.8769 at iter 5027 **\n",
            "  ** New min smooth loss: 2.8763 at iter 5028 **\n",
            "  ** New min smooth loss: 2.8759 at iter 5029 **\n",
            "  ** New min smooth loss: 2.8753 at iter 5030 **\n",
            "  ** New min smooth loss: 2.8751 at iter 5032 **\n",
            "  ** New min smooth loss: 2.8740 at iter 5033 **\n",
            "  ** New min smooth loss: 2.8737 at iter 5034 **\n",
            "  ** New min smooth loss: 2.8735 at iter 5035 **\n",
            "  ** New min smooth loss: 2.8728 at iter 5036 **\n",
            "  ** New min smooth loss: 2.8717 at iter 5037 **\n",
            "  ** New min smooth loss: 2.8708 at iter 5038 **\n",
            "  ** New min smooth loss: 2.8701 at iter 5039 **\n",
            "  ** New min smooth loss: 2.8693 at iter 5040 **\n",
            "  ** New min smooth loss: 2.8687 at iter 5041 **\n",
            "  ** New min smooth loss: 2.8677 at iter 5042 **\n",
            "  ** New min smooth loss: 2.8670 at iter 5043 **\n",
            "  ** New min smooth loss: 2.8665 at iter 5044 **\n",
            "  ** New min smooth loss: 2.8654 at iter 5045 **\n",
            "  ** New min smooth loss: 2.8651 at iter 5046 **\n",
            "  ** New min smooth loss: 2.8640 at iter 5047 **\n",
            "  ** New min smooth loss: 2.8635 at iter 5048 **\n",
            "  ** New min smooth loss: 2.8632 at iter 5049 **\n",
            "  ** New min smooth loss: 2.8625 at iter 5050 **\n",
            "  ** New min smooth loss: 2.8613 at iter 5051 **\n",
            "  ** New min smooth loss: 2.8608 at iter 5052 **\n",
            "  ** New min smooth loss: 2.8598 at iter 5053 **\n",
            "  ** New min smooth loss: 2.8594 at iter 5054 **\n",
            "  ** New min smooth loss: 2.8588 at iter 5056 **\n",
            "  ** New min smooth loss: 2.8580 at iter 5057 **\n",
            "  ** New min smooth loss: 2.8577 at iter 5058 **\n",
            "  ** New min smooth loss: 2.8571 at iter 5059 **\n",
            "  ** New min smooth loss: 2.8568 at iter 5060 **\n",
            "  ** New min smooth loss: 2.8560 at iter 5061 **\n",
            "  ** New min smooth loss: 2.8549 at iter 5062 **\n",
            "  ** New min smooth loss: 2.8546 at iter 5063 **\n",
            "  ** New min smooth loss: 2.8542 at iter 5064 **\n",
            "  ** New min smooth loss: 2.8536 at iter 5065 **\n",
            "  ** New min smooth loss: 2.8531 at iter 5066 **\n",
            "  ** New min smooth loss: 2.8522 at iter 5067 **\n",
            "  ** New min smooth loss: 2.8514 at iter 5068 **\n",
            "  ** New min smooth loss: 2.8505 at iter 5069 **\n",
            "  ** New min smooth loss: 2.8501 at iter 5070 **\n",
            "  ** New min smooth loss: 2.8496 at iter 5071 **\n",
            "  ** New min smooth loss: 2.8484 at iter 5072 **\n",
            "  ** New min smooth loss: 2.8477 at iter 5073 **\n",
            "  ** New min smooth loss: 2.8469 at iter 5074 **\n",
            "  ** New min smooth loss: 2.8460 at iter 5075 **\n",
            "  ** New min smooth loss: 2.8448 at iter 5076 **\n",
            "  ** New min smooth loss: 2.8441 at iter 5077 **\n",
            "  ** New min smooth loss: 2.8439 at iter 5078 **\n",
            "  ** New min smooth loss: 2.8434 at iter 5079 **\n",
            "  ** New min smooth loss: 2.8424 at iter 5080 **\n",
            "  ** New min smooth loss: 2.8417 at iter 5081 **\n",
            "  ** New min smooth loss: 2.8408 at iter 5082 **\n",
            "  ** New min smooth loss: 2.8399 at iter 5083 **\n",
            "  ** New min smooth loss: 2.8386 at iter 5084 **\n",
            "  ** New min smooth loss: 2.8385 at iter 5085 **\n",
            "  ** New min smooth loss: 2.8374 at iter 5086 **\n",
            "  ** New min smooth loss: 2.8364 at iter 5087 **\n",
            "  ** New min smooth loss: 2.8354 at iter 5088 **\n",
            "  ** New min smooth loss: 2.8349 at iter 5089 **\n",
            "  ** New min smooth loss: 2.8338 at iter 5090 **\n",
            "  ** New min smooth loss: 2.8325 at iter 5091 **\n",
            "  ** New min smooth loss: 2.8317 at iter 5092 **\n",
            "  ** New min smooth loss: 2.8306 at iter 5093 **\n",
            "  ** New min smooth loss: 2.8298 at iter 5094 **\n",
            "  ** New min smooth loss: 2.8294 at iter 5095 **\n",
            "  ** New min smooth loss: 2.8281 at iter 5096 **\n",
            "  ** New min smooth loss: 2.8269 at iter 5097 **\n",
            "  ** New min smooth loss: 2.8256 at iter 5098 **\n",
            "  ** New min smooth loss: 2.8250 at iter 5099 **\n",
            "  ** New min smooth loss: 2.8242 at iter 5100 **\n",
            "Iter: 5100/100000, Smooth Loss: 2.8242, Min Smooth Loss: 2.8242 (at iter 5100), Time: 26.06s\n",
            "  ** New min smooth loss: 2.8236 at iter 5101 **\n",
            "  ** New min smooth loss: 2.8228 at iter 5102 **\n",
            "  ** New min smooth loss: 2.8215 at iter 5103 **\n",
            "  ** New min smooth loss: 2.8207 at iter 5104 **\n",
            "  ** New min smooth loss: 2.8195 at iter 5105 **\n",
            "  ** New min smooth loss: 2.8189 at iter 5106 **\n",
            "  ** New min smooth loss: 2.8183 at iter 5107 **\n",
            "  ** New min smooth loss: 2.8172 at iter 5108 **\n",
            "  ** New min smooth loss: 2.8162 at iter 5109 **\n",
            "  ** New min smooth loss: 2.8155 at iter 5110 **\n",
            "  ** New min smooth loss: 2.8145 at iter 5111 **\n",
            "  ** New min smooth loss: 2.8136 at iter 5112 **\n",
            "  ** New min smooth loss: 2.8127 at iter 5113 **\n",
            "  ** New min smooth loss: 2.8117 at iter 5114 **\n",
            "  ** New min smooth loss: 2.8112 at iter 5115 **\n",
            "  ** New min smooth loss: 2.8105 at iter 5116 **\n",
            "  ** New min smooth loss: 2.8097 at iter 5117 **\n",
            "  ** New min smooth loss: 2.8094 at iter 5118 **\n",
            "  ** New min smooth loss: 2.8087 at iter 5119 **\n",
            "  ** New min smooth loss: 2.8076 at iter 5120 **\n",
            "  ** New min smooth loss: 2.8065 at iter 5121 **\n",
            "  ** New min smooth loss: 2.8062 at iter 5122 **\n",
            "  ** New min smooth loss: 2.8051 at iter 5123 **\n",
            "  ** New min smooth loss: 2.8044 at iter 5124 **\n",
            "  ** New min smooth loss: 2.8036 at iter 5126 **\n",
            "  ** New min smooth loss: 2.8029 at iter 5127 **\n",
            "  ** New min smooth loss: 2.8026 at iter 5128 **\n",
            "  ** New min smooth loss: 2.8024 at iter 5129 **\n",
            "  ** New min smooth loss: 2.8023 at iter 5130 **\n",
            "  ** New min smooth loss: 2.8020 at iter 5131 **\n",
            "  ** New min smooth loss: 2.8015 at iter 5132 **\n",
            "  ** New min smooth loss: 2.8012 at iter 5133 **\n",
            "  ** New min smooth loss: 2.8008 at iter 5134 **\n",
            "  ** New min smooth loss: 2.8006 at iter 5135 **\n",
            "  ** New min smooth loss: 2.8003 at iter 5137 **\n",
            "  ** New min smooth loss: 2.8000 at iter 5139 **\n",
            "  ** New min smooth loss: 2.7995 at iter 5140 **\n",
            "  ** New min smooth loss: 2.7994 at iter 5141 **\n",
            "  ** New min smooth loss: 2.7986 at iter 5142 **\n",
            "  ** New min smooth loss: 2.7983 at iter 5143 **\n",
            "  ** New min smooth loss: 2.7981 at iter 5144 **\n",
            "  ** New min smooth loss: 2.7974 at iter 5145 **\n",
            "  ** New min smooth loss: 2.7966 at iter 5146 **\n",
            "  ** New min smooth loss: 2.7954 at iter 5147 **\n",
            "  ** New min smooth loss: 2.7950 at iter 5148 **\n",
            "  ** New min smooth loss: 2.7945 at iter 5149 **\n",
            "  ** New min smooth loss: 2.7937 at iter 5150 **\n",
            "  ** New min smooth loss: 2.7926 at iter 5151 **\n",
            "  ** New min smooth loss: 2.7922 at iter 5152 **\n",
            "  ** New min smooth loss: 2.7913 at iter 5153 **\n",
            "  ** New min smooth loss: 2.7908 at iter 5154 **\n",
            "  ** New min smooth loss: 2.7901 at iter 5155 **\n",
            "  ** New min smooth loss: 2.7900 at iter 5156 **\n",
            "  ** New min smooth loss: 2.7889 at iter 5157 **\n",
            "  ** New min smooth loss: 2.7880 at iter 5158 **\n",
            "  ** New min smooth loss: 2.7871 at iter 5159 **\n",
            "  ** New min smooth loss: 2.7860 at iter 5160 **\n",
            "  ** New min smooth loss: 2.7853 at iter 5161 **\n",
            "  ** New min smooth loss: 2.7849 at iter 5162 **\n",
            "  ** New min smooth loss: 2.7844 at iter 5164 **\n",
            "  ** New min smooth loss: 2.7836 at iter 5165 **\n",
            "  ** New min smooth loss: 2.7832 at iter 5166 **\n",
            "  ** New min smooth loss: 2.7828 at iter 5167 **\n",
            "  ** New min smooth loss: 2.7818 at iter 5168 **\n",
            "  ** New min smooth loss: 2.7814 at iter 5169 **\n",
            "  ** New min smooth loss: 2.7812 at iter 5170 **\n",
            "  ** New min smooth loss: 2.7806 at iter 5171 **\n",
            "  ** New min smooth loss: 2.7803 at iter 5172 **\n",
            "  ** New min smooth loss: 2.7797 at iter 5173 **\n",
            "  ** New min smooth loss: 2.7786 at iter 5174 **\n",
            "  ** New min smooth loss: 2.7784 at iter 5175 **\n",
            "  ** New min smooth loss: 2.7780 at iter 5176 **\n",
            "  ** New min smooth loss: 2.7770 at iter 5177 **\n",
            "  ** New min smooth loss: 2.7766 at iter 5178 **\n",
            "  ** New min smooth loss: 2.7757 at iter 5179 **\n",
            "  ** New min smooth loss: 2.7747 at iter 5180 **\n",
            "  ** New min smooth loss: 2.7744 at iter 5181 **\n",
            "  ** New min smooth loss: 2.7737 at iter 5182 **\n",
            "  ** New min smooth loss: 2.7735 at iter 5183 **\n",
            "  ** New min smooth loss: 2.7730 at iter 5184 **\n",
            "  ** New min smooth loss: 2.7727 at iter 5185 **\n",
            "  ** New min smooth loss: 2.7726 at iter 5186 **\n",
            "  ** New min smooth loss: 2.7722 at iter 5187 **\n",
            "  ** New min smooth loss: 2.7715 at iter 5188 **\n",
            "  ** New min smooth loss: 2.7715 at iter 5189 **\n",
            "  ** New min smooth loss: 2.7712 at iter 5190 **\n",
            "  ** New min smooth loss: 2.7700 at iter 5191 **\n",
            "  ** New min smooth loss: 2.7691 at iter 5192 **\n",
            "  ** New min smooth loss: 2.7679 at iter 5193 **\n",
            "  ** New min smooth loss: 2.7671 at iter 5194 **\n",
            "  ** New min smooth loss: 2.7667 at iter 5195 **\n",
            "  ** New min smooth loss: 2.7663 at iter 5196 **\n",
            "  ** New min smooth loss: 2.7656 at iter 5197 **\n",
            "  ** New min smooth loss: 2.7655 at iter 5198 **\n",
            "  ** New min smooth loss: 2.7650 at iter 5199 **\n",
            "  ** New min smooth loss: 2.7648 at iter 5200 **\n",
            "Iter: 5200/100000, Smooth Loss: 2.7648, Min Smooth Loss: 2.7648 (at iter 5200), Time: 26.51s\n",
            "  ** New min smooth loss: 2.7647 at iter 5201 **\n",
            "  ** New min smooth loss: 2.7639 at iter 5202 **\n",
            "  ** New min smooth loss: 2.7630 at iter 5203 **\n",
            "  ** New min smooth loss: 2.7626 at iter 5204 **\n",
            "  ** New min smooth loss: 2.7621 at iter 5205 **\n",
            "  ** New min smooth loss: 2.7618 at iter 5206 **\n",
            "  ** New min smooth loss: 2.7610 at iter 5207 **\n",
            "  ** New min smooth loss: 2.7602 at iter 5208 **\n",
            "  ** New min smooth loss: 2.7597 at iter 5209 **\n",
            "  ** New min smooth loss: 2.7589 at iter 5210 **\n",
            "  ** New min smooth loss: 2.7586 at iter 5211 **\n",
            "  ** New min smooth loss: 2.7582 at iter 5212 **\n",
            "  ** New min smooth loss: 2.7576 at iter 5213 **\n",
            "  ** New min smooth loss: 2.7567 at iter 5214 **\n",
            "  ** New min smooth loss: 2.7563 at iter 5215 **\n",
            "  ** New min smooth loss: 2.7560 at iter 5216 **\n",
            "  ** New min smooth loss: 2.7558 at iter 5217 **\n",
            "  ** New min smooth loss: 2.7552 at iter 5218 **\n",
            "  ** New min smooth loss: 2.7546 at iter 5219 **\n",
            "  ** New min smooth loss: 2.7538 at iter 5220 **\n",
            "  ** New min smooth loss: 2.7527 at iter 5221 **\n",
            "  ** New min smooth loss: 2.7522 at iter 5222 **\n",
            "  ** New min smooth loss: 2.7511 at iter 5223 **\n",
            "  ** New min smooth loss: 2.7505 at iter 5224 **\n",
            "  ** New min smooth loss: 2.7502 at iter 5225 **\n",
            "  ** New min smooth loss: 2.7498 at iter 5226 **\n",
            "  ** New min smooth loss: 2.7496 at iter 5227 **\n",
            "  ** New min smooth loss: 2.7487 at iter 5228 **\n",
            "  ** New min smooth loss: 2.7479 at iter 5229 **\n",
            "  ** New min smooth loss: 2.7476 at iter 5230 **\n",
            "  ** New min smooth loss: 2.7470 at iter 5231 **\n",
            "  ** New min smooth loss: 2.7460 at iter 5232 **\n",
            "  ** New min smooth loss: 2.7454 at iter 5233 **\n",
            "  ** New min smooth loss: 2.7452 at iter 5234 **\n",
            "  ** New min smooth loss: 2.7444 at iter 5235 **\n",
            "  ** New min smooth loss: 2.7435 at iter 5236 **\n",
            "  ** New min smooth loss: 2.7433 at iter 5237 **\n",
            "  ** New min smooth loss: 2.7430 at iter 5238 **\n",
            "  ** New min smooth loss: 2.7426 at iter 5239 **\n",
            "  ** New min smooth loss: 2.7422 at iter 5240 **\n",
            "  ** New min smooth loss: 2.7417 at iter 5241 **\n",
            "  ** New min smooth loss: 2.7409 at iter 5242 **\n",
            "  ** New min smooth loss: 2.7401 at iter 5243 **\n",
            "  ** New min smooth loss: 2.7395 at iter 5244 **\n",
            "  ** New min smooth loss: 2.7390 at iter 5245 **\n",
            "  ** New min smooth loss: 2.7384 at iter 5246 **\n",
            "  ** New min smooth loss: 2.7377 at iter 5247 **\n",
            "  ** New min smooth loss: 2.7368 at iter 5248 **\n",
            "  ** New min smooth loss: 2.7368 at iter 5250 **\n",
            "  ** New min smooth loss: 2.7366 at iter 5251 **\n",
            "  ** New min smooth loss: 2.7364 at iter 5252 **\n",
            "  ** New min smooth loss: 2.7361 at iter 5253 **\n",
            "  ** New min smooth loss: 2.7359 at iter 5254 **\n",
            "  ** New min smooth loss: 2.7353 at iter 5255 **\n",
            "  ** New min smooth loss: 2.7347 at iter 5256 **\n",
            "  ** New min smooth loss: 2.7344 at iter 5258 **\n",
            "  ** New min smooth loss: 2.7342 at iter 5259 **\n",
            "  ** New min smooth loss: 2.7339 at iter 5260 **\n",
            "  ** New min smooth loss: 2.7323 at iter 5261 **\n",
            "  ** New min smooth loss: 2.7320 at iter 5263 **\n",
            "  ** New min smooth loss: 2.7316 at iter 5264 **\n",
            "  ** New min smooth loss: 2.7313 at iter 5265 **\n",
            "  ** New min smooth loss: 2.7309 at iter 5266 **\n",
            "  ** New min smooth loss: 2.7305 at iter 5267 **\n",
            "  ** New min smooth loss: 2.7300 at iter 5268 **\n",
            "  ** New min smooth loss: 2.7294 at iter 5269 **\n",
            "  ** New min smooth loss: 2.7278 at iter 5270 **\n",
            "  ** New min smooth loss: 2.7271 at iter 5271 **\n",
            "  ** New min smooth loss: 2.7271 at iter 5272 **\n",
            "  ** New min smooth loss: 2.7270 at iter 5273 **\n",
            "  ** New min smooth loss: 2.7263 at iter 5274 **\n",
            "  ** New min smooth loss: 2.7257 at iter 5275 **\n",
            "  ** New min smooth loss: 2.7251 at iter 5276 **\n",
            "  ** New min smooth loss: 2.7250 at iter 5277 **\n",
            "  ** New min smooth loss: 2.7249 at iter 5278 **\n",
            "  ** New min smooth loss: 2.7244 at iter 5279 **\n",
            "  ** New min smooth loss: 2.7235 at iter 5280 **\n",
            "  ** New min smooth loss: 2.7229 at iter 5281 **\n",
            "  ** New min smooth loss: 2.7229 at iter 5282 **\n",
            "  ** New min smooth loss: 2.7223 at iter 5283 **\n",
            "  ** New min smooth loss: 2.7212 at iter 5284 **\n",
            "  ** New min smooth loss: 2.7210 at iter 5285 **\n",
            "  ** New min smooth loss: 2.7208 at iter 5286 **\n",
            "  ** New min smooth loss: 2.7203 at iter 5287 **\n",
            "  ** New min smooth loss: 2.7196 at iter 5288 **\n",
            "  ** New min smooth loss: 2.7185 at iter 5289 **\n",
            "  ** New min smooth loss: 2.7181 at iter 5290 **\n",
            "  ** New min smooth loss: 2.7179 at iter 5291 **\n",
            "  ** New min smooth loss: 2.7174 at iter 5292 **\n",
            "  ** New min smooth loss: 2.7170 at iter 5293 **\n",
            "  ** New min smooth loss: 2.7163 at iter 5294 **\n",
            "  ** New min smooth loss: 2.7160 at iter 5295 **\n",
            "  ** New min smooth loss: 2.7154 at iter 5296 **\n",
            "  ** New min smooth loss: 2.7152 at iter 5297 **\n",
            "  ** New min smooth loss: 2.7139 at iter 5299 **\n",
            "  ** New min smooth loss: 2.7130 at iter 5300 **\n",
            "Iter: 5300/100000, Smooth Loss: 2.7130, Min Smooth Loss: 2.7130 (at iter 5300), Time: 26.96s\n",
            "  ** New min smooth loss: 2.7121 at iter 5301 **\n",
            "  ** New min smooth loss: 2.7115 at iter 5302 **\n",
            "  ** New min smooth loss: 2.7111 at iter 5303 **\n",
            "  ** New min smooth loss: 2.7108 at iter 5305 **\n",
            "  ** New min smooth loss: 2.7100 at iter 5306 **\n",
            "  ** New min smooth loss: 2.7094 at iter 5307 **\n",
            "  ** New min smooth loss: 2.7086 at iter 5308 **\n",
            "  ** New min smooth loss: 2.7081 at iter 5309 **\n",
            "  ** New min smooth loss: 2.7079 at iter 5310 **\n",
            "  ** New min smooth loss: 2.7074 at iter 5311 **\n",
            "  ** New min smooth loss: 2.7068 at iter 5312 **\n",
            "  ** New min smooth loss: 2.7063 at iter 5313 **\n",
            "  ** New min smooth loss: 2.7056 at iter 5314 **\n",
            "  ** New min smooth loss: 2.7049 at iter 5315 **\n",
            "  ** New min smooth loss: 2.7040 at iter 5316 **\n",
            "  ** New min smooth loss: 2.7029 at iter 5317 **\n",
            "  ** New min smooth loss: 2.7027 at iter 5318 **\n",
            "  ** New min smooth loss: 2.7023 at iter 5319 **\n",
            "  ** New min smooth loss: 2.7016 at iter 5320 **\n",
            "  ** New min smooth loss: 2.7014 at iter 5321 **\n",
            "  ** New min smooth loss: 2.7004 at iter 5322 **\n",
            "  ** New min smooth loss: 2.7000 at iter 5323 **\n",
            "  ** New min smooth loss: 2.6991 at iter 5324 **\n",
            "  ** New min smooth loss: 2.6982 at iter 5325 **\n",
            "  ** New min smooth loss: 2.6976 at iter 5326 **\n",
            "  ** New min smooth loss: 2.6971 at iter 5327 **\n",
            "  ** New min smooth loss: 2.6957 at iter 5328 **\n",
            "  ** New min smooth loss: 2.6956 at iter 5330 **\n",
            "  ** New min smooth loss: 2.6955 at iter 5331 **\n",
            "  ** New min smooth loss: 2.6952 at iter 5333 **\n",
            "  ** New min smooth loss: 2.6946 at iter 5334 **\n",
            "  ** New min smooth loss: 2.6943 at iter 5335 **\n",
            "  ** New min smooth loss: 2.6935 at iter 5336 **\n",
            "  ** New min smooth loss: 2.6928 at iter 5337 **\n",
            "  ** New min smooth loss: 2.6927 at iter 5338 **\n",
            "  ** New min smooth loss: 2.6919 at iter 5339 **\n",
            "  ** New min smooth loss: 2.6911 at iter 5340 **\n",
            "  ** New min smooth loss: 2.6910 at iter 5342 **\n",
            "  ** New min smooth loss: 2.6902 at iter 5343 **\n",
            "  ** New min smooth loss: 2.6901 at iter 5344 **\n",
            "  ** New min smooth loss: 2.6898 at iter 5345 **\n",
            "  ** New min smooth loss: 2.6897 at iter 5346 **\n",
            "  ** New min smooth loss: 2.6894 at iter 5347 **\n",
            "  ** New min smooth loss: 2.6888 at iter 5348 **\n",
            "  ** New min smooth loss: 2.6882 at iter 5349 **\n",
            "  ** New min smooth loss: 2.6877 at iter 5350 **\n",
            "  ** New min smooth loss: 2.6869 at iter 5351 **\n",
            "  ** New min smooth loss: 2.6865 at iter 5352 **\n",
            "  ** New min smooth loss: 2.6856 at iter 5353 **\n",
            "  ** New min smooth loss: 2.6850 at iter 5354 **\n",
            "  ** New min smooth loss: 2.6841 at iter 5356 **\n",
            "  ** New min smooth loss: 2.6839 at iter 5357 **\n",
            "  ** New min smooth loss: 2.6833 at iter 5358 **\n",
            "  ** New min smooth loss: 2.6826 at iter 5359 **\n",
            "  ** New min smooth loss: 2.6820 at iter 5360 **\n",
            "  ** New min smooth loss: 2.6816 at iter 5361 **\n",
            "  ** New min smooth loss: 2.6814 at iter 5362 **\n",
            "  ** New min smooth loss: 2.6813 at iter 5363 **\n",
            "  ** New min smooth loss: 2.6810 at iter 5364 **\n",
            "  ** New min smooth loss: 2.6806 at iter 5365 **\n",
            "  ** New min smooth loss: 2.6800 at iter 5366 **\n",
            "  ** New min smooth loss: 2.6793 at iter 5367 **\n",
            "  ** New min smooth loss: 2.6792 at iter 5368 **\n",
            "  ** New min smooth loss: 2.6786 at iter 5369 **\n",
            "  ** New min smooth loss: 2.6778 at iter 5370 **\n",
            "  ** New min smooth loss: 2.6771 at iter 5371 **\n",
            "  ** New min smooth loss: 2.6758 at iter 5372 **\n",
            "  ** New min smooth loss: 2.6754 at iter 5373 **\n",
            "  ** New min smooth loss: 2.6745 at iter 5374 **\n",
            "  ** New min smooth loss: 2.6736 at iter 5376 **\n",
            "  ** New min smooth loss: 2.6733 at iter 5377 **\n",
            "  ** New min smooth loss: 2.6732 at iter 5378 **\n",
            "  ** New min smooth loss: 2.6726 at iter 5379 **\n",
            "  ** New min smooth loss: 2.6720 at iter 5380 **\n",
            "  ** New min smooth loss: 2.6716 at iter 5381 **\n",
            "  ** New min smooth loss: 2.6709 at iter 5382 **\n",
            "  ** New min smooth loss: 2.6701 at iter 5383 **\n",
            "  ** New min smooth loss: 2.6695 at iter 5384 **\n",
            "  ** New min smooth loss: 2.6693 at iter 5385 **\n",
            "  ** New min smooth loss: 2.6686 at iter 5386 **\n",
            "  ** New min smooth loss: 2.6684 at iter 5387 **\n",
            "  ** New min smooth loss: 2.6676 at iter 5388 **\n",
            "  ** New min smooth loss: 2.6674 at iter 5390 **\n",
            "  ** New min smooth loss: 2.6668 at iter 5391 **\n",
            "  ** New min smooth loss: 2.6661 at iter 5392 **\n",
            "  ** New min smooth loss: 2.6655 at iter 5394 **\n",
            "  ** New min smooth loss: 2.6648 at iter 5396 **\n",
            "  ** New min smooth loss: 2.6645 at iter 5397 **\n",
            "  ** New min smooth loss: 2.6640 at iter 5398 **\n",
            "  ** New min smooth loss: 2.6632 at iter 5399 **\n",
            "  ** New min smooth loss: 2.6626 at iter 5400 **\n",
            "Iter: 5400/100000, Smooth Loss: 2.6626, Min Smooth Loss: 2.6626 (at iter 5400), Time: 27.42s\n",
            "  ** New min smooth loss: 2.6621 at iter 5401 **\n",
            "  ** New min smooth loss: 2.6619 at iter 5402 **\n",
            "  ** New min smooth loss: 2.6612 at iter 5403 **\n",
            "  ** New min smooth loss: 2.6603 at iter 5404 **\n",
            "  ** New min smooth loss: 2.6597 at iter 5405 **\n",
            "  ** New min smooth loss: 2.6589 at iter 5406 **\n",
            "  ** New min smooth loss: 2.6580 at iter 5407 **\n",
            "  ** New min smooth loss: 2.6575 at iter 5408 **\n",
            "  ** New min smooth loss: 2.6572 at iter 5409 **\n",
            "  ** New min smooth loss: 2.6565 at iter 5410 **\n",
            "  ** New min smooth loss: 2.6557 at iter 5411 **\n",
            "  ** New min smooth loss: 2.6556 at iter 5412 **\n",
            "  ** New min smooth loss: 2.6548 at iter 5413 **\n",
            "  ** New min smooth loss: 2.6539 at iter 5414 **\n",
            "  ** New min smooth loss: 2.6533 at iter 5415 **\n",
            "  ** New min smooth loss: 2.6522 at iter 5416 **\n",
            "  ** New min smooth loss: 2.6518 at iter 5417 **\n",
            "  ** New min smooth loss: 2.6513 at iter 5418 **\n",
            "  ** New min smooth loss: 2.6506 at iter 5419 **\n",
            "  ** New min smooth loss: 2.6503 at iter 5420 **\n",
            "  ** New min smooth loss: 2.6498 at iter 5421 **\n",
            "  ** New min smooth loss: 2.6496 at iter 5422 **\n",
            "  ** New min smooth loss: 2.6490 at iter 5423 **\n",
            "  ** New min smooth loss: 2.6484 at iter 5424 **\n",
            "  ** New min smooth loss: 2.6474 at iter 5425 **\n",
            "  ** New min smooth loss: 2.6469 at iter 5426 **\n",
            "  ** New min smooth loss: 2.6465 at iter 5427 **\n",
            "  ** New min smooth loss: 2.6460 at iter 5428 **\n",
            "  ** New min smooth loss: 2.6454 at iter 5429 **\n",
            "  ** New min smooth loss: 2.6448 at iter 5430 **\n",
            "  ** New min smooth loss: 2.6444 at iter 5431 **\n",
            "  ** New min smooth loss: 2.6441 at iter 5432 **\n",
            "  ** New min smooth loss: 2.6435 at iter 5433 **\n",
            "  ** New min smooth loss: 2.6424 at iter 5434 **\n",
            "  ** New min smooth loss: 2.6413 at iter 5435 **\n",
            "  ** New min smooth loss: 2.6411 at iter 5436 **\n",
            "  ** New min smooth loss: 2.6408 at iter 5437 **\n",
            "  ** New min smooth loss: 2.6399 at iter 5438 **\n",
            "  ** New min smooth loss: 2.6395 at iter 5439 **\n",
            "  ** New min smooth loss: 2.6390 at iter 5440 **\n",
            "  ** New min smooth loss: 2.6384 at iter 5441 **\n",
            "  ** New min smooth loss: 2.6376 at iter 5442 **\n",
            "  ** New min smooth loss: 2.6369 at iter 5443 **\n",
            "  ** New min smooth loss: 2.6357 at iter 5444 **\n",
            "  ** New min smooth loss: 2.6354 at iter 5445 **\n",
            "  ** New min smooth loss: 2.6345 at iter 5446 **\n",
            "  ** New min smooth loss: 2.6339 at iter 5447 **\n",
            "  ** New min smooth loss: 2.6331 at iter 5448 **\n",
            "  ** New min smooth loss: 2.6322 at iter 5450 **\n",
            "  ** New min smooth loss: 2.6317 at iter 5451 **\n",
            "  ** New min smooth loss: 2.6310 at iter 5452 **\n",
            "  ** New min smooth loss: 2.6305 at iter 5453 **\n",
            "  ** New min smooth loss: 2.6299 at iter 5454 **\n",
            "  ** New min smooth loss: 2.6298 at iter 5455 **\n",
            "  ** New min smooth loss: 2.6296 at iter 5456 **\n",
            "  ** New min smooth loss: 2.6294 at iter 5458 **\n",
            "  ** New min smooth loss: 2.6294 at iter 5459 **\n",
            "  ** New min smooth loss: 2.6286 at iter 5460 **\n",
            "  ** New min smooth loss: 2.6276 at iter 5461 **\n",
            "  ** New min smooth loss: 2.6269 at iter 5462 **\n",
            "  ** New min smooth loss: 2.6265 at iter 5463 **\n",
            "  ** New min smooth loss: 2.6260 at iter 5464 **\n",
            "  ** New min smooth loss: 2.6258 at iter 5465 **\n",
            "  ** New min smooth loss: 2.6250 at iter 5466 **\n",
            "  ** New min smooth loss: 2.6246 at iter 5467 **\n",
            "  ** New min smooth loss: 2.6237 at iter 5468 **\n",
            "  ** New min smooth loss: 2.6233 at iter 5469 **\n",
            "  ** New min smooth loss: 2.6230 at iter 5470 **\n",
            "  ** New min smooth loss: 2.6229 at iter 5471 **\n",
            "  ** New min smooth loss: 2.6223 at iter 5472 **\n",
            "  ** New min smooth loss: 2.6216 at iter 5473 **\n",
            "  ** New min smooth loss: 2.6211 at iter 5474 **\n",
            "  ** New min smooth loss: 2.6203 at iter 5475 **\n",
            "  ** New min smooth loss: 2.6200 at iter 5476 **\n",
            "  ** New min smooth loss: 2.6193 at iter 5477 **\n",
            "  ** New min smooth loss: 2.6190 at iter 5478 **\n",
            "  ** New min smooth loss: 2.6186 at iter 5479 **\n",
            "  ** New min smooth loss: 2.6185 at iter 5480 **\n",
            "  ** New min smooth loss: 2.6181 at iter 5481 **\n",
            "  ** New min smooth loss: 2.6170 at iter 5482 **\n",
            "  ** New min smooth loss: 2.6166 at iter 5484 **\n",
            "  ** New min smooth loss: 2.6164 at iter 5485 **\n",
            "  ** New min smooth loss: 2.6161 at iter 5486 **\n",
            "  ** New min smooth loss: 2.6155 at iter 5487 **\n",
            "  ** New min smooth loss: 2.6145 at iter 5488 **\n",
            "  ** New min smooth loss: 2.6138 at iter 5489 **\n",
            "  ** New min smooth loss: 2.6136 at iter 5490 **\n",
            "  ** New min smooth loss: 2.6135 at iter 5491 **\n",
            "  ** New min smooth loss: 2.6128 at iter 5492 **\n",
            "  ** New min smooth loss: 2.6122 at iter 5493 **\n",
            "  ** New min smooth loss: 2.6121 at iter 5494 **\n",
            "  ** New min smooth loss: 2.6118 at iter 5496 **\n",
            "  ** New min smooth loss: 2.6114 at iter 5497 **\n",
            "  ** New min smooth loss: 2.6109 at iter 5498 **\n",
            "  ** New min smooth loss: 2.6108 at iter 5499 **\n",
            "  ** New min smooth loss: 2.6107 at iter 5500 **\n",
            "Iter: 5500/100000, Smooth Loss: 2.6107, Min Smooth Loss: 2.6107 (at iter 5500), Time: 27.86s\n",
            "  ** New min smooth loss: 2.6098 at iter 5501 **\n",
            "  ** New min smooth loss: 2.6093 at iter 5502 **\n",
            "  ** New min smooth loss: 2.6087 at iter 5503 **\n",
            "  ** New min smooth loss: 2.6080 at iter 5504 **\n",
            "  ** New min smooth loss: 2.6073 at iter 5505 **\n",
            "  ** New min smooth loss: 2.6065 at iter 5506 **\n",
            "  ** New min smooth loss: 2.6063 at iter 5507 **\n",
            "  ** New min smooth loss: 2.6058 at iter 5508 **\n",
            "  ** New min smooth loss: 2.6056 at iter 5509 **\n",
            "  ** New min smooth loss: 2.6052 at iter 5510 **\n",
            "  ** New min smooth loss: 2.6049 at iter 5511 **\n",
            "  ** New min smooth loss: 2.6035 at iter 5512 **\n",
            "  ** New min smooth loss: 2.6033 at iter 5513 **\n",
            "  ** New min smooth loss: 2.6026 at iter 5514 **\n",
            "  ** New min smooth loss: 2.6024 at iter 5515 **\n",
            "  ** New min smooth loss: 2.6021 at iter 5516 **\n",
            "  ** New min smooth loss: 2.6014 at iter 5517 **\n",
            "  ** New min smooth loss: 2.6011 at iter 5518 **\n",
            "  ** New min smooth loss: 2.6007 at iter 5519 **\n",
            "  ** New min smooth loss: 2.5998 at iter 5520 **\n",
            "  ** New min smooth loss: 2.5991 at iter 5521 **\n",
            "  ** New min smooth loss: 2.5986 at iter 5522 **\n",
            "  ** New min smooth loss: 2.5981 at iter 5523 **\n",
            "  ** New min smooth loss: 2.5975 at iter 5524 **\n",
            "  ** New min smooth loss: 2.5973 at iter 5525 **\n",
            "  ** New min smooth loss: 2.5968 at iter 5526 **\n",
            "  ** New min smooth loss: 2.5964 at iter 5527 **\n",
            "  ** New min smooth loss: 2.5962 at iter 5528 **\n",
            "  ** New min smooth loss: 2.5954 at iter 5529 **\n",
            "  ** New min smooth loss: 2.5950 at iter 5530 **\n",
            "  ** New min smooth loss: 2.5943 at iter 5531 **\n",
            "  ** New min smooth loss: 2.5935 at iter 5532 **\n",
            "  ** New min smooth loss: 2.5929 at iter 5533 **\n",
            "  ** New min smooth loss: 2.5920 at iter 5534 **\n",
            "  ** New min smooth loss: 2.5919 at iter 5535 **\n",
            "  ** New min smooth loss: 2.5918 at iter 5536 **\n",
            "  ** New min smooth loss: 2.5916 at iter 5537 **\n",
            "  ** New min smooth loss: 2.5909 at iter 5538 **\n",
            "  ** New min smooth loss: 2.5901 at iter 5539 **\n",
            "  ** New min smooth loss: 2.5897 at iter 5540 **\n",
            "  ** New min smooth loss: 2.5895 at iter 5541 **\n",
            "  ** New min smooth loss: 2.5889 at iter 5542 **\n",
            "  ** New min smooth loss: 2.5884 at iter 5543 **\n",
            "  ** New min smooth loss: 2.5880 at iter 5544 **\n",
            "  ** New min smooth loss: 2.5875 at iter 5545 **\n",
            "  ** New min smooth loss: 2.5870 at iter 5546 **\n",
            "  ** New min smooth loss: 2.5869 at iter 5547 **\n",
            "  ** New min smooth loss: 2.5861 at iter 5548 **\n",
            "  ** New min smooth loss: 2.5856 at iter 5549 **\n",
            "  ** New min smooth loss: 2.5855 at iter 5550 **\n",
            "  ** New min smooth loss: 2.5847 at iter 5551 **\n",
            "  ** New min smooth loss: 2.5839 at iter 5552 **\n",
            "  ** New min smooth loss: 2.5836 at iter 5553 **\n",
            "  ** New min smooth loss: 2.5831 at iter 5554 **\n",
            "  ** New min smooth loss: 2.5823 at iter 5555 **\n",
            "  ** New min smooth loss: 2.5818 at iter 5556 **\n",
            "  ** New min smooth loss: 2.5812 at iter 5557 **\n",
            "  ** New min smooth loss: 2.5810 at iter 5558 **\n",
            "  ** New min smooth loss: 2.5808 at iter 5560 **\n",
            "  ** New min smooth loss: 2.5805 at iter 5561 **\n",
            "  ** New min smooth loss: 2.5800 at iter 5562 **\n",
            "  ** New min smooth loss: 2.5799 at iter 5563 **\n",
            "  ** New min smooth loss: 2.5794 at iter 5564 **\n",
            "  ** New min smooth loss: 2.5793 at iter 5565 **\n",
            "  ** New min smooth loss: 2.5792 at iter 5566 **\n",
            "  ** New min smooth loss: 2.5786 at iter 5567 **\n",
            "  ** New min smooth loss: 2.5778 at iter 5568 **\n",
            "  ** New min smooth loss: 2.5774 at iter 5569 **\n",
            "  ** New min smooth loss: 2.5774 at iter 5570 **\n",
            "  ** New min smooth loss: 2.5771 at iter 5571 **\n",
            "  ** New min smooth loss: 2.5766 at iter 5572 **\n",
            "  ** New min smooth loss: 2.5765 at iter 5573 **\n",
            "  ** New min smooth loss: 2.5761 at iter 5574 **\n",
            "  ** New min smooth loss: 2.5755 at iter 5575 **\n",
            "  ** New min smooth loss: 2.5749 at iter 5576 **\n",
            "  ** New min smooth loss: 2.5746 at iter 5578 **\n",
            "  ** New min smooth loss: 2.5744 at iter 5579 **\n",
            "  ** New min smooth loss: 2.5740 at iter 5580 **\n",
            "  ** New min smooth loss: 2.5734 at iter 5581 **\n",
            "  ** New min smooth loss: 2.5729 at iter 5582 **\n",
            "  ** New min smooth loss: 2.5727 at iter 5583 **\n",
            "  ** New min smooth loss: 2.5722 at iter 5584 **\n",
            "  ** New min smooth loss: 2.5714 at iter 5585 **\n",
            "  ** New min smooth loss: 2.5708 at iter 5586 **\n",
            "  ** New min smooth loss: 2.5702 at iter 5587 **\n",
            "  ** New min smooth loss: 2.5698 at iter 5588 **\n",
            "  ** New min smooth loss: 2.5693 at iter 5589 **\n",
            "  ** New min smooth loss: 2.5689 at iter 5590 **\n",
            "  ** New min smooth loss: 2.5687 at iter 5591 **\n",
            "  ** New min smooth loss: 2.5685 at iter 5593 **\n",
            "  ** New min smooth loss: 2.5683 at iter 5595 **\n",
            "  ** New min smooth loss: 2.5677 at iter 5596 **\n",
            "  ** New min smooth loss: 2.5672 at iter 5597 **\n",
            "  ** New min smooth loss: 2.5669 at iter 5598 **\n",
            "  ** New min smooth loss: 2.5658 at iter 5599 **\n",
            "  ** New min smooth loss: 2.5654 at iter 5600 **\n",
            "Iter: 5600/100000, Smooth Loss: 2.5654, Min Smooth Loss: 2.5654 (at iter 5600), Time: 28.28s\n",
            "  ** New min smooth loss: 2.5646 at iter 5601 **\n",
            "  ** New min smooth loss: 2.5636 at iter 5603 **\n",
            "  ** New min smooth loss: 2.5631 at iter 5606 **\n",
            "  ** New min smooth loss: 2.5627 at iter 5607 **\n",
            "  ** New min smooth loss: 2.5624 at iter 5608 **\n",
            "  ** New min smooth loss: 2.5622 at iter 5609 **\n",
            "  ** New min smooth loss: 2.5621 at iter 5611 **\n",
            "  ** New min smooth loss: 2.5618 at iter 5613 **\n",
            "  ** New min smooth loss: 2.5612 at iter 5614 **\n",
            "  ** New min smooth loss: 2.5605 at iter 5615 **\n",
            "  ** New min smooth loss: 2.5599 at iter 5616 **\n",
            "  ** New min smooth loss: 2.5597 at iter 5617 **\n",
            "  ** New min smooth loss: 2.5589 at iter 5618 **\n",
            "  ** New min smooth loss: 2.5582 at iter 5619 **\n",
            "  ** New min smooth loss: 2.5572 at iter 5620 **\n",
            "  ** New min smooth loss: 2.5568 at iter 5621 **\n",
            "  ** New min smooth loss: 2.5562 at iter 5622 **\n",
            "  ** New min smooth loss: 2.5555 at iter 5623 **\n",
            "  ** New min smooth loss: 2.5551 at iter 5624 **\n",
            "  ** New min smooth loss: 2.5545 at iter 5625 **\n",
            "  ** New min smooth loss: 2.5539 at iter 5626 **\n",
            "  ** New min smooth loss: 2.5535 at iter 5627 **\n",
            "  ** New min smooth loss: 2.5530 at iter 5628 **\n",
            "  ** New min smooth loss: 2.5524 at iter 5630 **\n",
            "  ** New min smooth loss: 2.5520 at iter 5631 **\n",
            "  ** New min smooth loss: 2.5511 at iter 5633 **\n",
            "  ** New min smooth loss: 2.5506 at iter 5634 **\n",
            "  ** New min smooth loss: 2.5498 at iter 5635 **\n",
            "  ** New min smooth loss: 2.5494 at iter 5637 **\n",
            "  ** New min smooth loss: 2.5490 at iter 5638 **\n",
            "  ** New min smooth loss: 2.5481 at iter 5639 **\n",
            "  ** New min smooth loss: 2.5475 at iter 5640 **\n",
            "  ** New min smooth loss: 2.5469 at iter 5641 **\n",
            "  ** New min smooth loss: 2.5460 at iter 5642 **\n",
            "  ** New min smooth loss: 2.5453 at iter 5643 **\n",
            "  ** New min smooth loss: 2.5448 at iter 5644 **\n",
            "  ** New min smooth loss: 2.5442 at iter 5645 **\n",
            "  ** New min smooth loss: 2.5436 at iter 5646 **\n",
            "  ** New min smooth loss: 2.5433 at iter 5647 **\n",
            "  ** New min smooth loss: 2.5429 at iter 5648 **\n",
            "  ** New min smooth loss: 2.5423 at iter 5649 **\n",
            "  ** New min smooth loss: 2.5416 at iter 5650 **\n",
            "  ** New min smooth loss: 2.5416 at iter 5651 **\n",
            "  ** New min smooth loss: 2.5407 at iter 5652 **\n",
            "  ** New min smooth loss: 2.5402 at iter 5653 **\n",
            "  ** New min smooth loss: 2.5388 at iter 5654 **\n",
            "  ** New min smooth loss: 2.5379 at iter 5655 **\n",
            "  ** New min smooth loss: 2.5375 at iter 5656 **\n",
            "  ** New min smooth loss: 2.5373 at iter 5669 **\n",
            "  ** New min smooth loss: 2.5370 at iter 5670 **\n",
            "  ** New min smooth loss: 2.5365 at iter 5671 **\n",
            "  ** New min smooth loss: 2.5357 at iter 5672 **\n",
            "  ** New min smooth loss: 2.5354 at iter 5673 **\n",
            "  ** New min smooth loss: 2.5348 at iter 5674 **\n",
            "  ** New min smooth loss: 2.5345 at iter 5675 **\n",
            "  ** New min smooth loss: 2.5337 at iter 5676 **\n",
            "  ** New min smooth loss: 2.5329 at iter 5677 **\n",
            "  ** New min smooth loss: 2.5323 at iter 5679 **\n",
            "  ** New min smooth loss: 2.5320 at iter 5680 **\n",
            "  ** New min smooth loss: 2.5318 at iter 5681 **\n",
            "  ** New min smooth loss: 2.5309 at iter 5682 **\n",
            "  ** New min smooth loss: 2.5306 at iter 5683 **\n",
            "  ** New min smooth loss: 2.5302 at iter 5684 **\n",
            "  ** New min smooth loss: 2.5301 at iter 5685 **\n",
            "  ** New min smooth loss: 2.5287 at iter 5686 **\n",
            "  ** New min smooth loss: 2.5280 at iter 5687 **\n",
            "  ** New min smooth loss: 2.5271 at iter 5688 **\n",
            "  ** New min smooth loss: 2.5270 at iter 5689 **\n",
            "  ** New min smooth loss: 2.5265 at iter 5690 **\n",
            "  ** New min smooth loss: 2.5260 at iter 5691 **\n",
            "  ** New min smooth loss: 2.5259 at iter 5692 **\n",
            "  ** New min smooth loss: 2.5252 at iter 5693 **\n",
            "  ** New min smooth loss: 2.5249 at iter 5694 **\n",
            "  ** New min smooth loss: 2.5244 at iter 5695 **\n",
            "  ** New min smooth loss: 2.5242 at iter 5696 **\n",
            "  ** New min smooth loss: 2.5239 at iter 5697 **\n",
            "  ** New min smooth loss: 2.5236 at iter 5698 **\n",
            "  ** New min smooth loss: 2.5235 at iter 5699 **\n",
            "  ** New min smooth loss: 2.5226 at iter 5700 **\n",
            "Iter: 5700/100000, Smooth Loss: 2.5226, Min Smooth Loss: 2.5226 (at iter 5700), Time: 28.72s\n",
            "  ** New min smooth loss: 2.5223 at iter 5701 **\n",
            "  ** New min smooth loss: 2.5219 at iter 5702 **\n",
            "  ** New min smooth loss: 2.5215 at iter 5703 **\n",
            "  ** New min smooth loss: 2.5207 at iter 5704 **\n",
            "  ** New min smooth loss: 2.5202 at iter 5705 **\n",
            "  ** New min smooth loss: 2.5196 at iter 5706 **\n",
            "  ** New min smooth loss: 2.5191 at iter 5707 **\n",
            "  ** New min smooth loss: 2.5189 at iter 5710 **\n",
            "  ** New min smooth loss: 2.5186 at iter 5711 **\n",
            "  ** New min smooth loss: 2.5183 at iter 5712 **\n",
            "  ** New min smooth loss: 2.5179 at iter 5713 **\n",
            "  ** New min smooth loss: 2.5171 at iter 5714 **\n",
            "  ** New min smooth loss: 2.5164 at iter 5715 **\n",
            "  ** New min smooth loss: 2.5160 at iter 5716 **\n",
            "  ** New min smooth loss: 2.5153 at iter 5717 **\n",
            "  ** New min smooth loss: 2.5147 at iter 5718 **\n",
            "  ** New min smooth loss: 2.5140 at iter 5719 **\n",
            "  ** New min smooth loss: 2.5131 at iter 5720 **\n",
            "  ** New min smooth loss: 2.5121 at iter 5721 **\n",
            "  ** New min smooth loss: 2.5117 at iter 5722 **\n",
            "  ** New min smooth loss: 2.5117 at iter 5723 **\n",
            "  ** New min smooth loss: 2.5109 at iter 5724 **\n",
            "  ** New min smooth loss: 2.5105 at iter 5725 **\n",
            "  ** New min smooth loss: 2.5098 at iter 5730 **\n",
            "  ** New min smooth loss: 2.5088 at iter 5731 **\n",
            "  ** New min smooth loss: 2.5079 at iter 5732 **\n",
            "  ** New min smooth loss: 2.5078 at iter 5733 **\n",
            "  ** New min smooth loss: 2.5070 at iter 5734 **\n",
            "  ** New min smooth loss: 2.5069 at iter 5735 **\n",
            "  ** New min smooth loss: 2.5059 at iter 5736 **\n",
            "  ** New min smooth loss: 2.5052 at iter 5737 **\n",
            "  ** New min smooth loss: 2.5042 at iter 5738 **\n",
            "  ** New min smooth loss: 2.5035 at iter 5739 **\n",
            "  ** New min smooth loss: 2.5034 at iter 5740 **\n",
            "  ** New min smooth loss: 2.5031 at iter 5741 **\n",
            "  ** New min smooth loss: 2.5029 at iter 5742 **\n",
            "  ** New min smooth loss: 2.5022 at iter 5743 **\n",
            "  ** New min smooth loss: 2.5019 at iter 5744 **\n",
            "  ** New min smooth loss: 2.5018 at iter 5745 **\n",
            "  ** New min smooth loss: 2.5015 at iter 5746 **\n",
            "  ** New min smooth loss: 2.5010 at iter 5747 **\n",
            "  ** New min smooth loss: 2.5006 at iter 5748 **\n",
            "  ** New min smooth loss: 2.5006 at iter 5750 **\n",
            "  ** New min smooth loss: 2.5001 at iter 5751 **\n",
            "  ** New min smooth loss: 2.5000 at iter 5754 **\n",
            "  ** New min smooth loss: 2.4997 at iter 5755 **\n",
            "  ** New min smooth loss: 2.4995 at iter 5756 **\n",
            "  ** New min smooth loss: 2.4990 at iter 5757 **\n",
            "  ** New min smooth loss: 2.4988 at iter 5758 **\n",
            "  ** New min smooth loss: 2.4982 at iter 5759 **\n",
            "  ** New min smooth loss: 2.4979 at iter 5760 **\n",
            "  ** New min smooth loss: 2.4976 at iter 5761 **\n",
            "  ** New min smooth loss: 2.4972 at iter 5778 **\n",
            "  ** New min smooth loss: 2.4964 at iter 5779 **\n",
            "  ** New min smooth loss: 2.4962 at iter 5780 **\n",
            "  ** New min smooth loss: 2.4960 at iter 5781 **\n",
            "  ** New min smooth loss: 2.4955 at iter 5782 **\n",
            "  ** New min smooth loss: 2.4945 at iter 5783 **\n",
            "  ** New min smooth loss: 2.4945 at iter 5784 **\n",
            "  ** New min smooth loss: 2.4943 at iter 5785 **\n",
            "  ** New min smooth loss: 2.4941 at iter 5786 **\n",
            "  ** New min smooth loss: 2.4940 at iter 5787 **\n",
            "  ** New min smooth loss: 2.4935 at iter 5789 **\n",
            "  ** New min smooth loss: 2.4935 at iter 5790 **\n",
            "  ** New min smooth loss: 2.4929 at iter 5791 **\n",
            "  ** New min smooth loss: 2.4924 at iter 5792 **\n",
            "  ** New min smooth loss: 2.4923 at iter 5793 **\n",
            "  ** New min smooth loss: 2.4921 at iter 5794 **\n",
            "  ** New min smooth loss: 2.4917 at iter 5795 **\n",
            "  ** New min smooth loss: 2.4916 at iter 5799 **\n",
            "  ** New min smooth loss: 2.4913 at iter 5800 **\n",
            "Iter: 5800/100000, Smooth Loss: 2.4913, Min Smooth Loss: 2.4913 (at iter 5800), Time: 29.15s\n",
            "  ** New min smooth loss: 2.4908 at iter 5801 **\n",
            "  ** New min smooth loss: 2.4905 at iter 5805 **\n",
            "  ** New min smooth loss: 2.4902 at iter 5806 **\n",
            "  ** New min smooth loss: 2.4899 at iter 5807 **\n",
            "  ** New min smooth loss: 2.4898 at iter 5808 **\n",
            "  ** New min smooth loss: 2.4890 at iter 5809 **\n",
            "  ** New min smooth loss: 2.4882 at iter 5810 **\n",
            "  ** New min smooth loss: 2.4877 at iter 5811 **\n",
            "  ** New min smooth loss: 2.4876 at iter 5812 **\n",
            "  ** New min smooth loss: 2.4871 at iter 5813 **\n",
            "  ** New min smooth loss: 2.4865 at iter 5814 **\n",
            "  ** New min smooth loss: 2.4858 at iter 5815 **\n",
            "  ** New min smooth loss: 2.4851 at iter 5816 **\n",
            "  ** New min smooth loss: 2.4845 at iter 5817 **\n",
            "  ** New min smooth loss: 2.4840 at iter 5818 **\n",
            "  ** New min smooth loss: 2.4830 at iter 5819 **\n",
            "  ** New min smooth loss: 2.4822 at iter 5820 **\n",
            "  ** New min smooth loss: 2.4815 at iter 5821 **\n",
            "  ** New min smooth loss: 2.4811 at iter 5822 **\n",
            "  ** New min smooth loss: 2.4810 at iter 5823 **\n",
            "  ** New min smooth loss: 2.4806 at iter 5826 **\n",
            "  ** New min smooth loss: 2.4803 at iter 5827 **\n",
            "  ** New min smooth loss: 2.4798 at iter 5828 **\n",
            "  ** New min smooth loss: 2.4797 at iter 5829 **\n",
            "  ** New min smooth loss: 2.4793 at iter 5833 **\n",
            "  ** New min smooth loss: 2.4789 at iter 5834 **\n",
            "  ** New min smooth loss: 2.4785 at iter 5835 **\n",
            "  ** New min smooth loss: 2.4783 at iter 5836 **\n",
            "  ** New min smooth loss: 2.4777 at iter 5837 **\n",
            "  ** New min smooth loss: 2.4771 at iter 5838 **\n",
            "  ** New min smooth loss: 2.4764 at iter 5839 **\n",
            "  ** New min smooth loss: 2.4760 at iter 5840 **\n",
            "  ** New min smooth loss: 2.4759 at iter 5841 **\n",
            "  ** New min smooth loss: 2.4751 at iter 5842 **\n",
            "  ** New min smooth loss: 2.4749 at iter 5843 **\n",
            "  ** New min smooth loss: 2.4744 at iter 5844 **\n",
            "  ** New min smooth loss: 2.4744 at iter 5845 **\n",
            "  ** New min smooth loss: 2.4742 at iter 5847 **\n",
            "  ** New min smooth loss: 2.4740 at iter 5848 **\n",
            "  ** New min smooth loss: 2.4735 at iter 5849 **\n",
            "  ** New min smooth loss: 2.4726 at iter 5850 **\n",
            "  ** New min smooth loss: 2.4724 at iter 5851 **\n",
            "  ** New min smooth loss: 2.4722 at iter 5852 **\n",
            "  ** New min smooth loss: 2.4718 at iter 5853 **\n",
            "  ** New min smooth loss: 2.4712 at iter 5854 **\n",
            "  ** New min smooth loss: 2.4711 at iter 5855 **\n",
            "  ** New min smooth loss: 2.4710 at iter 5856 **\n",
            "  ** New min smooth loss: 2.4708 at iter 5857 **\n",
            "  ** New min smooth loss: 2.4699 at iter 5858 **\n",
            "  ** New min smooth loss: 2.4699 at iter 5861 **\n",
            "  ** New min smooth loss: 2.4694 at iter 5862 **\n",
            "  ** New min smooth loss: 2.4689 at iter 5863 **\n",
            "  ** New min smooth loss: 2.4687 at iter 5864 **\n",
            "  ** New min smooth loss: 2.4682 at iter 5865 **\n",
            "  ** New min smooth loss: 2.4681 at iter 5866 **\n",
            "  ** New min smooth loss: 2.4676 at iter 5867 **\n",
            "  ** New min smooth loss: 2.4675 at iter 5868 **\n",
            "  ** New min smooth loss: 2.4674 at iter 5870 **\n",
            "  ** New min smooth loss: 2.4671 at iter 5871 **\n",
            "  ** New min smooth loss: 2.4669 at iter 5873 **\n",
            "  ** New min smooth loss: 2.4666 at iter 5875 **\n",
            "  ** New min smooth loss: 2.4663 at iter 5876 **\n",
            "  ** New min smooth loss: 2.4659 at iter 5877 **\n",
            "  ** New min smooth loss: 2.4657 at iter 5878 **\n",
            "  ** New min smooth loss: 2.4652 at iter 5880 **\n",
            "  ** New min smooth loss: 2.4646 at iter 5881 **\n",
            "  ** New min smooth loss: 2.4643 at iter 5882 **\n",
            "  ** New min smooth loss: 2.4637 at iter 5883 **\n",
            "  ** New min smooth loss: 2.4635 at iter 5886 **\n",
            "  ** New min smooth loss: 2.4633 at iter 5887 **\n",
            "  ** New min smooth loss: 2.4626 at iter 5888 **\n",
            "  ** New min smooth loss: 2.4621 at iter 5889 **\n",
            "  ** New min smooth loss: 2.4617 at iter 5890 **\n",
            "  ** New min smooth loss: 2.4609 at iter 5891 **\n",
            "  ** New min smooth loss: 2.4607 at iter 5892 **\n",
            "  ** New min smooth loss: 2.4606 at iter 5893 **\n",
            "  ** New min smooth loss: 2.4600 at iter 5894 **\n",
            "  ** New min smooth loss: 2.4594 at iter 5895 **\n",
            "  ** New min smooth loss: 2.4593 at iter 5896 **\n",
            "  ** New min smooth loss: 2.4587 at iter 5897 **\n",
            "  ** New min smooth loss: 2.4583 at iter 5900 **\n",
            "Iter: 5900/100000, Smooth Loss: 2.4583, Min Smooth Loss: 2.4583 (at iter 5900), Time: 29.60s\n",
            "  ** New min smooth loss: 2.4578 at iter 5901 **\n",
            "  ** New min smooth loss: 2.4575 at iter 5902 **\n",
            "  ** New min smooth loss: 2.4567 at iter 5903 **\n",
            "  ** New min smooth loss: 2.4564 at iter 5904 **\n",
            "  ** New min smooth loss: 2.4559 at iter 5905 **\n",
            "  ** New min smooth loss: 2.4550 at iter 5906 **\n",
            "  ** New min smooth loss: 2.4547 at iter 5907 **\n",
            "  ** New min smooth loss: 2.4542 at iter 5908 **\n",
            "  ** New min smooth loss: 2.4540 at iter 5909 **\n",
            "  ** New min smooth loss: 2.4533 at iter 5910 **\n",
            "  ** New min smooth loss: 2.4527 at iter 5911 **\n",
            "  ** New min smooth loss: 2.4522 at iter 5912 **\n",
            "  ** New min smooth loss: 2.4517 at iter 5913 **\n",
            "  ** New min smooth loss: 2.4512 at iter 5914 **\n",
            "  ** New min smooth loss: 2.4512 at iter 5915 **\n",
            "  ** New min smooth loss: 2.4507 at iter 5916 **\n",
            "  ** New min smooth loss: 2.4503 at iter 5917 **\n",
            "  ** New min smooth loss: 2.4499 at iter 5918 **\n",
            "  ** New min smooth loss: 2.4492 at iter 5919 **\n",
            "  ** New min smooth loss: 2.4490 at iter 5920 **\n",
            "  ** New min smooth loss: 2.4486 at iter 5921 **\n",
            "  ** New min smooth loss: 2.4479 at iter 5922 **\n",
            "  ** New min smooth loss: 2.4475 at iter 5923 **\n",
            "  ** New min smooth loss: 2.4471 at iter 5924 **\n",
            "  ** New min smooth loss: 2.4467 at iter 5926 **\n",
            "  ** New min smooth loss: 2.4463 at iter 5927 **\n",
            "  ** New min smooth loss: 2.4455 at iter 5928 **\n",
            "  ** New min smooth loss: 2.4453 at iter 5929 **\n",
            "  ** New min smooth loss: 2.4450 at iter 5931 **\n",
            "  ** New min smooth loss: 2.4445 at iter 5932 **\n",
            "  ** New min smooth loss: 2.4442 at iter 5933 **\n",
            "  ** New min smooth loss: 2.4440 at iter 5934 **\n",
            "  ** New min smooth loss: 2.4440 at iter 5935 **\n",
            "  ** New min smooth loss: 2.4439 at iter 5937 **\n",
            "  ** New min smooth loss: 2.4439 at iter 5942 **\n",
            "  ** New min smooth loss: 2.4426 at iter 5943 **\n",
            "  ** New min smooth loss: 2.4425 at iter 5945 **\n",
            "  ** New min smooth loss: 2.4422 at iter 5946 **\n",
            "  ** New min smooth loss: 2.4418 at iter 5947 **\n",
            "  ** New min smooth loss: 2.4411 at iter 5949 **\n",
            "  ** New min smooth loss: 2.4405 at iter 5950 **\n",
            "  ** New min smooth loss: 2.4401 at iter 5951 **\n",
            "  ** New min smooth loss: 2.4401 at iter 5952 **\n",
            "  ** New min smooth loss: 2.4396 at iter 5953 **\n",
            "  ** New min smooth loss: 2.4393 at iter 5954 **\n",
            "  ** New min smooth loss: 2.4385 at iter 5955 **\n",
            "  ** New min smooth loss: 2.4377 at iter 5956 **\n",
            "  ** New min smooth loss: 2.4377 at iter 5963 **\n",
            "  ** New min smooth loss: 2.4367 at iter 5964 **\n",
            "  ** New min smooth loss: 2.4365 at iter 5965 **\n",
            "  ** New min smooth loss: 2.4361 at iter 5966 **\n",
            "  ** New min smooth loss: 2.4359 at iter 5967 **\n",
            "  ** New min smooth loss: 2.4356 at iter 5969 **\n",
            "  ** New min smooth loss: 2.4349 at iter 5971 **\n",
            "  ** New min smooth loss: 2.4346 at iter 5972 **\n",
            "  ** New min smooth loss: 2.4344 at iter 5973 **\n",
            "  ** New min smooth loss: 2.4336 at iter 5974 **\n",
            "  ** New min smooth loss: 2.4328 at iter 5975 **\n",
            "  ** New min smooth loss: 2.4320 at iter 5976 **\n",
            "  ** New min smooth loss: 2.4317 at iter 5978 **\n",
            "  ** New min smooth loss: 2.4316 at iter 5979 **\n",
            "  ** New min smooth loss: 2.4312 at iter 5980 **\n",
            "  ** New min smooth loss: 2.4309 at iter 5981 **\n",
            "  ** New min smooth loss: 2.4304 at iter 5982 **\n",
            "  ** New min smooth loss: 2.4304 at iter 5983 **\n",
            "  ** New min smooth loss: 2.4301 at iter 5984 **\n",
            "  ** New min smooth loss: 2.4294 at iter 5989 **\n",
            "  ** New min smooth loss: 2.4291 at iter 5990 **\n",
            "  ** New min smooth loss: 2.4288 at iter 5991 **\n",
            "  ** New min smooth loss: 2.4286 at iter 5992 **\n",
            "  ** New min smooth loss: 2.4283 at iter 5993 **\n",
            "  ** New min smooth loss: 2.4277 at iter 5994 **\n",
            "  ** New min smooth loss: 2.4272 at iter 5997 **\n",
            "  ** New min smooth loss: 2.4271 at iter 5998 **\n",
            "  ** New min smooth loss: 2.4270 at iter 5999 **\n",
            "Iter: 6000/100000, Smooth Loss: 2.4272, Min Smooth Loss: 2.4270 (at iter 5999), Time: 30.05s\n",
            "  ** New min smooth loss: 2.4267 at iter 6001 **\n",
            "  ** New min smooth loss: 2.4266 at iter 6002 **\n",
            "  ** New min smooth loss: 2.4258 at iter 6004 **\n",
            "  ** New min smooth loss: 2.4257 at iter 6005 **\n",
            "  ** New min smooth loss: 2.4250 at iter 6006 **\n",
            "  ** New min smooth loss: 2.4244 at iter 6008 **\n",
            "  ** New min smooth loss: 2.4240 at iter 6009 **\n",
            "  ** New min smooth loss: 2.4237 at iter 6010 **\n",
            "  ** New min smooth loss: 2.4235 at iter 6014 **\n",
            "  ** New min smooth loss: 2.4235 at iter 6015 **\n",
            "  ** New min smooth loss: 2.4232 at iter 6016 **\n",
            "  ** New min smooth loss: 2.4231 at iter 6018 **\n",
            "  ** New min smooth loss: 2.4231 at iter 6019 **\n",
            "  ** New min smooth loss: 2.4228 at iter 6020 **\n",
            "  ** New min smooth loss: 2.4226 at iter 6021 **\n",
            "  ** New min smooth loss: 2.4223 at iter 6022 **\n",
            "  ** New min smooth loss: 2.4218 at iter 6023 **\n",
            "  ** New min smooth loss: 2.4215 at iter 6024 **\n",
            "  ** New min smooth loss: 2.4210 at iter 6025 **\n",
            "  ** New min smooth loss: 2.4204 at iter 6026 **\n",
            "  ** New min smooth loss: 2.4203 at iter 6027 **\n",
            "  ** New min smooth loss: 2.4197 at iter 6029 **\n",
            "  ** New min smooth loss: 2.4190 at iter 6030 **\n",
            "  ** New min smooth loss: 2.4187 at iter 6031 **\n",
            "  ** New min smooth loss: 2.4185 at iter 6032 **\n",
            "  ** New min smooth loss: 2.4183 at iter 6045 **\n",
            "  ** New min smooth loss: 2.4182 at iter 6047 **\n",
            "  ** New min smooth loss: 2.4178 at iter 6048 **\n",
            "  ** New min smooth loss: 2.4168 at iter 6049 **\n",
            "  ** New min smooth loss: 2.4160 at iter 6050 **\n",
            "  ** New min smooth loss: 2.4157 at iter 6051 **\n",
            "  ** New min smooth loss: 2.4150 at iter 6052 **\n",
            "  ** New min smooth loss: 2.4149 at iter 6055 **\n",
            "  ** New min smooth loss: 2.4145 at iter 6056 **\n",
            "  ** New min smooth loss: 2.4134 at iter 6058 **\n",
            "  ** New min smooth loss: 2.4129 at iter 6059 **\n",
            "  ** New min smooth loss: 2.4128 at iter 6060 **\n",
            "  ** New min smooth loss: 2.4122 at iter 6061 **\n",
            "  ** New min smooth loss: 2.4117 at iter 6062 **\n",
            "  ** New min smooth loss: 2.4117 at iter 6063 **\n",
            "  ** New min smooth loss: 2.4114 at iter 6064 **\n",
            "  ** New min smooth loss: 2.4112 at iter 6065 **\n",
            "  ** New min smooth loss: 2.4107 at iter 6066 **\n",
            "  ** New min smooth loss: 2.4107 at iter 6067 **\n",
            "  ** New min smooth loss: 2.4101 at iter 6073 **\n",
            "  ** New min smooth loss: 2.4095 at iter 6074 **\n",
            "  ** New min smooth loss: 2.4093 at iter 6075 **\n",
            "  ** New min smooth loss: 2.4089 at iter 6076 **\n",
            "  ** New min smooth loss: 2.4087 at iter 6077 **\n",
            "  ** New min smooth loss: 2.4086 at iter 6078 **\n",
            "  ** New min smooth loss: 2.4078 at iter 6079 **\n",
            "  ** New min smooth loss: 2.4074 at iter 6080 **\n",
            "  ** New min smooth loss: 2.4074 at iter 6081 **\n",
            "  ** New min smooth loss: 2.4071 at iter 6082 **\n",
            "  ** New min smooth loss: 2.4069 at iter 6083 **\n",
            "  ** New min smooth loss: 2.4068 at iter 6084 **\n",
            "  ** New min smooth loss: 2.4065 at iter 6085 **\n",
            "  ** New min smooth loss: 2.4064 at iter 6086 **\n",
            "  ** New min smooth loss: 2.4064 at iter 6087 **\n",
            "  ** New min smooth loss: 2.4058 at iter 6088 **\n",
            "  ** New min smooth loss: 2.4051 at iter 6089 **\n",
            "  ** New min smooth loss: 2.4047 at iter 6090 **\n",
            "  ** New min smooth loss: 2.4046 at iter 6091 **\n",
            "  ** New min smooth loss: 2.4033 at iter 6092 **\n",
            "  ** New min smooth loss: 2.4029 at iter 6093 **\n",
            "  ** New min smooth loss: 2.4027 at iter 6094 **\n",
            "  ** New min smooth loss: 2.4016 at iter 6095 **\n",
            "  ** New min smooth loss: 2.4015 at iter 6096 **\n",
            "  ** New min smooth loss: 2.4009 at iter 6098 **\n",
            "  ** New min smooth loss: 2.4000 at iter 6100 **\n",
            "Iter: 6100/100000, Smooth Loss: 2.4000, Min Smooth Loss: 2.4000 (at iter 6100), Time: 30.48s\n",
            "  ** New min smooth loss: 2.3993 at iter 6101 **\n",
            "  ** New min smooth loss: 2.3992 at iter 6103 **\n",
            "  ** New min smooth loss: 2.3986 at iter 6105 **\n",
            "  ** New min smooth loss: 2.3985 at iter 6106 **\n",
            "  ** New min smooth loss: 2.3983 at iter 6113 **\n",
            "  ** New min smooth loss: 2.3978 at iter 6114 **\n",
            "  ** New min smooth loss: 2.3973 at iter 6115 **\n",
            "  ** New min smooth loss: 2.3968 at iter 6116 **\n",
            "  ** New min smooth loss: 2.3966 at iter 6117 **\n",
            "  ** New min smooth loss: 2.3963 at iter 6118 **\n",
            "  ** New min smooth loss: 2.3959 at iter 6119 **\n",
            "  ** New min smooth loss: 2.3959 at iter 6120 **\n",
            "  ** New min smooth loss: 2.3956 at iter 6121 **\n",
            "  ** New min smooth loss: 2.3951 at iter 6122 **\n",
            "  ** New min smooth loss: 2.3950 at iter 6127 **\n",
            "  ** New min smooth loss: 2.3947 at iter 6128 **\n",
            "  ** New min smooth loss: 2.3946 at iter 6129 **\n",
            "  ** New min smooth loss: 2.3945 at iter 6130 **\n",
            "  ** New min smooth loss: 2.3941 at iter 6158 **\n",
            "  ** New min smooth loss: 2.3935 at iter 6159 **\n",
            "  ** New min smooth loss: 2.3934 at iter 6160 **\n",
            "  ** New min smooth loss: 2.3933 at iter 6161 **\n",
            "  ** New min smooth loss: 2.3933 at iter 6162 **\n",
            "  ** New min smooth loss: 2.3928 at iter 6163 **\n",
            "  ** New min smooth loss: 2.3925 at iter 6164 **\n",
            "  ** New min smooth loss: 2.3924 at iter 6167 **\n",
            "  ** New min smooth loss: 2.3920 at iter 6168 **\n",
            "  ** New min smooth loss: 2.3914 at iter 6169 **\n",
            "  ** New min smooth loss: 2.3908 at iter 6170 **\n",
            "  ** New min smooth loss: 2.3903 at iter 6171 **\n",
            "  ** New min smooth loss: 2.3901 at iter 6172 **\n",
            "  ** New min smooth loss: 2.3898 at iter 6173 **\n",
            "  ** New min smooth loss: 2.3896 at iter 6174 **\n",
            "  ** New min smooth loss: 2.3888 at iter 6175 **\n",
            "  ** New min smooth loss: 2.3886 at iter 6176 **\n",
            "  ** New min smooth loss: 2.3885 at iter 6177 **\n",
            "  ** New min smooth loss: 2.3877 at iter 6178 **\n",
            "  ** New min smooth loss: 2.3873 at iter 6179 **\n",
            "  ** New min smooth loss: 2.3871 at iter 6180 **\n",
            "  ** New min smooth loss: 2.3866 at iter 6181 **\n",
            "  ** New min smooth loss: 2.3863 at iter 6182 **\n",
            "  ** New min smooth loss: 2.3861 at iter 6183 **\n",
            "  ** New min smooth loss: 2.3855 at iter 6184 **\n",
            "  ** New min smooth loss: 2.3855 at iter 6185 **\n",
            "  ** New min smooth loss: 2.3849 at iter 6186 **\n",
            "  ** New min smooth loss: 2.3843 at iter 6187 **\n",
            "  ** New min smooth loss: 2.3839 at iter 6188 **\n",
            "  ** New min smooth loss: 2.3839 at iter 6190 **\n",
            "  ** New min smooth loss: 2.3836 at iter 6191 **\n",
            "  ** New min smooth loss: 2.3834 at iter 6192 **\n",
            "  ** New min smooth loss: 2.3831 at iter 6193 **\n",
            "  ** New min smooth loss: 2.3825 at iter 6194 **\n",
            "  ** New min smooth loss: 2.3822 at iter 6195 **\n",
            "  ** New min smooth loss: 2.3816 at iter 6197 **\n",
            "  ** New min smooth loss: 2.3810 at iter 6198 **\n",
            "  ** New min smooth loss: 2.3807 at iter 6199 **\n",
            "  ** New min smooth loss: 2.3799 at iter 6200 **\n",
            "Iter: 6200/100000, Smooth Loss: 2.3799, Min Smooth Loss: 2.3799 (at iter 6200), Time: 30.92s\n",
            "  ** New min smooth loss: 2.3796 at iter 6201 **\n",
            "  ** New min smooth loss: 2.3794 at iter 6202 **\n",
            "  ** New min smooth loss: 2.3786 at iter 6203 **\n",
            "  ** New min smooth loss: 2.3785 at iter 6204 **\n",
            "  ** New min smooth loss: 2.3781 at iter 6206 **\n",
            "  ** New min smooth loss: 2.3777 at iter 6207 **\n",
            "  ** New min smooth loss: 2.3769 at iter 6208 **\n",
            "  ** New min smooth loss: 2.3765 at iter 6209 **\n",
            "  ** New min smooth loss: 2.3763 at iter 6210 **\n",
            "  ** New min smooth loss: 2.3762 at iter 6211 **\n",
            "  ** New min smooth loss: 2.3760 at iter 6212 **\n",
            "  ** New min smooth loss: 2.3758 at iter 6213 **\n",
            "  ** New min smooth loss: 2.3755 at iter 6214 **\n",
            "  ** New min smooth loss: 2.3755 at iter 6215 **\n",
            "  ** New min smooth loss: 2.3751 at iter 6216 **\n",
            "  ** New min smooth loss: 2.3743 at iter 6217 **\n",
            "  ** New min smooth loss: 2.3737 at iter 6218 **\n",
            "  ** New min smooth loss: 2.3731 at iter 6219 **\n",
            "  ** New min smooth loss: 2.3725 at iter 6220 **\n",
            "  ** New min smooth loss: 2.3720 at iter 6221 **\n",
            "  ** New min smooth loss: 2.3718 at iter 6222 **\n",
            "  ** New min smooth loss: 2.3715 at iter 6224 **\n",
            "  ** New min smooth loss: 2.3709 at iter 6225 **\n",
            "  ** New min smooth loss: 2.3706 at iter 6226 **\n",
            "  ** New min smooth loss: 2.3701 at iter 6227 **\n",
            "  ** New min smooth loss: 2.3699 at iter 6228 **\n",
            "  ** New min smooth loss: 2.3694 at iter 6229 **\n",
            "  ** New min smooth loss: 2.3687 at iter 6230 **\n",
            "  ** New min smooth loss: 2.3684 at iter 6231 **\n",
            "  ** New min smooth loss: 2.3683 at iter 6232 **\n",
            "  ** New min smooth loss: 2.3678 at iter 6234 **\n",
            "  ** New min smooth loss: 2.3677 at iter 6239 **\n",
            "  ** New min smooth loss: 2.3673 at iter 6241 **\n",
            "  ** New min smooth loss: 2.3671 at iter 6243 **\n",
            "  ** New min smooth loss: 2.3667 at iter 6247 **\n",
            "  ** New min smooth loss: 2.3659 at iter 6253 **\n",
            "  ** New min smooth loss: 2.3657 at iter 6254 **\n",
            "  ** New min smooth loss: 2.3655 at iter 6255 **\n",
            "  ** New min smooth loss: 2.3650 at iter 6256 **\n",
            "  ** New min smooth loss: 2.3645 at iter 6257 **\n",
            "  ** New min smooth loss: 2.3642 at iter 6258 **\n",
            "  ** New min smooth loss: 2.3641 at iter 6259 **\n",
            "  ** New min smooth loss: 2.3637 at iter 6260 **\n",
            "  ** New min smooth loss: 2.3632 at iter 6261 **\n",
            "  ** New min smooth loss: 2.3630 at iter 6262 **\n",
            "  ** New min smooth loss: 2.3628 at iter 6263 **\n",
            "  ** New min smooth loss: 2.3623 at iter 6264 **\n",
            "  ** New min smooth loss: 2.3616 at iter 6265 **\n",
            "  ** New min smooth loss: 2.3609 at iter 6266 **\n",
            "  ** New min smooth loss: 2.3607 at iter 6267 **\n",
            "  ** New min smooth loss: 2.3603 at iter 6268 **\n",
            "  ** New min smooth loss: 2.3601 at iter 6269 **\n",
            "  ** New min smooth loss: 2.3601 at iter 6270 **\n",
            "  ** New min smooth loss: 2.3600 at iter 6271 **\n",
            "  ** New min smooth loss: 2.3600 at iter 6272 **\n",
            "  ** New min smooth loss: 2.3590 at iter 6273 **\n",
            "  ** New min smooth loss: 2.3590 at iter 6274 **\n",
            "  ** New min smooth loss: 2.3587 at iter 6275 **\n",
            "  ** New min smooth loss: 2.3581 at iter 6276 **\n",
            "  ** New min smooth loss: 2.3576 at iter 6277 **\n",
            "  ** New min smooth loss: 2.3573 at iter 6278 **\n",
            "  ** New min smooth loss: 2.3571 at iter 6279 **\n",
            "  ** New min smooth loss: 2.3570 at iter 6281 **\n",
            "  ** New min smooth loss: 2.3567 at iter 6282 **\n",
            "  ** New min smooth loss: 2.3566 at iter 6283 **\n",
            "  ** New min smooth loss: 2.3565 at iter 6288 **\n",
            "  ** New min smooth loss: 2.3558 at iter 6289 **\n",
            "  ** New min smooth loss: 2.3558 at iter 6291 **\n",
            "  ** New min smooth loss: 2.3553 at iter 6292 **\n",
            "Iter: 6300/100000, Smooth Loss: 2.3557, Min Smooth Loss: 2.3553 (at iter 6292), Time: 31.35s\n",
            "  ** New min smooth loss: 2.3553 at iter 6302 **\n",
            "  ** New min smooth loss: 2.3550 at iter 6382 **\n",
            "  ** New min smooth loss: 2.3546 at iter 6383 **\n",
            "  ** New min smooth loss: 2.3543 at iter 6384 **\n",
            "  ** New min smooth loss: 2.3539 at iter 6385 **\n",
            "  ** New min smooth loss: 2.3537 at iter 6386 **\n",
            "  ** New min smooth loss: 2.3533 at iter 6387 **\n",
            "  ** New min smooth loss: 2.3528 at iter 6388 **\n",
            "  ** New min smooth loss: 2.3523 at iter 6389 **\n",
            "  ** New min smooth loss: 2.3522 at iter 6391 **\n",
            "  ** New min smooth loss: 2.3519 at iter 6392 **\n",
            "  ** New min smooth loss: 2.3517 at iter 6393 **\n",
            "  ** New min smooth loss: 2.3516 at iter 6394 **\n",
            "  ** New min smooth loss: 2.3509 at iter 6395 **\n",
            "  ** New min smooth loss: 2.3504 at iter 6396 **\n",
            "Iter: 6400/100000, Smooth Loss: 2.3545, Min Smooth Loss: 2.3504 (at iter 6396), Time: 31.80s\n",
            "  ** New min smooth loss: 2.3498 at iter 6443 **\n",
            "  ** New min smooth loss: 2.3493 at iter 6444 **\n",
            "  ** New min smooth loss: 2.3491 at iter 6445 **\n",
            "  ** New min smooth loss: 2.3484 at iter 6446 **\n",
            "  ** New min smooth loss: 2.3482 at iter 6448 **\n",
            "  ** New min smooth loss: 2.3480 at iter 6449 **\n",
            "  ** New min smooth loss: 2.3478 at iter 6450 **\n",
            "  ** New min smooth loss: 2.3472 at iter 6452 **\n",
            "  ** New min smooth loss: 2.3471 at iter 6453 **\n",
            "  ** New min smooth loss: 2.3469 at iter 6454 **\n",
            "  ** New min smooth loss: 2.3467 at iter 6455 **\n",
            "  ** New min smooth loss: 2.3463 at iter 6456 **\n",
            "  ** New min smooth loss: 2.3459 at iter 6457 **\n",
            "  ** New min smooth loss: 2.3455 at iter 6458 **\n",
            "  ** New min smooth loss: 2.3454 at iter 6459 **\n",
            "  ** New min smooth loss: 2.3446 at iter 6460 **\n",
            "  ** New min smooth loss: 2.3444 at iter 6461 **\n",
            "  ** New min smooth loss: 2.3442 at iter 6465 **\n",
            "  ** New min smooth loss: 2.3442 at iter 6467 **\n",
            "  ** New min smooth loss: 2.3439 at iter 6468 **\n",
            "  ** New min smooth loss: 2.3437 at iter 6469 **\n",
            "  ** New min smooth loss: 2.3434 at iter 6470 **\n",
            "  ** New min smooth loss: 2.3431 at iter 6471 **\n",
            "  ** New min smooth loss: 2.3430 at iter 6474 **\n",
            "  ** New min smooth loss: 2.3430 at iter 6475 **\n",
            "  ** New min smooth loss: 2.3427 at iter 6476 **\n",
            "  ** New min smooth loss: 2.3417 at iter 6477 **\n",
            "  ** New min smooth loss: 2.3415 at iter 6478 **\n",
            "  ** New min smooth loss: 2.3413 at iter 6479 **\n",
            "  ** New min smooth loss: 2.3407 at iter 6480 **\n",
            "  ** New min smooth loss: 2.3402 at iter 6481 **\n",
            "  ** New min smooth loss: 2.3399 at iter 6487 **\n",
            "  ** New min smooth loss: 2.3393 at iter 6488 **\n",
            "  ** New min smooth loss: 2.3392 at iter 6489 **\n",
            "  ** New min smooth loss: 2.3390 at iter 6491 **\n",
            "  ** New min smooth loss: 2.3387 at iter 6492 **\n",
            "  ** New min smooth loss: 2.3382 at iter 6494 **\n",
            "  ** New min smooth loss: 2.3380 at iter 6495 **\n",
            "  ** New min smooth loss: 2.3375 at iter 6496 **\n",
            "  ** New min smooth loss: 2.3370 at iter 6497 **\n",
            "Iter: 6500/100000, Smooth Loss: 2.3385, Min Smooth Loss: 2.3370 (at iter 6497), Time: 32.22s\n",
            "  ** New min smooth loss: 2.3367 at iter 6506 **\n",
            "  ** New min smooth loss: 2.3365 at iter 6507 **\n",
            "  ** New min smooth loss: 2.3362 at iter 6509 **\n",
            "  ** New min smooth loss: 2.3361 at iter 6510 **\n",
            "  ** New min smooth loss: 2.3355 at iter 6511 **\n",
            "  ** New min smooth loss: 2.3354 at iter 6513 **\n",
            "  ** New min smooth loss: 2.3351 at iter 6514 **\n",
            "  ** New min smooth loss: 2.3345 at iter 6515 **\n",
            "  ** New min smooth loss: 2.3342 at iter 6516 **\n",
            "  ** New min smooth loss: 2.3337 at iter 6518 **\n",
            "  ** New min smooth loss: 2.3335 at iter 6520 **\n",
            "  ** New min smooth loss: 2.3331 at iter 6521 **\n",
            "  ** New min smooth loss: 2.3327 at iter 6522 **\n",
            "  ** New min smooth loss: 2.3326 at iter 6523 **\n",
            "  ** New min smooth loss: 2.3320 at iter 6525 **\n",
            "  ** New min smooth loss: 2.3319 at iter 6529 **\n",
            "  ** New min smooth loss: 2.3316 at iter 6534 **\n",
            "  ** New min smooth loss: 2.3311 at iter 6535 **\n",
            "  ** New min smooth loss: 2.3308 at iter 6537 **\n",
            "  ** New min smooth loss: 2.3306 at iter 6538 **\n",
            "  ** New min smooth loss: 2.3300 at iter 6539 **\n",
            "  ** New min smooth loss: 2.3299 at iter 6542 **\n",
            "  ** New min smooth loss: 2.3298 at iter 6543 **\n",
            "  ** New min smooth loss: 2.3296 at iter 6544 **\n",
            "  ** New min smooth loss: 2.3293 at iter 6566 **\n",
            "  ** New min smooth loss: 2.3289 at iter 6567 **\n",
            "  ** New min smooth loss: 2.3285 at iter 6568 **\n",
            "  ** New min smooth loss: 2.3282 at iter 6569 **\n",
            "  ** New min smooth loss: 2.3279 at iter 6572 **\n",
            "  ** New min smooth loss: 2.3274 at iter 6573 **\n",
            "  ** New min smooth loss: 2.3268 at iter 6576 **\n",
            "  ** New min smooth loss: 2.3268 at iter 6577 **\n",
            "  ** New min smooth loss: 2.3263 at iter 6578 **\n",
            "  ** New min smooth loss: 2.3261 at iter 6579 **\n",
            "  ** New min smooth loss: 2.3259 at iter 6580 **\n",
            "  ** New min smooth loss: 2.3257 at iter 6582 **\n",
            "  ** New min smooth loss: 2.3255 at iter 6583 **\n",
            "  ** New min smooth loss: 2.3252 at iter 6584 **\n",
            "  ** New min smooth loss: 2.3251 at iter 6585 **\n",
            "  ** New min smooth loss: 2.3247 at iter 6586 **\n",
            "  ** New min smooth loss: 2.3246 at iter 6587 **\n",
            "  ** New min smooth loss: 2.3244 at iter 6591 **\n",
            "  ** New min smooth loss: 2.3243 at iter 6593 **\n",
            "  ** New min smooth loss: 2.3240 at iter 6594 **\n",
            "  ** New min smooth loss: 2.3239 at iter 6595 **\n",
            "  ** New min smooth loss: 2.3236 at iter 6597 **\n",
            "  ** New min smooth loss: 2.3231 at iter 6598 **\n",
            "  ** New min smooth loss: 2.3230 at iter 6599 **\n",
            "  ** New min smooth loss: 2.3222 at iter 6600 **\n",
            "Iter: 6600/100000, Smooth Loss: 2.3222, Min Smooth Loss: 2.3222 (at iter 6600), Time: 32.66s\n",
            "  ** New min smooth loss: 2.3220 at iter 6603 **\n",
            "  ** New min smooth loss: 2.3218 at iter 6604 **\n",
            "  ** New min smooth loss: 2.3215 at iter 6605 **\n",
            "  ** New min smooth loss: 2.3214 at iter 6608 **\n",
            "  ** New min smooth loss: 2.3210 at iter 6609 **\n",
            "  ** New min smooth loss: 2.3207 at iter 6610 **\n",
            "  ** New min smooth loss: 2.3207 at iter 6611 **\n",
            "  ** New min smooth loss: 2.3206 at iter 6613 **\n",
            "  ** New min smooth loss: 2.3206 at iter 6614 **\n",
            "  ** New min smooth loss: 2.3204 at iter 6615 **\n",
            "  ** New min smooth loss: 2.3204 at iter 6618 **\n",
            "  ** New min smooth loss: 2.3203 at iter 6622 **\n",
            "  ** New min smooth loss: 2.3201 at iter 6623 **\n",
            "  ** New min smooth loss: 2.3197 at iter 6625 **\n",
            "  ** New min smooth loss: 2.3194 at iter 6626 **\n",
            "  ** New min smooth loss: 2.3193 at iter 6629 **\n",
            "  ** New min smooth loss: 2.3189 at iter 6630 **\n",
            "  ** New min smooth loss: 2.3185 at iter 6631 **\n",
            "  ** New min smooth loss: 2.3183 at iter 6632 **\n",
            "  ** New min smooth loss: 2.3176 at iter 6633 **\n",
            "  ** New min smooth loss: 2.3172 at iter 6634 **\n",
            "  ** New min smooth loss: 2.3168 at iter 6635 **\n",
            "  ** New min smooth loss: 2.3164 at iter 6636 **\n",
            "  ** New min smooth loss: 2.3160 at iter 6637 **\n",
            "  ** New min smooth loss: 2.3154 at iter 6638 **\n",
            "  ** New min smooth loss: 2.3147 at iter 6639 **\n",
            "  ** New min smooth loss: 2.3145 at iter 6640 **\n",
            "  ** New min smooth loss: 2.3143 at iter 6641 **\n",
            "  ** New min smooth loss: 2.3138 at iter 6642 **\n",
            "  ** New min smooth loss: 2.3133 at iter 6643 **\n",
            "  ** New min smooth loss: 2.3130 at iter 6644 **\n",
            "  ** New min smooth loss: 2.3127 at iter 6645 **\n",
            "  ** New min smooth loss: 2.3121 at iter 6646 **\n",
            "  ** New min smooth loss: 2.3115 at iter 6647 **\n",
            "  ** New min smooth loss: 2.3114 at iter 6649 **\n",
            "  ** New min smooth loss: 2.3111 at iter 6650 **\n",
            "  ** New min smooth loss: 2.3108 at iter 6651 **\n",
            "  ** New min smooth loss: 2.3104 at iter 6652 **\n",
            "  ** New min smooth loss: 2.3098 at iter 6653 **\n",
            "  ** New min smooth loss: 2.3095 at iter 6657 **\n",
            "  ** New min smooth loss: 2.3091 at iter 6658 **\n",
            "  ** New min smooth loss: 2.3086 at iter 6659 **\n",
            "  ** New min smooth loss: 2.3084 at iter 6660 **\n",
            "  ** New min smooth loss: 2.3081 at iter 6661 **\n",
            "  ** New min smooth loss: 2.3076 at iter 6662 **\n",
            "  ** New min smooth loss: 2.3073 at iter 6663 **\n",
            "  ** New min smooth loss: 2.3073 at iter 6664 **\n",
            "  ** New min smooth loss: 2.3066 at iter 6665 **\n",
            "  ** New min smooth loss: 2.3060 at iter 6666 **\n",
            "  ** New min smooth loss: 2.3054 at iter 6667 **\n",
            "  ** New min smooth loss: 2.3049 at iter 6668 **\n",
            "  ** New min smooth loss: 2.3045 at iter 6673 **\n",
            "  ** New min smooth loss: 2.3041 at iter 6674 **\n",
            "  ** New min smooth loss: 2.3039 at iter 6675 **\n",
            "  ** New min smooth loss: 2.3038 at iter 6676 **\n",
            "  ** New min smooth loss: 2.3035 at iter 6680 **\n",
            "  ** New min smooth loss: 2.3034 at iter 6682 **\n",
            "  ** New min smooth loss: 2.3030 at iter 6683 **\n",
            "  ** New min smooth loss: 2.3027 at iter 6684 **\n",
            "  ** New min smooth loss: 2.3026 at iter 6690 **\n",
            "  ** New min smooth loss: 2.3022 at iter 6691 **\n",
            "  ** New min smooth loss: 2.3019 at iter 6692 **\n",
            "  ** New min smooth loss: 2.3009 at iter 6696 **\n",
            "  ** New min smooth loss: 2.3008 at iter 6697 **\n",
            "  ** New min smooth loss: 2.3006 at iter 6698 **\n",
            "  ** New min smooth loss: 2.3004 at iter 6699 **\n",
            "  ** New min smooth loss: 2.3003 at iter 6700 **\n",
            "Iter: 6700/100000, Smooth Loss: 2.3003, Min Smooth Loss: 2.3003 (at iter 6700), Time: 33.08s\n",
            "  ** New min smooth loss: 2.2999 at iter 6701 **\n",
            "  ** New min smooth loss: 2.2998 at iter 6702 **\n",
            "  ** New min smooth loss: 2.2998 at iter 6703 **\n",
            "  ** New min smooth loss: 2.2998 at iter 6705 **\n",
            "  ** New min smooth loss: 2.2994 at iter 6706 **\n",
            "  ** New min smooth loss: 2.2993 at iter 6707 **\n",
            "  ** New min smooth loss: 2.2991 at iter 6709 **\n",
            "  ** New min smooth loss: 2.2991 at iter 6710 **\n",
            "  ** New min smooth loss: 2.2990 at iter 6713 **\n",
            "  ** New min smooth loss: 2.2988 at iter 6716 **\n",
            "  ** New min smooth loss: 2.2984 at iter 6717 **\n",
            "  ** New min smooth loss: 2.2980 at iter 6718 **\n",
            "  ** New min smooth loss: 2.2978 at iter 6719 **\n",
            "  ** New min smooth loss: 2.2975 at iter 6720 **\n",
            "  ** New min smooth loss: 2.2975 at iter 6722 **\n",
            "  ** New min smooth loss: 2.2975 at iter 6728 **\n",
            "  ** New min smooth loss: 2.2974 at iter 6729 **\n",
            "  ** New min smooth loss: 2.2966 at iter 6730 **\n",
            "  ** New min smooth loss: 2.2961 at iter 6731 **\n",
            "  ** New min smooth loss: 2.2955 at iter 6732 **\n",
            "  ** New min smooth loss: 2.2950 at iter 6734 **\n",
            "  ** New min smooth loss: 2.2948 at iter 6735 **\n",
            "  ** New min smooth loss: 2.2946 at iter 6737 **\n",
            "  ** New min smooth loss: 2.2944 at iter 6738 **\n",
            "  ** New min smooth loss: 2.2939 at iter 6739 **\n",
            "  ** New min smooth loss: 2.2938 at iter 6740 **\n",
            "  ** New min smooth loss: 2.2935 at iter 6741 **\n",
            "  ** New min smooth loss: 2.2931 at iter 6742 **\n",
            "  ** New min smooth loss: 2.2924 at iter 6743 **\n",
            "  ** New min smooth loss: 2.2920 at iter 6744 **\n",
            "  ** New min smooth loss: 2.2919 at iter 6747 **\n",
            "  ** New min smooth loss: 2.2916 at iter 6750 **\n",
            "  ** New min smooth loss: 2.2908 at iter 6752 **\n",
            "  ** New min smooth loss: 2.2905 at iter 6753 **\n",
            "  ** New min smooth loss: 2.2905 at iter 6754 **\n",
            "  ** New min smooth loss: 2.2904 at iter 6755 **\n",
            "  ** New min smooth loss: 2.2899 at iter 6756 **\n",
            "  ** New min smooth loss: 2.2893 at iter 6757 **\n",
            "  ** New min smooth loss: 2.2887 at iter 6758 **\n",
            "  ** New min smooth loss: 2.2884 at iter 6759 **\n",
            "  ** New min smooth loss: 2.2884 at iter 6761 **\n",
            "  ** New min smooth loss: 2.2878 at iter 6762 **\n",
            "  ** New min smooth loss: 2.2876 at iter 6763 **\n",
            "  ** New min smooth loss: 2.2869 at iter 6764 **\n",
            "  ** New min smooth loss: 2.2864 at iter 6765 **\n",
            "  ** New min smooth loss: 2.2859 at iter 6766 **\n",
            "  ** New min smooth loss: 2.2852 at iter 6767 **\n",
            "  ** New min smooth loss: 2.2852 at iter 6776 **\n",
            "  ** New min smooth loss: 2.2846 at iter 6778 **\n",
            "  ** New min smooth loss: 2.2838 at iter 6779 **\n",
            "  ** New min smooth loss: 2.2835 at iter 6790 **\n",
            "  ** New min smooth loss: 2.2833 at iter 6792 **\n",
            "  ** New min smooth loss: 2.2832 at iter 6795 **\n",
            "  ** New min smooth loss: 2.2828 at iter 6796 **\n",
            "  ** New min smooth loss: 2.2824 at iter 6797 **\n",
            "  ** New min smooth loss: 2.2823 at iter 6798 **\n",
            "  ** New min smooth loss: 2.2818 at iter 6799 **\n",
            "Iter: 6800/100000, Smooth Loss: 2.2818, Min Smooth Loss: 2.2818 (at iter 6799), Time: 33.51s\n",
            "  ** New min smooth loss: 2.2817 at iter 6801 **\n",
            "  ** New min smooth loss: 2.2810 at iter 6803 **\n",
            "  ** New min smooth loss: 2.2805 at iter 6804 **\n",
            "  ** New min smooth loss: 2.2801 at iter 6805 **\n",
            "  ** New min smooth loss: 2.2799 at iter 6809 **\n",
            "  ** New min smooth loss: 2.2799 at iter 6812 **\n",
            "  ** New min smooth loss: 2.2791 at iter 6813 **\n",
            "  ** New min smooth loss: 2.2785 at iter 6814 **\n",
            "  ** New min smooth loss: 2.2782 at iter 6817 **\n",
            "  ** New min smooth loss: 2.2781 at iter 6818 **\n",
            "  ** New min smooth loss: 2.2778 at iter 6819 **\n",
            "  ** New min smooth loss: 2.2776 at iter 6820 **\n",
            "  ** New min smooth loss: 2.2774 at iter 6821 **\n",
            "  ** New min smooth loss: 2.2771 at iter 6822 **\n",
            "  ** New min smooth loss: 2.2769 at iter 6823 **\n",
            "  ** New min smooth loss: 2.2769 at iter 6824 **\n",
            "  ** New min smooth loss: 2.2767 at iter 6825 **\n",
            "  ** New min smooth loss: 2.2762 at iter 6826 **\n",
            "  ** New min smooth loss: 2.2758 at iter 6827 **\n",
            "  ** New min smooth loss: 2.2753 at iter 6828 **\n",
            "  ** New min smooth loss: 2.2750 at iter 6829 **\n",
            "  ** New min smooth loss: 2.2746 at iter 6830 **\n",
            "  ** New min smooth loss: 2.2742 at iter 6831 **\n",
            "  ** New min smooth loss: 2.2741 at iter 6832 **\n",
            "  ** New min smooth loss: 2.2738 at iter 6833 **\n",
            "  ** New min smooth loss: 2.2732 at iter 6834 **\n",
            "  ** New min smooth loss: 2.2729 at iter 6835 **\n",
            "  ** New min smooth loss: 2.2727 at iter 6836 **\n",
            "  ** New min smooth loss: 2.2726 at iter 6837 **\n",
            "  ** New min smooth loss: 2.2722 at iter 6839 **\n",
            "  ** New min smooth loss: 2.2720 at iter 6840 **\n",
            "  ** New min smooth loss: 2.2713 at iter 6841 **\n",
            "  ** New min smooth loss: 2.2713 at iter 6842 **\n",
            "  ** New min smooth loss: 2.2711 at iter 6845 **\n",
            "  ** New min smooth loss: 2.2711 at iter 6846 **\n",
            "  ** New min smooth loss: 2.2707 at iter 6847 **\n",
            "  ** New min smooth loss: 2.2705 at iter 6848 **\n",
            "  ** New min smooth loss: 2.2700 at iter 6849 **\n",
            "  ** New min smooth loss: 2.2698 at iter 6850 **\n",
            "  ** New min smooth loss: 2.2694 at iter 6851 **\n",
            "  ** New min smooth loss: 2.2691 at iter 6852 **\n",
            "  ** New min smooth loss: 2.2687 at iter 6853 **\n",
            "  ** New min smooth loss: 2.2685 at iter 6854 **\n",
            "  ** New min smooth loss: 2.2684 at iter 6855 **\n",
            "  ** New min smooth loss: 2.2678 at iter 6889 **\n",
            "  ** New min smooth loss: 2.2676 at iter 6890 **\n",
            "  ** New min smooth loss: 2.2671 at iter 6891 **\n",
            "  ** New min smooth loss: 2.2667 at iter 6892 **\n",
            "  ** New min smooth loss: 2.2661 at iter 6893 **\n",
            "  ** New min smooth loss: 2.2660 at iter 6894 **\n",
            "  ** New min smooth loss: 2.2656 at iter 6895 **\n",
            "  ** New min smooth loss: 2.2651 at iter 6896 **\n",
            "  ** New min smooth loss: 2.2651 at iter 6897 **\n",
            "  ** New min smooth loss: 2.2646 at iter 6898 **\n",
            "  ** New min smooth loss: 2.2643 at iter 6899 **\n",
            "  ** New min smooth loss: 2.2642 at iter 6900 **\n",
            "Iter: 6900/100000, Smooth Loss: 2.2642, Min Smooth Loss: 2.2642 (at iter 6900), Time: 33.96s\n",
            "  ** New min smooth loss: 2.2640 at iter 6901 **\n",
            "  ** New min smooth loss: 2.2635 at iter 6902 **\n",
            "  ** New min smooth loss: 2.2634 at iter 6904 **\n",
            "  ** New min smooth loss: 2.2630 at iter 6905 **\n",
            "  ** New min smooth loss: 2.2625 at iter 6906 **\n",
            "  ** New min smooth loss: 2.2625 at iter 6907 **\n",
            "  ** New min smooth loss: 2.2620 at iter 6908 **\n",
            "  ** New min smooth loss: 2.2619 at iter 6910 **\n",
            "  ** New min smooth loss: 2.2615 at iter 6911 **\n",
            "  ** New min smooth loss: 2.2611 at iter 6912 **\n",
            "  ** New min smooth loss: 2.2610 at iter 6913 **\n",
            "  ** New min smooth loss: 2.2609 at iter 6914 **\n",
            "  ** New min smooth loss: 2.2604 at iter 6915 **\n",
            "  ** New min smooth loss: 2.2602 at iter 6916 **\n",
            "  ** New min smooth loss: 2.2597 at iter 6917 **\n",
            "  ** New min smooth loss: 2.2596 at iter 6918 **\n",
            "  ** New min smooth loss: 2.2589 at iter 6919 **\n",
            "  ** New min smooth loss: 2.2587 at iter 6920 **\n",
            "  ** New min smooth loss: 2.2585 at iter 6921 **\n",
            "  ** New min smooth loss: 2.2582 at iter 6922 **\n",
            "  ** New min smooth loss: 2.2580 at iter 6923 **\n",
            "  ** New min smooth loss: 2.2576 at iter 6925 **\n",
            "  ** New min smooth loss: 2.2576 at iter 6927 **\n",
            "  ** New min smooth loss: 2.2575 at iter 6928 **\n",
            "  ** New min smooth loss: 2.2568 at iter 6929 **\n",
            "  ** New min smooth loss: 2.2564 at iter 6930 **\n",
            "  ** New min smooth loss: 2.2563 at iter 6938 **\n",
            "  ** New min smooth loss: 2.2562 at iter 6941 **\n",
            "  ** New min smooth loss: 2.2561 at iter 6944 **\n",
            "  ** New min smooth loss: 2.2558 at iter 6949 **\n",
            "  ** New min smooth loss: 2.2556 at iter 6950 **\n",
            "  ** New min smooth loss: 2.2553 at iter 6951 **\n",
            "  ** New min smooth loss: 2.2551 at iter 6952 **\n",
            "  ** New min smooth loss: 2.2547 at iter 6954 **\n",
            "  ** New min smooth loss: 2.2545 at iter 6956 **\n",
            "  ** New min smooth loss: 2.2542 at iter 6957 **\n",
            "  ** New min smooth loss: 2.2539 at iter 6961 **\n",
            "  ** New min smooth loss: 2.2538 at iter 6962 **\n",
            "  ** New min smooth loss: 2.2535 at iter 6963 **\n",
            "  ** New min smooth loss: 2.2530 at iter 6964 **\n",
            "  ** New min smooth loss: 2.2527 at iter 6966 **\n",
            "  ** New min smooth loss: 2.2521 at iter 6968 **\n",
            "  ** New min smooth loss: 2.2519 at iter 6969 **\n",
            "  ** New min smooth loss: 2.2517 at iter 6970 **\n",
            "  ** New min smooth loss: 2.2511 at iter 6971 **\n",
            "  ** New min smooth loss: 2.2509 at iter 6973 **\n",
            "  ** New min smooth loss: 2.2503 at iter 6974 **\n",
            "  ** New min smooth loss: 2.2498 at iter 6975 **\n",
            "  ** New min smooth loss: 2.2494 at iter 6976 **\n",
            "  ** New min smooth loss: 2.2490 at iter 6977 **\n",
            "  ** New min smooth loss: 2.2484 at iter 6978 **\n",
            "  ** New min smooth loss: 2.2484 at iter 6979 **\n",
            "  ** New min smooth loss: 2.2481 at iter 6980 **\n",
            "  ** New min smooth loss: 2.2481 at iter 6981 **\n",
            "  ** New min smooth loss: 2.2480 at iter 6982 **\n",
            "  ** New min smooth loss: 2.2476 at iter 6983 **\n",
            "  ** New min smooth loss: 2.2473 at iter 6984 **\n",
            "  ** New min smooth loss: 2.2468 at iter 6985 **\n",
            "  ** New min smooth loss: 2.2461 at iter 6987 **\n",
            "  ** New min smooth loss: 2.2459 at iter 6991 **\n",
            "  ** New min smooth loss: 2.2455 at iter 6994 **\n",
            "  ** New min smooth loss: 2.2454 at iter 6997 **\n",
            "  ** New min smooth loss: 2.2454 at iter 6998 **\n",
            "  ** New min smooth loss: 2.2450 at iter 6999 **\n",
            "Iter: 7000/100000, Smooth Loss: 2.2452, Min Smooth Loss: 2.2450 (at iter 6999), Time: 34.38s\n",
            "  ** New min smooth loss: 2.2449 at iter 7002 **\n",
            "  ** New min smooth loss: 2.2443 at iter 7003 **\n",
            "  ** New min smooth loss: 2.2439 at iter 7004 **\n",
            "  ** New min smooth loss: 2.2436 at iter 7005 **\n",
            "  ** New min smooth loss: 2.2432 at iter 7007 **\n",
            "  ** New min smooth loss: 2.2430 at iter 7008 **\n",
            "  ** New min smooth loss: 2.2427 at iter 7013 **\n",
            "  ** New min smooth loss: 2.2426 at iter 7014 **\n",
            "  ** New min smooth loss: 2.2420 at iter 7016 **\n",
            "  ** New min smooth loss: 2.2415 at iter 7023 **\n",
            "  ** New min smooth loss: 2.2412 at iter 7028 **\n",
            "  ** New min smooth loss: 2.2412 at iter 7029 **\n",
            "  ** New min smooth loss: 2.2409 at iter 7030 **\n",
            "  ** New min smooth loss: 2.2404 at iter 7031 **\n",
            "  ** New min smooth loss: 2.2400 at iter 7057 **\n",
            "  ** New min smooth loss: 2.2399 at iter 7060 **\n",
            "  ** New min smooth loss: 2.2396 at iter 7061 **\n",
            "  ** New min smooth loss: 2.2391 at iter 7062 **\n",
            "  ** New min smooth loss: 2.2387 at iter 7063 **\n",
            "  ** New min smooth loss: 2.2386 at iter 7066 **\n",
            "  ** New min smooth loss: 2.2378 at iter 7067 **\n",
            "  ** New min smooth loss: 2.2370 at iter 7068 **\n",
            "  ** New min smooth loss: 2.2369 at iter 7071 **\n",
            "  ** New min smooth loss: 2.2367 at iter 7073 **\n",
            "  ** New min smooth loss: 2.2366 at iter 7076 **\n",
            "  ** New min smooth loss: 2.2365 at iter 7077 **\n",
            "  ** New min smooth loss: 2.2362 at iter 7078 **\n",
            "  ** New min smooth loss: 2.2360 at iter 7079 **\n",
            "  ** New min smooth loss: 2.2353 at iter 7080 **\n",
            "  ** New min smooth loss: 2.2352 at iter 7081 **\n",
            "  ** New min smooth loss: 2.2346 at iter 7082 **\n",
            "  ** New min smooth loss: 2.2344 at iter 7083 **\n",
            "  ** New min smooth loss: 2.2339 at iter 7084 **\n",
            "  ** New min smooth loss: 2.2331 at iter 7085 **\n",
            "  ** New min smooth loss: 2.2321 at iter 7086 **\n",
            "  ** New min smooth loss: 2.2315 at iter 7088 **\n",
            "  ** New min smooth loss: 2.2312 at iter 7089 **\n",
            "  ** New min smooth loss: 2.2305 at iter 7090 **\n",
            "  ** New min smooth loss: 2.2303 at iter 7091 **\n",
            "  ** New min smooth loss: 2.2300 at iter 7092 **\n",
            "  ** New min smooth loss: 2.2299 at iter 7096 **\n",
            "  ** New min smooth loss: 2.2294 at iter 7097 **\n",
            "  ** New min smooth loss: 2.2292 at iter 7098 **\n",
            "  ** New min smooth loss: 2.2286 at iter 7099 **\n",
            "Iter: 7100/100000, Smooth Loss: 2.2288, Min Smooth Loss: 2.2286 (at iter 7099), Time: 34.81s\n",
            "  ** New min smooth loss: 2.2281 at iter 7102 **\n",
            "  ** New min smooth loss: 2.2278 at iter 7103 **\n",
            "  ** New min smooth loss: 2.2274 at iter 7108 **\n",
            "  ** New min smooth loss: 2.2273 at iter 7109 **\n",
            "  ** New min smooth loss: 2.2271 at iter 7117 **\n",
            "  ** New min smooth loss: 2.2264 at iter 7118 **\n",
            "  ** New min smooth loss: 2.2262 at iter 7119 **\n",
            "  ** New min smooth loss: 2.2262 at iter 7125 **\n",
            "  ** New min smooth loss: 2.2261 at iter 7126 **\n",
            "  ** New min smooth loss: 2.2257 at iter 7132 **\n",
            "  ** New min smooth loss: 2.2254 at iter 7134 **\n",
            "  ** New min smooth loss: 2.2248 at iter 7136 **\n",
            "  ** New min smooth loss: 2.2243 at iter 7137 **\n",
            "  ** New min smooth loss: 2.2243 at iter 7140 **\n",
            "  ** New min smooth loss: 2.2237 at iter 7141 **\n",
            "  ** New min smooth loss: 2.2232 at iter 7142 **\n",
            "  ** New min smooth loss: 2.2226 at iter 7143 **\n",
            "  ** New min smooth loss: 2.2221 at iter 7144 **\n",
            "  ** New min smooth loss: 2.2216 at iter 7146 **\n",
            "  ** New min smooth loss: 2.2211 at iter 7147 **\n",
            "  ** New min smooth loss: 2.2204 at iter 7148 **\n",
            "  ** New min smooth loss: 2.2199 at iter 7149 **\n",
            "  ** New min smooth loss: 2.2196 at iter 7151 **\n",
            "  ** New min smooth loss: 2.2191 at iter 7154 **\n",
            "  ** New min smooth loss: 2.2186 at iter 7155 **\n",
            "  ** New min smooth loss: 2.2184 at iter 7156 **\n",
            "  ** New min smooth loss: 2.2182 at iter 7157 **\n",
            "  ** New min smooth loss: 2.2178 at iter 7158 **\n",
            "  ** New min smooth loss: 2.2177 at iter 7159 **\n",
            "  ** New min smooth loss: 2.2174 at iter 7160 **\n",
            "  ** New min smooth loss: 2.2168 at iter 7161 **\n",
            "  ** New min smooth loss: 2.2164 at iter 7162 **\n",
            "  ** New min smooth loss: 2.2160 at iter 7163 **\n",
            "  ** New min smooth loss: 2.2158 at iter 7165 **\n",
            "  ** New min smooth loss: 2.2158 at iter 7167 **\n",
            "  ** New min smooth loss: 2.2155 at iter 7169 **\n",
            "  ** New min smooth loss: 2.2155 at iter 7170 **\n",
            "  ** New min smooth loss: 2.2153 at iter 7171 **\n",
            "  ** New min smooth loss: 2.2153 at iter 7172 **\n",
            "  ** New min smooth loss: 2.2149 at iter 7173 **\n",
            "  ** New min smooth loss: 2.2145 at iter 7174 **\n",
            "  ** New min smooth loss: 2.2144 at iter 7175 **\n",
            "  ** New min smooth loss: 2.2140 at iter 7176 **\n",
            "  ** New min smooth loss: 2.2134 at iter 7177 **\n",
            "  ** New min smooth loss: 2.2124 at iter 7178 **\n",
            "  ** New min smooth loss: 2.2123 at iter 7179 **\n",
            "  ** New min smooth loss: 2.2118 at iter 7180 **\n",
            "  ** New min smooth loss: 2.2112 at iter 7181 **\n",
            "  ** New min smooth loss: 2.2108 at iter 7182 **\n",
            "  ** New min smooth loss: 2.2107 at iter 7184 **\n",
            "  ** New min smooth loss: 2.2107 at iter 7185 **\n",
            "  ** New min smooth loss: 2.2103 at iter 7186 **\n",
            "  ** New min smooth loss: 2.2098 at iter 7188 **\n",
            "  ** New min smooth loss: 2.2091 at iter 7189 **\n",
            "  ** New min smooth loss: 2.2088 at iter 7190 **\n",
            "  ** New min smooth loss: 2.2087 at iter 7191 **\n",
            "  ** New min smooth loss: 2.2082 at iter 7193 **\n",
            "  ** New min smooth loss: 2.2076 at iter 7195 **\n",
            "  ** New min smooth loss: 2.2075 at iter 7196 **\n",
            "  ** New min smooth loss: 2.2069 at iter 7197 **\n",
            "  ** New min smooth loss: 2.2065 at iter 7198 **\n",
            "  ** New min smooth loss: 2.2062 at iter 7199 **\n",
            "  ** New min smooth loss: 2.2061 at iter 7200 **\n",
            "Iter: 7200/100000, Smooth Loss: 2.2061, Min Smooth Loss: 2.2061 (at iter 7200), Time: 35.24s\n",
            "  ** New min smooth loss: 2.2057 at iter 7201 **\n",
            "  ** New min smooth loss: 2.2053 at iter 7203 **\n",
            "  ** New min smooth loss: 2.2050 at iter 7204 **\n",
            "  ** New min smooth loss: 2.2046 at iter 7205 **\n",
            "  ** New min smooth loss: 2.2044 at iter 7206 **\n",
            "  ** New min smooth loss: 2.2042 at iter 7207 **\n",
            "  ** New min smooth loss: 2.2040 at iter 7208 **\n",
            "  ** New min smooth loss: 2.2040 at iter 7209 **\n",
            "  ** New min smooth loss: 2.2037 at iter 7210 **\n",
            "  ** New min smooth loss: 2.2036 at iter 7211 **\n",
            "  ** New min smooth loss: 2.2035 at iter 7212 **\n",
            "  ** New min smooth loss: 2.2030 at iter 7213 **\n",
            "  ** New min smooth loss: 2.2024 at iter 7214 **\n",
            "  ** New min smooth loss: 2.2019 at iter 7215 **\n",
            "  ** New min smooth loss: 2.2018 at iter 7216 **\n",
            "  ** New min smooth loss: 2.2014 at iter 7217 **\n",
            "  ** New min smooth loss: 2.2014 at iter 7219 **\n",
            "  ** New min smooth loss: 2.2013 at iter 7223 **\n",
            "  ** New min smooth loss: 2.2011 at iter 7224 **\n",
            "  ** New min smooth loss: 2.2008 at iter 7225 **\n",
            "  ** New min smooth loss: 2.2006 at iter 7226 **\n",
            "  ** New min smooth loss: 2.2003 at iter 7227 **\n",
            "  ** New min smooth loss: 2.2002 at iter 7228 **\n",
            "  ** New min smooth loss: 2.2000 at iter 7232 **\n",
            "  ** New min smooth loss: 2.1993 at iter 7233 **\n",
            "  ** New min smooth loss: 2.1991 at iter 7234 **\n",
            "  ** New min smooth loss: 2.1987 at iter 7235 **\n",
            "  ** New min smooth loss: 2.1978 at iter 7237 **\n",
            "  ** New min smooth loss: 2.1973 at iter 7238 **\n",
            "  ** New min smooth loss: 2.1971 at iter 7240 **\n",
            "  ** New min smooth loss: 2.1970 at iter 7242 **\n",
            "  ** New min smooth loss: 2.1965 at iter 7243 **\n",
            "  ** New min smooth loss: 2.1961 at iter 7244 **\n",
            "  ** New min smooth loss: 2.1955 at iter 7245 **\n",
            "  ** New min smooth loss: 2.1950 at iter 7246 **\n",
            "  ** New min smooth loss: 2.1949 at iter 7247 **\n",
            "  ** New min smooth loss: 2.1943 at iter 7249 **\n",
            "  ** New min smooth loss: 2.1941 at iter 7250 **\n",
            "  ** New min smooth loss: 2.1939 at iter 7253 **\n",
            "  ** New min smooth loss: 2.1936 at iter 7254 **\n",
            "  ** New min smooth loss: 2.1934 at iter 7255 **\n",
            "  ** New min smooth loss: 2.1928 at iter 7256 **\n",
            "  ** New min smooth loss: 2.1922 at iter 7257 **\n",
            "  ** New min smooth loss: 2.1919 at iter 7258 **\n",
            "  ** New min smooth loss: 2.1912 at iter 7259 **\n",
            "  ** New min smooth loss: 2.1910 at iter 7260 **\n",
            "  ** New min smooth loss: 2.1908 at iter 7264 **\n",
            "  ** New min smooth loss: 2.1902 at iter 7265 **\n",
            "  ** New min smooth loss: 2.1896 at iter 7266 **\n",
            "  ** New min smooth loss: 2.1894 at iter 7267 **\n",
            "  ** New min smooth loss: 2.1887 at iter 7268 **\n",
            "  ** New min smooth loss: 2.1881 at iter 7269 **\n",
            "  ** New min smooth loss: 2.1879 at iter 7270 **\n",
            "  ** New min smooth loss: 2.1878 at iter 7271 **\n",
            "  ** New min smooth loss: 2.1871 at iter 7272 **\n",
            "  ** New min smooth loss: 2.1864 at iter 7273 **\n",
            "  ** New min smooth loss: 2.1859 at iter 7274 **\n",
            "  ** New min smooth loss: 2.1853 at iter 7275 **\n",
            "  ** New min smooth loss: 2.1846 at iter 7276 **\n",
            "  ** New min smooth loss: 2.1840 at iter 7278 **\n",
            "  ** New min smooth loss: 2.1840 at iter 7279 **\n",
            "  ** New min smooth loss: 2.1835 at iter 7280 **\n",
            "  ** New min smooth loss: 2.1833 at iter 7281 **\n",
            "  ** New min smooth loss: 2.1831 at iter 7282 **\n",
            "  ** New min smooth loss: 2.1826 at iter 7283 **\n",
            "  ** New min smooth loss: 2.1824 at iter 7286 **\n",
            "  ** New min smooth loss: 2.1824 at iter 7289 **\n",
            "  ** New min smooth loss: 2.1817 at iter 7290 **\n",
            "  ** New min smooth loss: 2.1814 at iter 7291 **\n",
            "  ** New min smooth loss: 2.1810 at iter 7292 **\n",
            "  ** New min smooth loss: 2.1809 at iter 7293 **\n",
            "  ** New min smooth loss: 2.1806 at iter 7294 **\n",
            "  ** New min smooth loss: 2.1803 at iter 7295 **\n",
            "  ** New min smooth loss: 2.1801 at iter 7296 **\n",
            "  ** New min smooth loss: 2.1800 at iter 7297 **\n",
            "  ** New min smooth loss: 2.1794 at iter 7299 **\n",
            "Iter: 7300/100000, Smooth Loss: 2.1794, Min Smooth Loss: 2.1794 (at iter 7299), Time: 35.67s\n",
            "  ** New min smooth loss: 2.1790 at iter 7306 **\n",
            "  ** New min smooth loss: 2.1789 at iter 7307 **\n",
            "  ** New min smooth loss: 2.1786 at iter 7308 **\n",
            "  ** New min smooth loss: 2.1785 at iter 7309 **\n",
            "  ** New min smooth loss: 2.1783 at iter 7310 **\n",
            "  ** New min smooth loss: 2.1782 at iter 7311 **\n",
            "  ** New min smooth loss: 2.1777 at iter 7312 **\n",
            "  ** New min smooth loss: 2.1771 at iter 7313 **\n",
            "  ** New min smooth loss: 2.1766 at iter 7319 **\n",
            "  ** New min smooth loss: 2.1763 at iter 7320 **\n",
            "  ** New min smooth loss: 2.1761 at iter 7321 **\n",
            "  ** New min smooth loss: 2.1756 at iter 7323 **\n",
            "  ** New min smooth loss: 2.1755 at iter 7324 **\n",
            "  ** New min smooth loss: 2.1751 at iter 7325 **\n",
            "  ** New min smooth loss: 2.1750 at iter 7326 **\n",
            "  ** New min smooth loss: 2.1749 at iter 7327 **\n",
            "  ** New min smooth loss: 2.1746 at iter 7332 **\n",
            "  ** New min smooth loss: 2.1741 at iter 7333 **\n",
            "  ** New min smooth loss: 2.1736 at iter 7335 **\n",
            "  ** New min smooth loss: 2.1736 at iter 7336 **\n",
            "  ** New min smooth loss: 2.1734 at iter 7337 **\n",
            "  ** New min smooth loss: 2.1727 at iter 7338 **\n",
            "  ** New min smooth loss: 2.1722 at iter 7339 **\n",
            "  ** New min smooth loss: 2.1722 at iter 7341 **\n",
            "  ** New min smooth loss: 2.1721 at iter 7351 **\n",
            "  ** New min smooth loss: 2.1721 at iter 7365 **\n",
            "  ** New min smooth loss: 2.1718 at iter 7366 **\n",
            "  ** New min smooth loss: 2.1709 at iter 7367 **\n",
            "  ** New min smooth loss: 2.1705 at iter 7368 **\n",
            "  ** New min smooth loss: 2.1704 at iter 7380 **\n",
            "  ** New min smooth loss: 2.1701 at iter 7381 **\n",
            "  ** New min smooth loss: 2.1697 at iter 7382 **\n",
            "  ** New min smooth loss: 2.1693 at iter 7383 **\n",
            "  ** New min smooth loss: 2.1693 at iter 7384 **\n",
            "  ** New min smooth loss: 2.1692 at iter 7385 **\n",
            "  ** New min smooth loss: 2.1685 at iter 7386 **\n",
            "  ** New min smooth loss: 2.1683 at iter 7387 **\n",
            "  ** New min smooth loss: 2.1682 at iter 7388 **\n",
            "  ** New min smooth loss: 2.1679 at iter 7393 **\n",
            "  ** New min smooth loss: 2.1677 at iter 7394 **\n",
            "  ** New min smooth loss: 2.1673 at iter 7395 **\n",
            "  ** New min smooth loss: 2.1670 at iter 7396 **\n",
            "  ** New min smooth loss: 2.1666 at iter 7397 **\n",
            "  ** New min smooth loss: 2.1663 at iter 7398 **\n",
            "  ** New min smooth loss: 2.1662 at iter 7399 **\n",
            "  ** New min smooth loss: 2.1656 at iter 7400 **\n",
            "Iter: 7400/100000, Smooth Loss: 2.1656, Min Smooth Loss: 2.1656 (at iter 7400), Time: 36.30s\n",
            "  ** New min smooth loss: 2.1653 at iter 7402 **\n",
            "  ** New min smooth loss: 2.1647 at iter 7404 **\n",
            "  ** New min smooth loss: 2.1643 at iter 7405 **\n",
            "  ** New min smooth loss: 2.1638 at iter 7406 **\n",
            "  ** New min smooth loss: 2.1637 at iter 7408 **\n",
            "  ** New min smooth loss: 2.1634 at iter 7409 **\n",
            "  ** New min smooth loss: 2.1629 at iter 7410 **\n",
            "  ** New min smooth loss: 2.1629 at iter 7415 **\n",
            "  ** New min smooth loss: 2.1626 at iter 7416 **\n",
            "  ** New min smooth loss: 2.1626 at iter 7441 **\n",
            "  ** New min smooth loss: 2.1622 at iter 7442 **\n",
            "  ** New min smooth loss: 2.1622 at iter 7443 **\n",
            "  ** New min smooth loss: 2.1619 at iter 7444 **\n",
            "  ** New min smooth loss: 2.1613 at iter 7445 **\n",
            "  ** New min smooth loss: 2.1611 at iter 7450 **\n",
            "  ** New min smooth loss: 2.1608 at iter 7451 **\n",
            "  ** New min smooth loss: 2.1602 at iter 7453 **\n",
            "  ** New min smooth loss: 2.1597 at iter 7454 **\n",
            "  ** New min smooth loss: 2.1591 at iter 7455 **\n",
            "  ** New min smooth loss: 2.1584 at iter 7457 **\n",
            "  ** New min smooth loss: 2.1583 at iter 7460 **\n",
            "  ** New min smooth loss: 2.1582 at iter 7461 **\n",
            "  ** New min smooth loss: 2.1579 at iter 7463 **\n",
            "  ** New min smooth loss: 2.1576 at iter 7464 **\n",
            "  ** New min smooth loss: 2.1572 at iter 7465 **\n",
            "  ** New min smooth loss: 2.1569 at iter 7466 **\n",
            "  ** New min smooth loss: 2.1568 at iter 7467 **\n",
            "  ** New min smooth loss: 2.1567 at iter 7468 **\n",
            "  ** New min smooth loss: 2.1566 at iter 7479 **\n",
            "  ** New min smooth loss: 2.1563 at iter 7480 **\n",
            "  ** New min smooth loss: 2.1562 at iter 7481 **\n",
            "  ** New min smooth loss: 2.1561 at iter 7483 **\n",
            "  ** New min smooth loss: 2.1557 at iter 7486 **\n",
            "  ** New min smooth loss: 2.1557 at iter 7487 **\n",
            "  ** New min smooth loss: 2.1551 at iter 7488 **\n",
            "  ** New min smooth loss: 2.1545 at iter 7489 **\n",
            "  ** New min smooth loss: 2.1539 at iter 7490 **\n",
            "  ** New min smooth loss: 2.1535 at iter 7491 **\n",
            "  ** New min smooth loss: 2.1532 at iter 7492 **\n",
            "  ** New min smooth loss: 2.1524 at iter 7493 **\n",
            "  ** New min smooth loss: 2.1520 at iter 7494 **\n",
            "  ** New min smooth loss: 2.1515 at iter 7498 **\n",
            "  ** New min smooth loss: 2.1515 at iter 7500 **\n",
            "Iter: 7500/100000, Smooth Loss: 2.1515, Min Smooth Loss: 2.1515 (at iter 7500), Time: 36.94s\n",
            "  ** New min smooth loss: 2.1510 at iter 7501 **\n",
            "  ** New min smooth loss: 2.1509 at iter 7503 **\n",
            "  ** New min smooth loss: 2.1507 at iter 7508 **\n",
            "  ** New min smooth loss: 2.1503 at iter 7510 **\n",
            "  ** New min smooth loss: 2.1498 at iter 7511 **\n",
            "  ** New min smooth loss: 2.1495 at iter 7512 **\n",
            "  ** New min smooth loss: 2.1494 at iter 7513 **\n",
            "  ** New min smooth loss: 2.1493 at iter 7519 **\n",
            "  ** New min smooth loss: 2.1489 at iter 7520 **\n",
            "  ** New min smooth loss: 2.1488 at iter 7524 **\n",
            "  ** New min smooth loss: 2.1486 at iter 7525 **\n",
            "  ** New min smooth loss: 2.1484 at iter 7526 **\n",
            "  ** New min smooth loss: 2.1483 at iter 7535 **\n",
            "  ** New min smooth loss: 2.1481 at iter 7537 **\n",
            "  ** New min smooth loss: 2.1479 at iter 7538 **\n",
            "  ** New min smooth loss: 2.1474 at iter 7539 **\n",
            "  ** New min smooth loss: 2.1474 at iter 7540 **\n",
            "  ** New min smooth loss: 2.1469 at iter 7541 **\n",
            "  ** New min smooth loss: 2.1468 at iter 7542 **\n",
            "  ** New min smooth loss: 2.1464 at iter 7543 **\n",
            "  ** New min smooth loss: 2.1463 at iter 7544 **\n",
            "  ** New min smooth loss: 2.1461 at iter 7545 **\n",
            "  ** New min smooth loss: 2.1458 at iter 7548 **\n",
            "  ** New min smooth loss: 2.1456 at iter 7549 **\n",
            "  ** New min smooth loss: 2.1453 at iter 7550 **\n",
            "  ** New min smooth loss: 2.1451 at iter 7551 **\n",
            "  ** New min smooth loss: 2.1447 at iter 7552 **\n",
            "  ** New min smooth loss: 2.1444 at iter 7553 **\n",
            "  ** New min smooth loss: 2.1441 at iter 7555 **\n",
            "  ** New min smooth loss: 2.1436 at iter 7556 **\n",
            "  ** New min smooth loss: 2.1432 at iter 7557 **\n",
            "  ** New min smooth loss: 2.1430 at iter 7558 **\n",
            "  ** New min smooth loss: 2.1421 at iter 7559 **\n",
            "  ** New min smooth loss: 2.1419 at iter 7560 **\n",
            "  ** New min smooth loss: 2.1419 at iter 7563 **\n",
            "  ** New min smooth loss: 2.1414 at iter 7564 **\n",
            "  ** New min smooth loss: 2.1409 at iter 7565 **\n",
            "Iter: 7600/100000, Smooth Loss: 2.1424, Min Smooth Loss: 2.1409 (at iter 7565), Time: 37.60s\n",
            "  ** New min smooth loss: 2.1405 at iter 7607 **\n",
            "  ** New min smooth loss: 2.1404 at iter 7608 **\n",
            "  ** New min smooth loss: 2.1399 at iter 7610 **\n",
            "  ** New min smooth loss: 2.1397 at iter 7615 **\n",
            "  ** New min smooth loss: 2.1395 at iter 7619 **\n",
            "  ** New min smooth loss: 2.1389 at iter 7620 **\n",
            "  ** New min smooth loss: 2.1385 at iter 7621 **\n",
            "  ** New min smooth loss: 2.1382 at iter 7627 **\n",
            "  ** New min smooth loss: 2.1379 at iter 7628 **\n",
            "  ** New min smooth loss: 2.1376 at iter 7629 **\n",
            "  ** New min smooth loss: 2.1373 at iter 7630 **\n",
            "  ** New min smooth loss: 2.1370 at iter 7631 **\n",
            "  ** New min smooth loss: 2.1363 at iter 7632 **\n",
            "  ** New min smooth loss: 2.1359 at iter 7635 **\n",
            "  ** New min smooth loss: 2.1354 at iter 7636 **\n",
            "  ** New min smooth loss: 2.1351 at iter 7637 **\n",
            "  ** New min smooth loss: 2.1349 at iter 7638 **\n",
            "  ** New min smooth loss: 2.1347 at iter 7639 **\n",
            "  ** New min smooth loss: 2.1347 at iter 7640 **\n",
            "  ** New min smooth loss: 2.1342 at iter 7641 **\n",
            "  ** New min smooth loss: 2.1341 at iter 7642 **\n",
            "  ** New min smooth loss: 2.1335 at iter 7647 **\n",
            "  ** New min smooth loss: 2.1331 at iter 7648 **\n",
            "  ** New min smooth loss: 2.1323 at iter 7649 **\n",
            "  ** New min smooth loss: 2.1320 at iter 7650 **\n",
            "  ** New min smooth loss: 2.1319 at iter 7651 **\n",
            "  ** New min smooth loss: 2.1316 at iter 7652 **\n",
            "  ** New min smooth loss: 2.1315 at iter 7653 **\n",
            "  ** New min smooth loss: 2.1314 at iter 7657 **\n",
            "  ** New min smooth loss: 2.1314 at iter 7658 **\n",
            "  ** New min smooth loss: 2.1311 at iter 7659 **\n",
            "  ** New min smooth loss: 2.1304 at iter 7660 **\n",
            "  ** New min smooth loss: 2.1300 at iter 7662 **\n",
            "  ** New min smooth loss: 2.1299 at iter 7668 **\n",
            "  ** New min smooth loss: 2.1295 at iter 7669 **\n",
            "  ** New min smooth loss: 2.1290 at iter 7673 **\n",
            "  ** New min smooth loss: 2.1288 at iter 7674 **\n",
            "  ** New min smooth loss: 2.1285 at iter 7675 **\n",
            "  ** New min smooth loss: 2.1281 at iter 7678 **\n",
            "  ** New min smooth loss: 2.1277 at iter 7679 **\n",
            "  ** New min smooth loss: 2.1272 at iter 7680 **\n",
            "  ** New min smooth loss: 2.1271 at iter 7681 **\n",
            "  ** New min smooth loss: 2.1270 at iter 7683 **\n",
            "  ** New min smooth loss: 2.1267 at iter 7686 **\n",
            "  ** New min smooth loss: 2.1260 at iter 7687 **\n",
            "  ** New min smooth loss: 2.1256 at iter 7688 **\n",
            "  ** New min smooth loss: 2.1254 at iter 7689 **\n",
            "  ** New min smooth loss: 2.1245 at iter 7690 **\n",
            "  ** New min smooth loss: 2.1242 at iter 7691 **\n",
            "  ** New min smooth loss: 2.1239 at iter 7694 **\n",
            "  ** New min smooth loss: 2.1238 at iter 7695 **\n",
            "  ** New min smooth loss: 2.1232 at iter 7696 **\n",
            "  ** New min smooth loss: 2.1231 at iter 7697 **\n",
            "  ** New min smooth loss: 2.1230 at iter 7700 **\n",
            "Iter: 7700/100000, Smooth Loss: 2.1230, Min Smooth Loss: 2.1230 (at iter 7700), Time: 38.25s\n",
            "  ** New min smooth loss: 2.1229 at iter 7701 **\n",
            "  ** New min smooth loss: 2.1225 at iter 7702 **\n",
            "  ** New min smooth loss: 2.1219 at iter 7703 **\n",
            "  ** New min smooth loss: 2.1211 at iter 7704 **\n",
            "  ** New min smooth loss: 2.1208 at iter 7705 **\n",
            "  ** New min smooth loss: 2.1205 at iter 7706 **\n",
            "  ** New min smooth loss: 2.1203 at iter 7707 **\n",
            "  ** New min smooth loss: 2.1198 at iter 7708 **\n",
            "  ** New min smooth loss: 2.1195 at iter 7709 **\n",
            "  ** New min smooth loss: 2.1192 at iter 7714 **\n",
            "  ** New min smooth loss: 2.1188 at iter 7715 **\n",
            "  ** New min smooth loss: 2.1184 at iter 7716 **\n",
            "  ** New min smooth loss: 2.1182 at iter 7717 **\n",
            "  ** New min smooth loss: 2.1181 at iter 7719 **\n",
            "  ** New min smooth loss: 2.1179 at iter 7720 **\n",
            "  ** New min smooth loss: 2.1173 at iter 7721 **\n",
            "  ** New min smooth loss: 2.1169 at iter 7722 **\n",
            "  ** New min smooth loss: 2.1167 at iter 7725 **\n",
            "  ** New min smooth loss: 2.1166 at iter 7726 **\n",
            "  ** New min smooth loss: 2.1163 at iter 7727 **\n",
            "  ** New min smooth loss: 2.1161 at iter 7741 **\n",
            "  ** New min smooth loss: 2.1160 at iter 7743 **\n",
            "  ** New min smooth loss: 2.1159 at iter 7768 **\n",
            "  ** New min smooth loss: 2.1157 at iter 7769 **\n",
            "  ** New min smooth loss: 2.1150 at iter 7770 **\n",
            "  ** New min smooth loss: 2.1148 at iter 7771 **\n",
            "  ** New min smooth loss: 2.1147 at iter 7772 **\n",
            "  ** New min smooth loss: 2.1140 at iter 7773 **\n",
            "  ** New min smooth loss: 2.1137 at iter 7774 **\n",
            "  ** New min smooth loss: 2.1137 at iter 7775 **\n",
            "  ** New min smooth loss: 2.1134 at iter 7777 **\n",
            "  ** New min smooth loss: 2.1131 at iter 7778 **\n",
            "  ** New min smooth loss: 2.1130 at iter 7786 **\n",
            "  ** New min smooth loss: 2.1129 at iter 7792 **\n",
            "  ** New min smooth loss: 2.1124 at iter 7793 **\n",
            "  ** New min smooth loss: 2.1121 at iter 7796 **\n",
            "  ** New min smooth loss: 2.1116 at iter 7797 **\n",
            "  ** New min smooth loss: 2.1113 at iter 7798 **\n",
            "  ** New min smooth loss: 2.1111 at iter 7799 **\n",
            "Iter: 7800/100000, Smooth Loss: 2.1113, Min Smooth Loss: 2.1111 (at iter 7799), Time: 38.88s\n",
            "  ** New min smooth loss: 2.1110 at iter 7802 **\n",
            "  ** New min smooth loss: 2.1109 at iter 7806 **\n",
            "  ** New min smooth loss: 2.1107 at iter 7807 **\n",
            "  ** New min smooth loss: 2.1105 at iter 7808 **\n",
            "  ** New min smooth loss: 2.1105 at iter 7811 **\n",
            "  ** New min smooth loss: 2.1105 at iter 7813 **\n",
            "  ** New min smooth loss: 2.1100 at iter 7814 **\n",
            "  ** New min smooth loss: 2.1098 at iter 7815 **\n",
            "  ** New min smooth loss: 2.1095 at iter 7825 **\n",
            "  ** New min smooth loss: 2.1088 at iter 7826 **\n",
            "  ** New min smooth loss: 2.1084 at iter 7827 **\n",
            "  ** New min smooth loss: 2.1081 at iter 7828 **\n",
            "  ** New min smooth loss: 2.1079 at iter 7829 **\n",
            "  ** New min smooth loss: 2.1076 at iter 7831 **\n",
            "  ** New min smooth loss: 2.1074 at iter 7834 **\n",
            "  ** New min smooth loss: 2.1069 at iter 7835 **\n",
            "  ** New min smooth loss: 2.1062 at iter 7836 **\n",
            "  ** New min smooth loss: 2.1060 at iter 7837 **\n",
            "  ** New min smooth loss: 2.1058 at iter 7838 **\n",
            "  ** New min smooth loss: 2.1057 at iter 7839 **\n",
            "  ** New min smooth loss: 2.1056 at iter 7840 **\n",
            "  ** New min smooth loss: 2.1055 at iter 7842 **\n",
            "  ** New min smooth loss: 2.1055 at iter 7843 **\n",
            "  ** New min smooth loss: 2.1052 at iter 7845 **\n",
            "  ** New min smooth loss: 2.1049 at iter 7847 **\n",
            "  ** New min smooth loss: 2.1048 at iter 7848 **\n",
            "  ** New min smooth loss: 2.1047 at iter 7849 **\n",
            "  ** New min smooth loss: 2.1042 at iter 7850 **\n",
            "  ** New min smooth loss: 2.1037 at iter 7854 **\n",
            "  ** New min smooth loss: 2.1036 at iter 7855 **\n",
            "  ** New min smooth loss: 2.1030 at iter 7856 **\n",
            "  ** New min smooth loss: 2.1028 at iter 7857 **\n",
            "  ** New min smooth loss: 2.1025 at iter 7858 **\n",
            "  ** New min smooth loss: 2.1024 at iter 7859 **\n",
            "  ** New min smooth loss: 2.1023 at iter 7860 **\n",
            "  ** New min smooth loss: 2.1023 at iter 7865 **\n",
            "  ** New min smooth loss: 2.1014 at iter 7866 **\n",
            "  ** New min smooth loss: 2.1007 at iter 7867 **\n",
            "  ** New min smooth loss: 2.1004 at iter 7868 **\n",
            "  ** New min smooth loss: 2.0997 at iter 7869 **\n",
            "  ** New min smooth loss: 2.0992 at iter 7870 **\n",
            "  ** New min smooth loss: 2.0989 at iter 7871 **\n",
            "  ** New min smooth loss: 2.0988 at iter 7872 **\n",
            "  ** New min smooth loss: 2.0981 at iter 7875 **\n",
            "  ** New min smooth loss: 2.0978 at iter 7876 **\n",
            "  ** New min smooth loss: 2.0977 at iter 7877 **\n",
            "  ** New min smooth loss: 2.0975 at iter 7878 **\n",
            "  ** New min smooth loss: 2.0975 at iter 7880 **\n",
            "  ** New min smooth loss: 2.0973 at iter 7881 **\n",
            "  ** New min smooth loss: 2.0973 at iter 7884 **\n",
            "  ** New min smooth loss: 2.0967 at iter 7885 **\n",
            "  ** New min smooth loss: 2.0966 at iter 7886 **\n",
            "  ** New min smooth loss: 2.0959 at iter 7887 **\n",
            "  ** New min smooth loss: 2.0958 at iter 7888 **\n",
            "  ** New min smooth loss: 2.0956 at iter 7889 **\n",
            "  ** New min smooth loss: 2.0951 at iter 7890 **\n",
            "  ** New min smooth loss: 2.0946 at iter 7891 **\n",
            "  ** New min smooth loss: 2.0939 at iter 7892 **\n",
            "  ** New min smooth loss: 2.0936 at iter 7894 **\n",
            "  ** New min smooth loss: 2.0935 at iter 7895 **\n",
            "  ** New min smooth loss: 2.0926 at iter 7896 **\n",
            "  ** New min smooth loss: 2.0926 at iter 7897 **\n",
            "  ** New min smooth loss: 2.0923 at iter 7898 **\n",
            "  ** New min smooth loss: 2.0923 at iter 7900 **\n",
            "Iter: 7900/100000, Smooth Loss: 2.0923, Min Smooth Loss: 2.0923 (at iter 7900), Time: 39.57s\n",
            "  ** New min smooth loss: 2.0921 at iter 7912 **\n",
            "  ** New min smooth loss: 2.0916 at iter 7913 **\n",
            "  ** New min smooth loss: 2.0913 at iter 7914 **\n",
            "  ** New min smooth loss: 2.0911 at iter 7915 **\n",
            "  ** New min smooth loss: 2.0911 at iter 7919 **\n",
            "  ** New min smooth loss: 2.0910 at iter 7923 **\n",
            "  ** New min smooth loss: 2.0907 at iter 7930 **\n",
            "  ** New min smooth loss: 2.0905 at iter 7937 **\n",
            "  ** New min smooth loss: 2.0902 at iter 7938 **\n",
            "  ** New min smooth loss: 2.0894 at iter 7939 **\n",
            "  ** New min smooth loss: 2.0888 at iter 7940 **\n",
            "  ** New min smooth loss: 2.0888 at iter 7945 **\n",
            "  ** New min smooth loss: 2.0888 at iter 7946 **\n",
            "  ** New min smooth loss: 2.0887 at iter 7947 **\n",
            "  ** New min smooth loss: 2.0882 at iter 7949 **\n",
            "  ** New min smooth loss: 2.0880 at iter 7950 **\n",
            "  ** New min smooth loss: 2.0876 at iter 7951 **\n",
            "  ** New min smooth loss: 2.0872 at iter 7952 **\n",
            "  ** New min smooth loss: 2.0867 at iter 7953 **\n",
            "  ** New min smooth loss: 2.0864 at iter 7954 **\n",
            "  ** New min smooth loss: 2.0862 at iter 7956 **\n",
            "  ** New min smooth loss: 2.0860 at iter 7957 **\n",
            "  ** New min smooth loss: 2.0856 at iter 7958 **\n",
            "  ** New min smooth loss: 2.0851 at iter 7959 **\n",
            "  ** New min smooth loss: 2.0849 at iter 7960 **\n",
            "  ** New min smooth loss: 2.0848 at iter 7974 **\n",
            "  ** New min smooth loss: 2.0842 at iter 7975 **\n",
            "  ** New min smooth loss: 2.0834 at iter 7976 **\n",
            "  ** New min smooth loss: 2.0832 at iter 7977 **\n",
            "  ** New min smooth loss: 2.0828 at iter 7978 **\n",
            "  ** New min smooth loss: 2.0822 at iter 7979 **\n",
            "Iter: 8000/100000, Smooth Loss: 2.0848, Min Smooth Loss: 2.0822 (at iter 7979), Time: 40.27s\n",
            "  ** New min smooth loss: 2.0821 at iter 8031 **\n",
            "  ** New min smooth loss: 2.0818 at iter 8033 **\n",
            "  ** New min smooth loss: 2.0818 at iter 8034 **\n",
            "  ** New min smooth loss: 2.0817 at iter 8035 **\n",
            "  ** New min smooth loss: 2.0814 at iter 8037 **\n",
            "  ** New min smooth loss: 2.0805 at iter 8038 **\n",
            "  ** New min smooth loss: 2.0803 at iter 8039 **\n",
            "  ** New min smooth loss: 2.0800 at iter 8041 **\n",
            "  ** New min smooth loss: 2.0797 at iter 8042 **\n",
            "  ** New min smooth loss: 2.0793 at iter 8052 **\n",
            "  ** New min smooth loss: 2.0793 at iter 8053 **\n",
            "  ** New min smooth loss: 2.0793 at iter 8054 **\n",
            "  ** New min smooth loss: 2.0792 at iter 8055 **\n",
            "  ** New min smooth loss: 2.0787 at iter 8056 **\n",
            "  ** New min smooth loss: 2.0784 at iter 8057 **\n",
            "  ** New min smooth loss: 2.0781 at iter 8080 **\n",
            "  ** New min smooth loss: 2.0777 at iter 8089 **\n",
            "  ** New min smooth loss: 2.0773 at iter 8090 **\n",
            "  ** New min smooth loss: 2.0768 at iter 8091 **\n",
            "  ** New min smooth loss: 2.0764 at iter 8092 **\n",
            "  ** New min smooth loss: 2.0757 at iter 8093 **\n",
            "  ** New min smooth loss: 2.0755 at iter 8097 **\n",
            "  ** New min smooth loss: 2.0753 at iter 8098 **\n",
            "  ** New min smooth loss: 2.0751 at iter 8099 **\n",
            "  ** New min smooth loss: 2.0745 at iter 8100 **\n",
            "Iter: 8100/100000, Smooth Loss: 2.0745, Min Smooth Loss: 2.0745 (at iter 8100), Time: 40.75s\n",
            "  ** New min smooth loss: 2.0741 at iter 8101 **\n",
            "  ** New min smooth loss: 2.0740 at iter 8102 **\n",
            "  ** New min smooth loss: 2.0740 at iter 8103 **\n",
            "  ** New min smooth loss: 2.0736 at iter 8108 **\n",
            "  ** New min smooth loss: 2.0733 at iter 8109 **\n",
            "  ** New min smooth loss: 2.0732 at iter 8111 **\n",
            "  ** New min smooth loss: 2.0729 at iter 8113 **\n",
            "  ** New min smooth loss: 2.0729 at iter 8114 **\n",
            "  ** New min smooth loss: 2.0724 at iter 8116 **\n",
            "  ** New min smooth loss: 2.0722 at iter 8118 **\n",
            "  ** New min smooth loss: 2.0717 at iter 8120 **\n",
            "  ** New min smooth loss: 2.0710 at iter 8121 **\n",
            "  ** New min smooth loss: 2.0709 at iter 8122 **\n",
            "  ** New min smooth loss: 2.0708 at iter 8123 **\n",
            "  ** New min smooth loss: 2.0702 at iter 8124 **\n",
            "  ** New min smooth loss: 2.0698 at iter 8126 **\n",
            "  ** New min smooth loss: 2.0698 at iter 8127 **\n",
            "  ** New min smooth loss: 2.0696 at iter 8128 **\n",
            "  ** New min smooth loss: 2.0695 at iter 8131 **\n",
            "  ** New min smooth loss: 2.0694 at iter 8132 **\n",
            "  ** New min smooth loss: 2.0692 at iter 8134 **\n",
            "  ** New min smooth loss: 2.0690 at iter 8136 **\n",
            "  ** New min smooth loss: 2.0687 at iter 8138 **\n",
            "  ** New min smooth loss: 2.0681 at iter 8139 **\n",
            "  ** New min smooth loss: 2.0677 at iter 8143 **\n",
            "  ** New min smooth loss: 2.0675 at iter 8145 **\n",
            "  ** New min smooth loss: 2.0674 at iter 8146 **\n",
            "  ** New min smooth loss: 2.0674 at iter 8147 **\n",
            "  ** New min smooth loss: 2.0666 at iter 8148 **\n",
            "  ** New min smooth loss: 2.0663 at iter 8149 **\n",
            "  ** New min smooth loss: 2.0660 at iter 8150 **\n",
            "  ** New min smooth loss: 2.0659 at iter 8151 **\n",
            "  ** New min smooth loss: 2.0652 at iter 8152 **\n",
            "  ** New min smooth loss: 2.0651 at iter 8153 **\n",
            "  ** New min smooth loss: 2.0649 at iter 8154 **\n",
            "  ** New min smooth loss: 2.0645 at iter 8158 **\n",
            "  ** New min smooth loss: 2.0643 at iter 8161 **\n",
            "  ** New min smooth loss: 2.0642 at iter 8163 **\n",
            "  ** New min smooth loss: 2.0641 at iter 8165 **\n",
            "  ** New min smooth loss: 2.0633 at iter 8166 **\n",
            "  ** New min smooth loss: 2.0631 at iter 8167 **\n",
            "  ** New min smooth loss: 2.0629 at iter 8171 **\n",
            "  ** New min smooth loss: 2.0626 at iter 8184 **\n",
            "  ** New min smooth loss: 2.0619 at iter 8185 **\n",
            "  ** New min smooth loss: 2.0618 at iter 8192 **\n",
            "  ** New min smooth loss: 2.0616 at iter 8195 **\n",
            "  ** New min smooth loss: 2.0615 at iter 8197 **\n",
            "  ** New min smooth loss: 2.0613 at iter 8199 **\n",
            "  ** New min smooth loss: 2.0605 at iter 8200 **\n",
            "Iter: 8200/100000, Smooth Loss: 2.0605, Min Smooth Loss: 2.0605 (at iter 8200), Time: 41.20s\n",
            "  ** New min smooth loss: 2.0603 at iter 8201 **\n",
            "  ** New min smooth loss: 2.0599 at iter 8204 **\n",
            "  ** New min smooth loss: 2.0596 at iter 8207 **\n",
            "  ** New min smooth loss: 2.0596 at iter 8215 **\n",
            "  ** New min smooth loss: 2.0594 at iter 8218 **\n",
            "  ** New min smooth loss: 2.0591 at iter 8219 **\n",
            "  ** New min smooth loss: 2.0585 at iter 8220 **\n",
            "  ** New min smooth loss: 2.0584 at iter 8226 **\n",
            "  ** New min smooth loss: 2.0581 at iter 8235 **\n",
            "  ** New min smooth loss: 2.0581 at iter 8237 **\n",
            "  ** New min smooth loss: 2.0576 at iter 8238 **\n",
            "  ** New min smooth loss: 2.0575 at iter 8244 **\n",
            "  ** New min smooth loss: 2.0572 at iter 8245 **\n",
            "  ** New min smooth loss: 2.0570 at iter 8246 **\n",
            "  ** New min smooth loss: 2.0566 at iter 8248 **\n",
            "  ** New min smooth loss: 2.0560 at iter 8249 **\n",
            "  ** New min smooth loss: 2.0559 at iter 8250 **\n",
            "  ** New min smooth loss: 2.0557 at iter 8251 **\n",
            "  ** New min smooth loss: 2.0554 at iter 8252 **\n",
            "  ** New min smooth loss: 2.0551 at iter 8253 **\n",
            "  ** New min smooth loss: 2.0548 at iter 8254 **\n",
            "  ** New min smooth loss: 2.0543 at iter 8256 **\n",
            "  ** New min smooth loss: 2.0539 at iter 8257 **\n",
            "  ** New min smooth loss: 2.0535 at iter 8262 **\n",
            "  ** New min smooth loss: 2.0532 at iter 8263 **\n",
            "  ** New min smooth loss: 2.0529 at iter 8264 **\n",
            "  ** New min smooth loss: 2.0522 at iter 8265 **\n",
            "  ** New min smooth loss: 2.0518 at iter 8266 **\n",
            "  ** New min smooth loss: 2.0516 at iter 8269 **\n",
            "  ** New min smooth loss: 2.0516 at iter 8270 **\n",
            "  ** New min smooth loss: 2.0514 at iter 8271 **\n",
            "  ** New min smooth loss: 2.0511 at iter 8272 **\n",
            "  ** New min smooth loss: 2.0508 at iter 8273 **\n",
            "  ** New min smooth loss: 2.0504 at iter 8274 **\n",
            "  ** New min smooth loss: 2.0502 at iter 8275 **\n",
            "  ** New min smooth loss: 2.0501 at iter 8276 **\n",
            "  ** New min smooth loss: 2.0501 at iter 8277 **\n",
            "  ** New min smooth loss: 2.0498 at iter 8280 **\n",
            "  ** New min smooth loss: 2.0498 at iter 8288 **\n",
            "  ** New min smooth loss: 2.0491 at iter 8290 **\n",
            "  ** New min smooth loss: 2.0491 at iter 8293 **\n",
            "  ** New min smooth loss: 2.0490 at iter 8294 **\n",
            "  ** New min smooth loss: 2.0487 at iter 8298 **\n",
            "  ** New min smooth loss: 2.0484 at iter 8299 **\n",
            "Iter: 8300/100000, Smooth Loss: 2.0485, Min Smooth Loss: 2.0484 (at iter 8299), Time: 41.61s\n",
            "  ** New min smooth loss: 2.0483 at iter 8301 **\n",
            "  ** New min smooth loss: 2.0475 at iter 8302 **\n",
            "  ** New min smooth loss: 2.0475 at iter 8304 **\n",
            "  ** New min smooth loss: 2.0473 at iter 8311 **\n",
            "  ** New min smooth loss: 2.0466 at iter 8343 **\n",
            "  ** New min smooth loss: 2.0464 at iter 8344 **\n",
            "  ** New min smooth loss: 2.0462 at iter 8345 **\n",
            "  ** New min smooth loss: 2.0461 at iter 8348 **\n",
            "  ** New min smooth loss: 2.0455 at iter 8349 **\n",
            "  ** New min smooth loss: 2.0454 at iter 8353 **\n",
            "  ** New min smooth loss: 2.0450 at iter 8354 **\n",
            "  ** New min smooth loss: 2.0447 at iter 8355 **\n",
            "  ** New min smooth loss: 2.0447 at iter 8361 **\n",
            "  ** New min smooth loss: 2.0445 at iter 8363 **\n",
            "  ** New min smooth loss: 2.0443 at iter 8365 **\n",
            "  ** New min smooth loss: 2.0437 at iter 8367 **\n",
            "  ** New min smooth loss: 2.0437 at iter 8368 **\n",
            "  ** New min smooth loss: 2.0435 at iter 8369 **\n",
            "  ** New min smooth loss: 2.0431 at iter 8370 **\n",
            "  ** New min smooth loss: 2.0428 at iter 8378 **\n",
            "  ** New min smooth loss: 2.0427 at iter 8379 **\n",
            "  ** New min smooth loss: 2.0426 at iter 8380 **\n",
            "  ** New min smooth loss: 2.0424 at iter 8381 **\n",
            "  ** New min smooth loss: 2.0419 at iter 8382 **\n",
            "  ** New min smooth loss: 2.0419 at iter 8383 **\n",
            "  ** New min smooth loss: 2.0418 at iter 8384 **\n",
            "  ** New min smooth loss: 2.0411 at iter 8385 **\n",
            "  ** New min smooth loss: 2.0410 at iter 8386 **\n",
            "  ** New min smooth loss: 2.0403 at iter 8388 **\n",
            "  ** New min smooth loss: 2.0401 at iter 8389 **\n",
            "  ** New min smooth loss: 2.0394 at iter 8390 **\n",
            "  ** New min smooth loss: 2.0388 at iter 8391 **\n",
            "  ** New min smooth loss: 2.0381 at iter 8392 **\n",
            "  ** New min smooth loss: 2.0378 at iter 8393 **\n",
            "  ** New min smooth loss: 2.0370 at iter 8394 **\n",
            "  ** New min smooth loss: 2.0362 at iter 8395 **\n",
            "  ** New min smooth loss: 2.0354 at iter 8396 **\n",
            "  ** New min smooth loss: 2.0348 at iter 8397 **\n",
            "  ** New min smooth loss: 2.0344 at iter 8399 **\n",
            "  ** New min smooth loss: 2.0343 at iter 8400 **\n",
            "Iter: 8400/100000, Smooth Loss: 2.0343, Min Smooth Loss: 2.0343 (at iter 8400), Time: 42.04s\n",
            "  ** New min smooth loss: 2.0340 at iter 8401 **\n",
            "  ** New min smooth loss: 2.0339 at iter 8402 **\n",
            "  ** New min smooth loss: 2.0338 at iter 8403 **\n",
            "  ** New min smooth loss: 2.0335 at iter 8407 **\n",
            "  ** New min smooth loss: 2.0331 at iter 8408 **\n",
            "  ** New min smooth loss: 2.0330 at iter 8409 **\n",
            "  ** New min smooth loss: 2.0328 at iter 8411 **\n",
            "  ** New min smooth loss: 2.0324 at iter 8412 **\n",
            "  ** New min smooth loss: 2.0314 at iter 8413 **\n",
            "  ** New min smooth loss: 2.0312 at iter 8414 **\n",
            "  ** New min smooth loss: 2.0311 at iter 8415 **\n",
            "  ** New min smooth loss: 2.0306 at iter 8417 **\n",
            "  ** New min smooth loss: 2.0305 at iter 8419 **\n",
            "  ** New min smooth loss: 2.0305 at iter 8420 **\n",
            "  ** New min smooth loss: 2.0299 at iter 8421 **\n",
            "  ** New min smooth loss: 2.0296 at iter 8425 **\n",
            "  ** New min smooth loss: 2.0290 at iter 8426 **\n",
            "  ** New min smooth loss: 2.0287 at iter 8427 **\n",
            "  ** New min smooth loss: 2.0283 at iter 8433 **\n",
            "  ** New min smooth loss: 2.0277 at iter 8434 **\n",
            "  ** New min smooth loss: 2.0276 at iter 8435 **\n",
            "  ** New min smooth loss: 2.0269 at iter 8436 **\n",
            "  ** New min smooth loss: 2.0268 at iter 8437 **\n",
            "  ** New min smooth loss: 2.0264 at iter 8447 **\n",
            "  ** New min smooth loss: 2.0261 at iter 8448 **\n",
            "  ** New min smooth loss: 2.0259 at iter 8449 **\n",
            "  ** New min smooth loss: 2.0255 at iter 8450 **\n",
            "  ** New min smooth loss: 2.0251 at iter 8456 **\n",
            "  ** New min smooth loss: 2.0248 at iter 8459 **\n",
            "  ** New min smooth loss: 2.0237 at iter 8460 **\n",
            "  ** New min smooth loss: 2.0235 at iter 8461 **\n",
            "  ** New min smooth loss: 2.0228 at iter 8462 **\n",
            "  ** New min smooth loss: 2.0222 at iter 8463 **\n",
            "  ** New min smooth loss: 2.0221 at iter 8465 **\n",
            "  ** New min smooth loss: 2.0221 at iter 8467 **\n",
            "  ** New min smooth loss: 2.0220 at iter 8476 **\n",
            "  ** New min smooth loss: 2.0209 at iter 8477 **\n",
            "  ** New min smooth loss: 2.0207 at iter 8478 **\n",
            "  ** New min smooth loss: 2.0203 at iter 8479 **\n",
            "  ** New min smooth loss: 2.0201 at iter 8483 **\n",
            "  ** New min smooth loss: 2.0197 at iter 8484 **\n",
            "  ** New min smooth loss: 2.0188 at iter 8485 **\n",
            "  ** New min smooth loss: 2.0187 at iter 8486 **\n",
            "  ** New min smooth loss: 2.0178 at iter 8489 **\n",
            "  ** New min smooth loss: 2.0175 at iter 8490 **\n",
            "  ** New min smooth loss: 2.0171 at iter 8491 **\n",
            "  ** New min smooth loss: 2.0171 at iter 8492 **\n",
            "  ** New min smooth loss: 2.0168 at iter 8494 **\n",
            "  ** New min smooth loss: 2.0167 at iter 8495 **\n",
            "Iter: 8500/100000, Smooth Loss: 2.0170, Min Smooth Loss: 2.0167 (at iter 8495), Time: 42.49s\n",
            "  ** New min smooth loss: 2.0166 at iter 8502 **\n",
            "  ** New min smooth loss: 2.0164 at iter 8503 **\n",
            "  ** New min smooth loss: 2.0161 at iter 8507 **\n",
            "  ** New min smooth loss: 2.0157 at iter 8508 **\n",
            "  ** New min smooth loss: 2.0156 at iter 8509 **\n",
            "  ** New min smooth loss: 2.0151 at iter 8510 **\n",
            "  ** New min smooth loss: 2.0149 at iter 8515 **\n",
            "  ** New min smooth loss: 2.0141 at iter 8516 **\n",
            "  ** New min smooth loss: 2.0141 at iter 8544 **\n",
            "  ** New min smooth loss: 2.0133 at iter 8550 **\n",
            "  ** New min smooth loss: 2.0131 at iter 8556 **\n",
            "  ** New min smooth loss: 2.0129 at iter 8557 **\n",
            "  ** New min smooth loss: 2.0126 at iter 8558 **\n",
            "  ** New min smooth loss: 2.0123 at iter 8559 **\n",
            "  ** New min smooth loss: 2.0122 at iter 8560 **\n",
            "  ** New min smooth loss: 2.0121 at iter 8561 **\n",
            "  ** New min smooth loss: 2.0120 at iter 8575 **\n",
            "  ** New min smooth loss: 2.0120 at iter 8578 **\n",
            "  ** New min smooth loss: 2.0120 at iter 8588 **\n",
            "Iter: 8600/100000, Smooth Loss: 2.0137, Min Smooth Loss: 2.0120 (at iter 8588), Time: 42.91s\n",
            "  ** New min smooth loss: 2.0119 at iter 8632 **\n",
            "  ** New min smooth loss: 2.0118 at iter 8633 **\n",
            "  ** New min smooth loss: 2.0115 at iter 8634 **\n",
            "  ** New min smooth loss: 2.0115 at iter 8636 **\n",
            "  ** New min smooth loss: 2.0114 at iter 8638 **\n",
            "  ** New min smooth loss: 2.0113 at iter 8639 **\n",
            "  ** New min smooth loss: 2.0112 at iter 8641 **\n",
            "  ** New min smooth loss: 2.0109 at iter 8642 **\n",
            "  ** New min smooth loss: 2.0107 at iter 8643 **\n",
            "  ** New min smooth loss: 2.0106 at iter 8645 **\n",
            "  ** New min smooth loss: 2.0106 at iter 8646 **\n",
            "  ** New min smooth loss: 2.0104 at iter 8647 **\n",
            "  ** New min smooth loss: 2.0102 at iter 8649 **\n",
            "  ** New min smooth loss: 2.0099 at iter 8650 **\n",
            "  ** New min smooth loss: 2.0089 at iter 8651 **\n",
            "  ** New min smooth loss: 2.0086 at iter 8652 **\n",
            "  ** New min smooth loss: 2.0085 at iter 8653 **\n",
            "  ** New min smooth loss: 2.0084 at iter 8654 **\n",
            "  ** New min smooth loss: 2.0084 at iter 8655 **\n",
            "  ** New min smooth loss: 2.0073 at iter 8657 **\n",
            "  ** New min smooth loss: 2.0067 at iter 8659 **\n",
            "  ** New min smooth loss: 2.0067 at iter 8661 **\n",
            "  ** New min smooth loss: 2.0066 at iter 8662 **\n",
            "  ** New min smooth loss: 2.0066 at iter 8667 **\n",
            "  ** New min smooth loss: 2.0065 at iter 8668 **\n",
            "Iter: 8700/100000, Smooth Loss: 2.0122, Min Smooth Loss: 2.0065 (at iter 8668), Time: 43.34s\n",
            "Iter: 8800/100000, Smooth Loss: 2.0255, Min Smooth Loss: 2.0065 (at iter 8668), Time: 43.76s\n",
            "Iter: 8900/100000, Smooth Loss: 2.0265, Min Smooth Loss: 2.0065 (at iter 8668), Time: 44.16s\n",
            "Iter: 9000/100000, Smooth Loss: 2.0221, Min Smooth Loss: 2.0065 (at iter 8668), Time: 44.60s\n",
            "Iter: 9100/100000, Smooth Loss: 2.0283, Min Smooth Loss: 2.0065 (at iter 8668), Time: 45.03s\n",
            "Iter: 9200/100000, Smooth Loss: 2.0334, Min Smooth Loss: 2.0065 (at iter 8668), Time: 45.48s\n",
            "Iter: 9300/100000, Smooth Loss: 2.0270, Min Smooth Loss: 2.0065 (at iter 8668), Time: 45.91s\n",
            "Iter: 9400/100000, Smooth Loss: 2.0230, Min Smooth Loss: 2.0065 (at iter 8668), Time: 46.34s\n",
            "Iter: 9500/100000, Smooth Loss: 2.0240, Min Smooth Loss: 2.0065 (at iter 8668), Time: 46.81s\n",
            "Iter: 9600/100000, Smooth Loss: 2.0161, Min Smooth Loss: 2.0065 (at iter 8668), Time: 47.23s\n",
            "Iter: 9700/100000, Smooth Loss: 2.0105, Min Smooth Loss: 2.0065 (at iter 8668), Time: 47.67s\n",
            "Iter: 9800/100000, Smooth Loss: 2.0158, Min Smooth Loss: 2.0065 (at iter 8668), Time: 48.08s\n",
            "  ** New min smooth loss: 2.0064 at iter 9840 **\n",
            "  ** New min smooth loss: 2.0063 at iter 9841 **\n",
            "  ** New min smooth loss: 2.0063 at iter 9847 **\n",
            "  ** New min smooth loss: 2.0060 at iter 9851 **\n",
            "  ** New min smooth loss: 2.0051 at iter 9852 **\n",
            "  ** New min smooth loss: 2.0051 at iter 9853 **\n",
            "  ** New min smooth loss: 2.0050 at iter 9854 **\n",
            "  ** New min smooth loss: 2.0042 at iter 9855 **\n",
            "  ** New min smooth loss: 2.0040 at iter 9858 **\n",
            "  ** New min smooth loss: 2.0039 at iter 9859 **\n",
            "  ** New min smooth loss: 2.0031 at iter 9860 **\n",
            "  ** New min smooth loss: 2.0027 at iter 9861 **\n",
            "  ** New min smooth loss: 2.0026 at iter 9862 **\n",
            "  ** New min smooth loss: 2.0022 at iter 9863 **\n",
            "  ** New min smooth loss: 2.0022 at iter 9866 **\n",
            "  ** New min smooth loss: 2.0022 at iter 9867 **\n",
            "  ** New min smooth loss: 2.0021 at iter 9872 **\n",
            "  ** New min smooth loss: 2.0018 at iter 9874 **\n",
            "  ** New min smooth loss: 2.0014 at iter 9883 **\n",
            "Iter: 9900/100000, Smooth Loss: 2.0040, Min Smooth Loss: 2.0014 (at iter 9883), Time: 48.52s\n",
            "Iter: 10000/100000, Smooth Loss: 2.0093, Min Smooth Loss: 2.0014 (at iter 9883), Time: 48.95s\n",
            "--- Synthesized text at iter 10000 ---\n",
            "\"\n",
            "\"I levere hers ao sly chat, ance, s arr armaar momberuus, burn nlf to slet serom afire comming the ffleeps, cemale surd rou maly.\n",
            "\"D ster lyoof yea larevery, the forthar al D\"\n",
            "UStmy gor core ond.  \"\n",
            "---\n",
            "Iter: 10100/100000, Smooth Loss: 2.0158, Min Smooth Loss: 2.0014 (at iter 9883), Time: 49.39s\n",
            "Iter: 10200/100000, Smooth Loss: 2.0156, Min Smooth Loss: 2.0014 (at iter 9883), Time: 49.83s\n",
            "Iter: 10300/100000, Smooth Loss: 2.0240, Min Smooth Loss: 2.0014 (at iter 9883), Time: 50.26s\n",
            "Iter: 10400/100000, Smooth Loss: 2.0257, Min Smooth Loss: 2.0014 (at iter 9883), Time: 50.86s\n",
            "Iter: 10500/100000, Smooth Loss: 2.0273, Min Smooth Loss: 2.0014 (at iter 9883), Time: 51.49s\n",
            "Iter: 10600/100000, Smooth Loss: 2.0385, Min Smooth Loss: 2.0014 (at iter 9883), Time: 52.71s\n",
            "Iter: 10700/100000, Smooth Loss: 2.0447, Min Smooth Loss: 2.0014 (at iter 9883), Time: 53.34s\n",
            "Iter: 10800/100000, Smooth Loss: 2.0474, Min Smooth Loss: 2.0014 (at iter 9883), Time: 53.98s\n",
            "Iter: 10900/100000, Smooth Loss: 2.0436, Min Smooth Loss: 2.0014 (at iter 9883), Time: 54.63s\n",
            "Iter: 11000/100000, Smooth Loss: 2.0438, Min Smooth Loss: 2.0014 (at iter 9883), Time: 55.26s\n",
            "Iter: 11100/100000, Smooth Loss: 2.0357, Min Smooth Loss: 2.0014 (at iter 9883), Time: 55.68s\n",
            "Iter: 11200/100000, Smooth Loss: 2.0380, Min Smooth Loss: 2.0014 (at iter 9883), Time: 56.12s\n",
            "Iter: 11300/100000, Smooth Loss: 2.0376, Min Smooth Loss: 2.0014 (at iter 9883), Time: 56.54s\n",
            "Iter: 11400/100000, Smooth Loss: 2.0271, Min Smooth Loss: 2.0014 (at iter 9883), Time: 56.99s\n",
            "Iter: 11500/100000, Smooth Loss: 2.0293, Min Smooth Loss: 2.0014 (at iter 9883), Time: 57.44s\n",
            "Iter: 11600/100000, Smooth Loss: 2.0291, Min Smooth Loss: 2.0014 (at iter 9883), Time: 57.86s\n",
            "Iter: 11700/100000, Smooth Loss: 2.0327, Min Smooth Loss: 2.0014 (at iter 9883), Time: 58.30s\n",
            "Iter: 11800/100000, Smooth Loss: 2.0389, Min Smooth Loss: 2.0014 (at iter 9883), Time: 58.73s\n",
            "Iter: 11900/100000, Smooth Loss: 2.0301, Min Smooth Loss: 2.0014 (at iter 9883), Time: 59.14s\n",
            "Iter: 12000/100000, Smooth Loss: 2.0249, Min Smooth Loss: 2.0014 (at iter 9883), Time: 59.57s\n",
            "Iter: 12100/100000, Smooth Loss: 2.0205, Min Smooth Loss: 2.0014 (at iter 9883), Time: 59.99s\n",
            "Iter: 12200/100000, Smooth Loss: 2.0264, Min Smooth Loss: 2.0014 (at iter 9883), Time: 60.43s\n",
            "Iter: 12300/100000, Smooth Loss: 2.0175, Min Smooth Loss: 2.0014 (at iter 9883), Time: 60.84s\n",
            "Iter: 12400/100000, Smooth Loss: 2.0054, Min Smooth Loss: 2.0014 (at iter 9883), Time: 61.26s\n",
            "  ** New min smooth loss: 2.0014 at iter 12414 **\n",
            "  ** New min smooth loss: 2.0012 at iter 12423 **\n",
            "  ** New min smooth loss: 2.0008 at iter 12433 **\n",
            "  ** New min smooth loss: 2.0007 at iter 12435 **\n",
            "  ** New min smooth loss: 2.0006 at iter 12436 **\n",
            "  ** New min smooth loss: 2.0006 at iter 12437 **\n",
            "  ** New min smooth loss: 2.0001 at iter 12438 **\n",
            "  ** New min smooth loss: 1.9994 at iter 12440 **\n",
            "  ** New min smooth loss: 1.9994 at iter 12441 **\n",
            "  ** New min smooth loss: 1.9991 at iter 12442 **\n",
            "  ** New min smooth loss: 1.9990 at iter 12443 **\n",
            "  ** New min smooth loss: 1.9983 at iter 12444 **\n",
            "  ** New min smooth loss: 1.9980 at iter 12445 **\n",
            "  ** New min smooth loss: 1.9973 at iter 12446 **\n",
            "  ** New min smooth loss: 1.9968 at iter 12447 **\n",
            "  ** New min smooth loss: 1.9965 at iter 12448 **\n",
            "  ** New min smooth loss: 1.9962 at iter 12449 **\n",
            "Iter: 12500/100000, Smooth Loss: 2.0015, Min Smooth Loss: 1.9962 (at iter 12449), Time: 61.71s\n",
            "Iter: 12600/100000, Smooth Loss: 2.0018, Min Smooth Loss: 1.9962 (at iter 12449), Time: 62.14s\n",
            "Iter: 12700/100000, Smooth Loss: 1.9980, Min Smooth Loss: 1.9962 (at iter 12449), Time: 62.58s\n",
            "  ** New min smooth loss: 1.9959 at iter 12719 **\n",
            "  ** New min smooth loss: 1.9957 at iter 12720 **\n",
            "  ** New min smooth loss: 1.9957 at iter 12730 **\n",
            "  ** New min smooth loss: 1.9957 at iter 12732 **\n",
            "  ** New min smooth loss: 1.9956 at iter 12734 **\n",
            "  ** New min smooth loss: 1.9953 at iter 12769 **\n",
            "  ** New min smooth loss: 1.9951 at iter 12770 **\n",
            "  ** New min smooth loss: 1.9946 at iter 12778 **\n",
            "  ** New min smooth loss: 1.9944 at iter 12780 **\n",
            "  ** New min smooth loss: 1.9941 at iter 12787 **\n",
            "  ** New min smooth loss: 1.9937 at iter 12788 **\n",
            "  ** New min smooth loss: 1.9936 at iter 12796 **\n",
            "  ** New min smooth loss: 1.9934 at iter 12797 **\n",
            "  ** New min smooth loss: 1.9932 at iter 12798 **\n",
            "  ** New min smooth loss: 1.9930 at iter 12799 **\n",
            "  ** New min smooth loss: 1.9925 at iter 12800 **\n",
            "Iter: 12800/100000, Smooth Loss: 1.9925, Min Smooth Loss: 1.9925 (at iter 12800), Time: 63.01s\n",
            "  ** New min smooth loss: 1.9924 at iter 12802 **\n",
            "  ** New min smooth loss: 1.9922 at iter 12803 **\n",
            "  ** New min smooth loss: 1.9916 at iter 12804 **\n",
            "  ** New min smooth loss: 1.9909 at iter 12805 **\n",
            "  ** New min smooth loss: 1.9901 at iter 12806 **\n",
            "  ** New min smooth loss: 1.9900 at iter 12807 **\n",
            "  ** New min smooth loss: 1.9897 at iter 12808 **\n",
            "  ** New min smooth loss: 1.9895 at iter 12809 **\n",
            "  ** New min smooth loss: 1.9894 at iter 12810 **\n",
            "  ** New min smooth loss: 1.9890 at iter 12813 **\n",
            "  ** New min smooth loss: 1.9889 at iter 12814 **\n",
            "  ** New min smooth loss: 1.9885 at iter 12819 **\n",
            "  ** New min smooth loss: 1.9882 at iter 12820 **\n",
            "  ** New min smooth loss: 1.9880 at iter 12821 **\n",
            "  ** New min smooth loss: 1.9878 at iter 12833 **\n",
            "  ** New min smooth loss: 1.9876 at iter 12834 **\n",
            "  ** New min smooth loss: 1.9870 at iter 12836 **\n",
            "  ** New min smooth loss: 1.9865 at iter 12837 **\n",
            "  ** New min smooth loss: 1.9865 at iter 12843 **\n",
            "  ** New min smooth loss: 1.9861 at iter 12845 **\n",
            "  ** New min smooth loss: 1.9859 at iter 12855 **\n",
            "  ** New min smooth loss: 1.9858 at iter 12856 **\n",
            "  ** New min smooth loss: 1.9857 at iter 12858 **\n",
            "  ** New min smooth loss: 1.9857 at iter 12859 **\n",
            "  ** New min smooth loss: 1.9849 at iter 12860 **\n",
            "  ** New min smooth loss: 1.9848 at iter 12870 **\n",
            "  ** New min smooth loss: 1.9848 at iter 12877 **\n",
            "  ** New min smooth loss: 1.9844 at iter 12878 **\n",
            "  ** New min smooth loss: 1.9840 at iter 12879 **\n",
            "  ** New min smooth loss: 1.9839 at iter 12880 **\n",
            "  ** New min smooth loss: 1.9838 at iter 12887 **\n",
            "  ** New min smooth loss: 1.9836 at iter 12888 **\n",
            "  ** New min smooth loss: 1.9835 at iter 12890 **\n",
            "  ** New min smooth loss: 1.9830 at iter 12892 **\n",
            "  ** New min smooth loss: 1.9825 at iter 12893 **\n",
            "Iter: 12900/100000, Smooth Loss: 1.9826, Min Smooth Loss: 1.9825 (at iter 12893), Time: 63.45s\n",
            "  ** New min smooth loss: 1.9824 at iter 12903 **\n",
            "  ** New min smooth loss: 1.9823 at iter 12907 **\n",
            "  ** New min smooth loss: 1.9822 at iter 12908 **\n",
            "  ** New min smooth loss: 1.9820 at iter 12911 **\n",
            "  ** New min smooth loss: 1.9819 at iter 12912 **\n",
            "  ** New min smooth loss: 1.9813 at iter 12913 **\n",
            "Iter: 13000/100000, Smooth Loss: 1.9858, Min Smooth Loss: 1.9813 (at iter 12913), Time: 63.87s\n",
            "  ** New min smooth loss: 1.9810 at iter 13026 **\n",
            "  ** New min smooth loss: 1.9810 at iter 13027 **\n",
            "  ** New min smooth loss: 1.9809 at iter 13028 **\n",
            "  ** New min smooth loss: 1.9808 at iter 13029 **\n",
            "  ** New min smooth loss: 1.9808 at iter 13034 **\n",
            "  ** New min smooth loss: 1.9804 at iter 13035 **\n",
            "  ** New min smooth loss: 1.9803 at iter 13037 **\n",
            "  ** New min smooth loss: 1.9801 at iter 13042 **\n",
            "  ** New min smooth loss: 1.9796 at iter 13043 **\n",
            "  ** New min smooth loss: 1.9795 at iter 13045 **\n",
            "  ** New min smooth loss: 1.9791 at iter 13046 **\n",
            "  ** New min smooth loss: 1.9786 at iter 13047 **\n",
            "  ** New min smooth loss: 1.9785 at iter 13048 **\n",
            "  ** New min smooth loss: 1.9785 at iter 13053 **\n",
            "  ** New min smooth loss: 1.9784 at iter 13054 **\n",
            "  ** New min smooth loss: 1.9780 at iter 13055 **\n",
            "  ** New min smooth loss: 1.9779 at iter 13056 **\n",
            "  ** New min smooth loss: 1.9779 at iter 13059 **\n",
            "  ** New min smooth loss: 1.9777 at iter 13060 **\n",
            "  ** New min smooth loss: 1.9777 at iter 13061 **\n",
            "  ** New min smooth loss: 1.9774 at iter 13063 **\n",
            "  ** New min smooth loss: 1.9773 at iter 13064 **\n",
            "  ** New min smooth loss: 1.9770 at iter 13065 **\n",
            "  ** New min smooth loss: 1.9765 at iter 13066 **\n",
            "  ** New min smooth loss: 1.9762 at iter 13071 **\n",
            "  ** New min smooth loss: 1.9759 at iter 13073 **\n",
            "  ** New min smooth loss: 1.9758 at iter 13074 **\n",
            "  ** New min smooth loss: 1.9758 at iter 13077 **\n",
            "  ** New min smooth loss: 1.9754 at iter 13078 **\n",
            "  ** New min smooth loss: 1.9742 at iter 13079 **\n",
            "  ** New min smooth loss: 1.9742 at iter 13080 **\n",
            "  ** New min smooth loss: 1.9739 at iter 13081 **\n",
            "  ** New min smooth loss: 1.9738 at iter 13085 **\n",
            "  ** New min smooth loss: 1.9732 at iter 13086 **\n",
            "  ** New min smooth loss: 1.9727 at iter 13087 **\n",
            "  ** New min smooth loss: 1.9723 at iter 13088 **\n",
            "  ** New min smooth loss: 1.9715 at iter 13089 **\n",
            "  ** New min smooth loss: 1.9711 at iter 13090 **\n",
            "  ** New min smooth loss: 1.9710 at iter 13091 **\n",
            "  ** New min smooth loss: 1.9701 at iter 13092 **\n",
            "  ** New min smooth loss: 1.9700 at iter 13094 **\n",
            "  ** New min smooth loss: 1.9697 at iter 13095 **\n",
            "Iter: 13100/100000, Smooth Loss: 1.9707, Min Smooth Loss: 1.9697 (at iter 13095), Time: 64.29s\n",
            "  ** New min smooth loss: 1.9694 at iter 13105 **\n",
            "  ** New min smooth loss: 1.9688 at iter 13106 **\n",
            "  ** New min smooth loss: 1.9686 at iter 13112 **\n",
            "  ** New min smooth loss: 1.9685 at iter 13114 **\n",
            "  ** New min smooth loss: 1.9682 at iter 13116 **\n",
            "  ** New min smooth loss: 1.9675 at iter 13118 **\n",
            "  ** New min smooth loss: 1.9674 at iter 13119 **\n",
            "  ** New min smooth loss: 1.9674 at iter 13120 **\n",
            "  ** New min smooth loss: 1.9671 at iter 13125 **\n",
            "  ** New min smooth loss: 1.9667 at iter 13127 **\n",
            "  ** New min smooth loss: 1.9666 at iter 13130 **\n",
            "  ** New min smooth loss: 1.9663 at iter 13132 **\n",
            "  ** New min smooth loss: 1.9661 at iter 13133 **\n",
            "  ** New min smooth loss: 1.9658 at iter 13154 **\n",
            "  ** New min smooth loss: 1.9653 at iter 13157 **\n",
            "  ** New min smooth loss: 1.9648 at iter 13158 **\n",
            "  ** New min smooth loss: 1.9644 at iter 13160 **\n",
            "  ** New min smooth loss: 1.9639 at iter 13161 **\n",
            "  ** New min smooth loss: 1.9638 at iter 13166 **\n",
            "  ** New min smooth loss: 1.9637 at iter 13168 **\n",
            "  ** New min smooth loss: 1.9635 at iter 13170 **\n",
            "  ** New min smooth loss: 1.9632 at iter 13171 **\n",
            "  ** New min smooth loss: 1.9630 at iter 13172 **\n",
            "  ** New min smooth loss: 1.9629 at iter 13173 **\n",
            "  ** New min smooth loss: 1.9622 at iter 13174 **\n",
            "  ** New min smooth loss: 1.9615 at iter 13175 **\n",
            "  ** New min smooth loss: 1.9615 at iter 13178 **\n",
            "  ** New min smooth loss: 1.9613 at iter 13179 **\n",
            "  ** New min smooth loss: 1.9607 at iter 13180 **\n",
            "  ** New min smooth loss: 1.9605 at iter 13181 **\n",
            "  ** New min smooth loss: 1.9600 at iter 13182 **\n",
            "  ** New min smooth loss: 1.9599 at iter 13189 **\n",
            "  ** New min smooth loss: 1.9598 at iter 13191 **\n",
            "  ** New min smooth loss: 1.9593 at iter 13196 **\n",
            "  ** New min smooth loss: 1.9590 at iter 13197 **\n",
            "Iter: 13200/100000, Smooth Loss: 1.9594, Min Smooth Loss: 1.9590 (at iter 13197), Time: 64.73s\n",
            "  ** New min smooth loss: 1.9582 at iter 13203 **\n",
            "  ** New min smooth loss: 1.9581 at iter 13205 **\n",
            "  ** New min smooth loss: 1.9580 at iter 13206 **\n",
            "  ** New min smooth loss: 1.9580 at iter 13208 **\n",
            "  ** New min smooth loss: 1.9579 at iter 13211 **\n",
            "  ** New min smooth loss: 1.9577 at iter 13213 **\n",
            "  ** New min smooth loss: 1.9577 at iter 13228 **\n",
            "  ** New min smooth loss: 1.9576 at iter 13229 **\n",
            "  ** New min smooth loss: 1.9573 at iter 13230 **\n",
            "Iter: 13300/100000, Smooth Loss: 1.9591, Min Smooth Loss: 1.9573 (at iter 13230), Time: 65.15s\n",
            "  ** New min smooth loss: 1.9569 at iter 13320 **\n",
            "  ** New min smooth loss: 1.9566 at iter 13325 **\n",
            "  ** New min smooth loss: 1.9565 at iter 13326 **\n",
            "  ** New min smooth loss: 1.9564 at iter 13329 **\n",
            "  ** New min smooth loss: 1.9564 at iter 13330 **\n",
            "  ** New min smooth loss: 1.9564 at iter 13335 **\n",
            "  ** New min smooth loss: 1.9564 at iter 13336 **\n",
            "  ** New min smooth loss: 1.9564 at iter 13339 **\n",
            "  ** New min smooth loss: 1.9561 at iter 13340 **\n",
            "  ** New min smooth loss: 1.9558 at iter 13341 **\n",
            "  ** New min smooth loss: 1.9555 at iter 13345 **\n",
            "  ** New min smooth loss: 1.9553 at iter 13346 **\n",
            "  ** New min smooth loss: 1.9551 at iter 13352 **\n",
            "  ** New min smooth loss: 1.9550 at iter 13353 **\n",
            "  ** New min smooth loss: 1.9544 at iter 13354 **\n",
            "  ** New min smooth loss: 1.9543 at iter 13356 **\n",
            "  ** New min smooth loss: 1.9539 at iter 13363 **\n",
            "  ** New min smooth loss: 1.9536 at iter 13367 **\n",
            "  ** New min smooth loss: 1.9535 at iter 13368 **\n",
            "  ** New min smooth loss: 1.9534 at iter 13369 **\n",
            "  ** New min smooth loss: 1.9533 at iter 13370 **\n",
            "  ** New min smooth loss: 1.9530 at iter 13374 **\n",
            "  ** New min smooth loss: 1.9529 at iter 13375 **\n",
            "  ** New min smooth loss: 1.9527 at iter 13376 **\n",
            "  ** New min smooth loss: 1.9524 at iter 13377 **\n",
            "  ** New min smooth loss: 1.9517 at iter 13399 **\n",
            "  ** New min smooth loss: 1.9517 at iter 13400 **\n",
            "Iter: 13400/100000, Smooth Loss: 1.9517, Min Smooth Loss: 1.9517 (at iter 13400), Time: 65.79s\n",
            "Iter: 13500/100000, Smooth Loss: 1.9555, Min Smooth Loss: 1.9517 (at iter 13400), Time: 66.40s\n",
            "  ** New min smooth loss: 1.9514 at iter 13531 **\n",
            "  ** New min smooth loss: 1.9507 at iter 13532 **\n",
            "  ** New min smooth loss: 1.9507 at iter 13534 **\n",
            "  ** New min smooth loss: 1.9506 at iter 13535 **\n",
            "  ** New min smooth loss: 1.9505 at iter 13536 **\n",
            "  ** New min smooth loss: 1.9503 at iter 13539 **\n",
            "  ** New min smooth loss: 1.9499 at iter 13540 **\n",
            "  ** New min smooth loss: 1.9498 at iter 13549 **\n",
            "  ** New min smooth loss: 1.9495 at iter 13550 **\n",
            "  ** New min smooth loss: 1.9488 at iter 13552 **\n",
            "  ** New min smooth loss: 1.9486 at iter 13553 **\n",
            "Iter: 13600/100000, Smooth Loss: 1.9544, Min Smooth Loss: 1.9486 (at iter 13553), Time: 67.06s\n",
            "Iter: 13700/100000, Smooth Loss: 1.9500, Min Smooth Loss: 1.9486 (at iter 13553), Time: 67.69s\n",
            "  ** New min smooth loss: 1.9484 at iter 13755 **\n",
            "  ** New min smooth loss: 1.9482 at iter 13756 **\n",
            "  ** New min smooth loss: 1.9478 at iter 13757 **\n",
            "  ** New min smooth loss: 1.9478 at iter 13788 **\n",
            "  ** New min smooth loss: 1.9477 at iter 13793 **\n",
            "  ** New min smooth loss: 1.9476 at iter 13795 **\n",
            "Iter: 13800/100000, Smooth Loss: 1.9485, Min Smooth Loss: 1.9476 (at iter 13795), Time: 68.34s\n",
            "  ** New min smooth loss: 1.9475 at iter 13844 **\n",
            "  ** New min smooth loss: 1.9475 at iter 13845 **\n",
            "  ** New min smooth loss: 1.9475 at iter 13849 **\n",
            "  ** New min smooth loss: 1.9474 at iter 13856 **\n",
            "  ** New min smooth loss: 1.9472 at iter 13857 **\n",
            "  ** New min smooth loss: 1.9470 at iter 13859 **\n",
            "  ** New min smooth loss: 1.9467 at iter 13861 **\n",
            "  ** New min smooth loss: 1.9467 at iter 13862 **\n",
            "  ** New min smooth loss: 1.9466 at iter 13863 **\n",
            "  ** New min smooth loss: 1.9463 at iter 13865 **\n",
            "  ** New min smooth loss: 1.9457 at iter 13866 **\n",
            "Iter: 13900/100000, Smooth Loss: 1.9492, Min Smooth Loss: 1.9457 (at iter 13866), Time: 68.99s\n",
            "Iter: 14000/100000, Smooth Loss: 1.9579, Min Smooth Loss: 1.9457 (at iter 13866), Time: 69.64s\n",
            "Iter: 14100/100000, Smooth Loss: 1.9799, Min Smooth Loss: 1.9457 (at iter 13866), Time: 70.14s\n",
            "Iter: 14200/100000, Smooth Loss: 1.9791, Min Smooth Loss: 1.9457 (at iter 13866), Time: 70.55s\n",
            "Iter: 14300/100000, Smooth Loss: 1.9782, Min Smooth Loss: 1.9457 (at iter 13866), Time: 70.97s\n",
            "Iter: 14400/100000, Smooth Loss: 1.9752, Min Smooth Loss: 1.9457 (at iter 13866), Time: 71.38s\n",
            "Iter: 14500/100000, Smooth Loss: 1.9752, Min Smooth Loss: 1.9457 (at iter 13866), Time: 71.80s\n",
            "Iter: 14600/100000, Smooth Loss: 1.9767, Min Smooth Loss: 1.9457 (at iter 13866), Time: 72.23s\n",
            "Iter: 14700/100000, Smooth Loss: 1.9714, Min Smooth Loss: 1.9457 (at iter 13866), Time: 72.64s\n",
            "Iter: 14800/100000, Smooth Loss: 1.9619, Min Smooth Loss: 1.9457 (at iter 13866), Time: 73.08s\n",
            "Iter: 14900/100000, Smooth Loss: 1.9565, Min Smooth Loss: 1.9457 (at iter 13866), Time: 73.50s\n",
            "Iter: 15000/100000, Smooth Loss: 1.9489, Min Smooth Loss: 1.9457 (at iter 13866), Time: 73.91s\n",
            "  ** New min smooth loss: 1.9455 at iter 15037 **\n",
            "  ** New min smooth loss: 1.9448 at iter 15038 **\n",
            "  ** New min smooth loss: 1.9446 at iter 15044 **\n",
            "  ** New min smooth loss: 1.9444 at iter 15045 **\n",
            "  ** New min smooth loss: 1.9444 at iter 15046 **\n",
            "  ** New min smooth loss: 1.9443 at iter 15078 **\n",
            "  ** New min smooth loss: 1.9440 at iter 15079 **\n",
            "  ** New min smooth loss: 1.9439 at iter 15081 **\n",
            "  ** New min smooth loss: 1.9436 at iter 15082 **\n",
            "  ** New min smooth loss: 1.9429 at iter 15084 **\n",
            "  ** New min smooth loss: 1.9428 at iter 15088 **\n",
            "  ** New min smooth loss: 1.9426 at iter 15091 **\n",
            "  ** New min smooth loss: 1.9424 at iter 15094 **\n",
            "  ** New min smooth loss: 1.9422 at iter 15099 **\n",
            "Iter: 15100/100000, Smooth Loss: 1.9424, Min Smooth Loss: 1.9422 (at iter 15099), Time: 74.36s\n",
            "  ** New min smooth loss: 1.9414 at iter 15101 **\n",
            "  ** New min smooth loss: 1.9413 at iter 15103 **\n",
            "  ** New min smooth loss: 1.9409 at iter 15105 **\n",
            "  ** New min smooth loss: 1.9408 at iter 15106 **\n",
            "  ** New min smooth loss: 1.9406 at iter 15107 **\n",
            "  ** New min smooth loss: 1.9403 at iter 15108 **\n",
            "  ** New min smooth loss: 1.9399 at iter 15109 **\n",
            "  ** New min smooth loss: 1.9398 at iter 15110 **\n",
            "  ** New min smooth loss: 1.9392 at iter 15111 **\n",
            "  ** New min smooth loss: 1.9392 at iter 15120 **\n",
            "  ** New min smooth loss: 1.9390 at iter 15123 **\n",
            "  ** New min smooth loss: 1.9387 at iter 15124 **\n",
            "  ** New min smooth loss: 1.9382 at iter 15125 **\n",
            "  ** New min smooth loss: 1.9381 at iter 15126 **\n",
            "  ** New min smooth loss: 1.9379 at iter 15127 **\n",
            "  ** New min smooth loss: 1.9378 at iter 15149 **\n",
            "  ** New min smooth loss: 1.9375 at iter 15150 **\n",
            "  ** New min smooth loss: 1.9369 at iter 15151 **\n",
            "  ** New min smooth loss: 1.9369 at iter 15152 **\n",
            "  ** New min smooth loss: 1.9365 at iter 15153 **\n",
            "  ** New min smooth loss: 1.9357 at iter 15154 **\n",
            "  ** New min smooth loss: 1.9356 at iter 15155 **\n",
            "  ** New min smooth loss: 1.9355 at iter 15156 **\n",
            "  ** New min smooth loss: 1.9353 at iter 15158 **\n",
            "  ** New min smooth loss: 1.9352 at iter 15160 **\n",
            "Iter: 15200/100000, Smooth Loss: 1.9396, Min Smooth Loss: 1.9352 (at iter 15160), Time: 74.78s\n",
            "  ** New min smooth loss: 1.9351 at iter 15276 **\n",
            "  ** New min smooth loss: 1.9351 at iter 15280 **\n",
            "  ** New min smooth loss: 1.9350 at iter 15283 **\n",
            "  ** New min smooth loss: 1.9348 at iter 15284 **\n",
            "  ** New min smooth loss: 1.9346 at iter 15285 **\n",
            "  ** New min smooth loss: 1.9345 at iter 15286 **\n",
            "  ** New min smooth loss: 1.9342 at iter 15293 **\n",
            "  ** New min smooth loss: 1.9338 at iter 15294 **\n",
            "Iter: 15300/100000, Smooth Loss: 1.9341, Min Smooth Loss: 1.9338 (at iter 15294), Time: 75.24s\n",
            "  ** New min smooth loss: 1.9333 at iter 15303 **\n",
            "  ** New min smooth loss: 1.9333 at iter 15304 **\n",
            "  ** New min smooth loss: 1.9332 at iter 15305 **\n",
            "  ** New min smooth loss: 1.9329 at iter 15306 **\n",
            "  ** New min smooth loss: 1.9329 at iter 15307 **\n",
            "  ** New min smooth loss: 1.9326 at iter 15310 **\n",
            "  ** New min smooth loss: 1.9323 at iter 15313 **\n",
            "  ** New min smooth loss: 1.9321 at iter 15316 **\n",
            "  ** New min smooth loss: 1.9317 at iter 15319 **\n",
            "  ** New min smooth loss: 1.9313 at iter 15321 **\n",
            "  ** New min smooth loss: 1.9313 at iter 15322 **\n",
            "  ** New min smooth loss: 1.9309 at iter 15323 **\n",
            "  ** New min smooth loss: 1.9309 at iter 15324 **\n",
            "  ** New min smooth loss: 1.9305 at iter 15325 **\n",
            "  ** New min smooth loss: 1.9299 at iter 15326 **\n",
            "  ** New min smooth loss: 1.9294 at iter 15327 **\n",
            "  ** New min smooth loss: 1.9293 at iter 15328 **\n",
            "  ** New min smooth loss: 1.9290 at iter 15332 **\n",
            "  ** New min smooth loss: 1.9290 at iter 15333 **\n",
            "  ** New min smooth loss: 1.9289 at iter 15337 **\n",
            "  ** New min smooth loss: 1.9286 at iter 15344 **\n",
            "  ** New min smooth loss: 1.9282 at iter 15345 **\n",
            "  ** New min smooth loss: 1.9282 at iter 15355 **\n",
            "  ** New min smooth loss: 1.9281 at iter 15356 **\n",
            "  ** New min smooth loss: 1.9281 at iter 15357 **\n",
            "  ** New min smooth loss: 1.9279 at iter 15359 **\n",
            "  ** New min smooth loss: 1.9274 at iter 15360 **\n",
            "  ** New min smooth loss: 1.9270 at iter 15361 **\n",
            "  ** New min smooth loss: 1.9269 at iter 15362 **\n",
            "  ** New min smooth loss: 1.9265 at iter 15364 **\n",
            "  ** New min smooth loss: 1.9264 at iter 15374 **\n",
            "  ** New min smooth loss: 1.9263 at iter 15375 **\n",
            "  ** New min smooth loss: 1.9262 at iter 15376 **\n",
            "  ** New min smooth loss: 1.9256 at iter 15377 **\n",
            "  ** New min smooth loss: 1.9254 at iter 15378 **\n",
            "  ** New min smooth loss: 1.9247 at iter 15379 **\n",
            "Iter: 15400/100000, Smooth Loss: 1.9260, Min Smooth Loss: 1.9247 (at iter 15379), Time: 75.74s\n",
            "  ** New min smooth loss: 1.9246 at iter 15414 **\n",
            "  ** New min smooth loss: 1.9244 at iter 15416 **\n",
            "  ** New min smooth loss: 1.9237 at iter 15418 **\n",
            "  ** New min smooth loss: 1.9232 at iter 15419 **\n",
            "  ** New min smooth loss: 1.9230 at iter 15420 **\n",
            "  ** New min smooth loss: 1.9228 at iter 15422 **\n",
            "  ** New min smooth loss: 1.9227 at iter 15428 **\n",
            "  ** New min smooth loss: 1.9224 at iter 15429 **\n",
            "  ** New min smooth loss: 1.9222 at iter 15430 **\n",
            "  ** New min smooth loss: 1.9221 at iter 15434 **\n",
            "  ** New min smooth loss: 1.9221 at iter 15435 **\n",
            "  ** New min smooth loss: 1.9219 at iter 15436 **\n",
            "  ** New min smooth loss: 1.9218 at iter 15437 **\n",
            "  ** New min smooth loss: 1.9217 at iter 15438 **\n",
            "  ** New min smooth loss: 1.9215 at iter 15439 **\n",
            "  ** New min smooth loss: 1.9213 at iter 15440 **\n",
            "  ** New min smooth loss: 1.9209 at iter 15442 **\n",
            "  ** New min smooth loss: 1.9208 at iter 15443 **\n",
            "  ** New min smooth loss: 1.9208 at iter 15445 **\n",
            "  ** New min smooth loss: 1.9207 at iter 15448 **\n",
            "  ** New min smooth loss: 1.9205 at iter 15451 **\n",
            "  ** New min smooth loss: 1.9202 at iter 15452 **\n",
            "  ** New min smooth loss: 1.9200 at iter 15453 **\n",
            "  ** New min smooth loss: 1.9199 at iter 15454 **\n",
            "  ** New min smooth loss: 1.9197 at iter 15457 **\n",
            "  ** New min smooth loss: 1.9196 at iter 15459 **\n",
            "  ** New min smooth loss: 1.9193 at iter 15461 **\n",
            "  ** New min smooth loss: 1.9189 at iter 15462 **\n",
            "  ** New min smooth loss: 1.9188 at iter 15463 **\n",
            "  ** New min smooth loss: 1.9186 at iter 15464 **\n",
            "  ** New min smooth loss: 1.9182 at iter 15466 **\n",
            "  ** New min smooth loss: 1.9181 at iter 15467 **\n",
            "  ** New min smooth loss: 1.9178 at iter 15468 **\n",
            "  ** New min smooth loss: 1.9177 at iter 15469 **\n",
            "  ** New min smooth loss: 1.9175 at iter 15470 **\n",
            "  ** New min smooth loss: 1.9175 at iter 15488 **\n",
            "  ** New min smooth loss: 1.9169 at iter 15489 **\n",
            "Iter: 15500/100000, Smooth Loss: 1.9183, Min Smooth Loss: 1.9169 (at iter 15489), Time: 76.26s\n",
            "  ** New min smooth loss: 1.9168 at iter 15511 **\n",
            "  ** New min smooth loss: 1.9168 at iter 15516 **\n",
            "  ** New min smooth loss: 1.9164 at iter 15519 **\n",
            "  ** New min smooth loss: 1.9159 at iter 15520 **\n",
            "  ** New min smooth loss: 1.9158 at iter 15522 **\n",
            "  ** New min smooth loss: 1.9155 at iter 15523 **\n",
            "  ** New min smooth loss: 1.9154 at iter 15534 **\n",
            "  ** New min smooth loss: 1.9148 at iter 15535 **\n",
            "  ** New min smooth loss: 1.9144 at iter 15536 **\n",
            "  ** New min smooth loss: 1.9141 at iter 15549 **\n",
            "  ** New min smooth loss: 1.9141 at iter 15555 **\n",
            "  ** New min smooth loss: 1.9140 at iter 15556 **\n",
            "  ** New min smooth loss: 1.9139 at iter 15558 **\n",
            "  ** New min smooth loss: 1.9135 at iter 15584 **\n",
            "  ** New min smooth loss: 1.9129 at iter 15585 **\n",
            "  ** New min smooth loss: 1.9125 at iter 15586 **\n",
            "  ** New min smooth loss: 1.9124 at iter 15587 **\n",
            "Iter: 15600/100000, Smooth Loss: 1.9156, Min Smooth Loss: 1.9124 (at iter 15587), Time: 76.73s\n",
            "Iter: 15700/100000, Smooth Loss: 1.9131, Min Smooth Loss: 1.9124 (at iter 15587), Time: 77.19s\n",
            "  ** New min smooth loss: 1.9122 at iter 15705 **\n",
            "  ** New min smooth loss: 1.9121 at iter 15706 **\n",
            "  ** New min smooth loss: 1.9121 at iter 15709 **\n",
            "  ** New min smooth loss: 1.9120 at iter 15731 **\n",
            "  ** New min smooth loss: 1.9115 at iter 15732 **\n",
            "  ** New min smooth loss: 1.9111 at iter 15733 **\n",
            "  ** New min smooth loss: 1.9107 at iter 15734 **\n",
            "  ** New min smooth loss: 1.9102 at iter 15735 **\n",
            "  ** New min smooth loss: 1.9097 at iter 15736 **\n",
            "  ** New min smooth loss: 1.9094 at iter 15737 **\n",
            "  ** New min smooth loss: 1.9093 at iter 15738 **\n",
            "  ** New min smooth loss: 1.9093 at iter 15740 **\n",
            "  ** New min smooth loss: 1.9091 at iter 15749 **\n",
            "Iter: 15800/100000, Smooth Loss: 1.9127, Min Smooth Loss: 1.9091 (at iter 15749), Time: 77.66s\n",
            "Iter: 15900/100000, Smooth Loss: 1.9142, Min Smooth Loss: 1.9091 (at iter 15749), Time: 78.13s\n",
            "  ** New min smooth loss: 1.9086 at iter 15967 **\n",
            "  ** New min smooth loss: 1.9079 at iter 15968 **\n",
            "  ** New min smooth loss: 1.9078 at iter 15969 **\n",
            "  ** New min smooth loss: 1.9074 at iter 15970 **\n",
            "  ** New min smooth loss: 1.9072 at iter 15971 **\n",
            "  ** New min smooth loss: 1.9071 at iter 15972 **\n",
            "  ** New min smooth loss: 1.9070 at iter 15973 **\n",
            "  ** New min smooth loss: 1.9067 at iter 15975 **\n",
            "  ** New min smooth loss: 1.9062 at iter 15976 **\n",
            "  ** New min smooth loss: 1.9057 at iter 15977 **\n",
            "  ** New min smooth loss: 1.9055 at iter 15984 **\n",
            "  ** New min smooth loss: 1.9050 at iter 15985 **\n",
            "  ** New min smooth loss: 1.9044 at iter 15986 **\n",
            "  ** New min smooth loss: 1.9041 at iter 15990 **\n",
            "  ** New min smooth loss: 1.9035 at iter 15991 **\n",
            "  ** New min smooth loss: 1.9034 at iter 15993 **\n",
            "  ** New min smooth loss: 1.9033 at iter 15996 **\n",
            "Iter: 16000/100000, Smooth Loss: 1.9041, Min Smooth Loss: 1.9033 (at iter 15996), Time: 78.59s\n",
            "  ** New min smooth loss: 1.9033 at iter 16009 **\n",
            "  ** New min smooth loss: 1.9029 at iter 16013 **\n",
            "  ** New min smooth loss: 1.9029 at iter 16014 **\n",
            "  ** New min smooth loss: 1.9028 at iter 16015 **\n",
            "  ** New min smooth loss: 1.9027 at iter 16017 **\n",
            "  ** New min smooth loss: 1.9023 at iter 16018 **\n",
            "  ** New min smooth loss: 1.9019 at iter 16019 **\n",
            "  ** New min smooth loss: 1.9019 at iter 16020 **\n",
            "  ** New min smooth loss: 1.9018 at iter 16021 **\n",
            "  ** New min smooth loss: 1.9014 at iter 16022 **\n",
            "  ** New min smooth loss: 1.9008 at iter 16024 **\n",
            "  ** New min smooth loss: 1.9006 at iter 16025 **\n",
            "  ** New min smooth loss: 1.9002 at iter 16026 **\n",
            "  ** New min smooth loss: 1.9001 at iter 16031 **\n",
            "  ** New min smooth loss: 1.9000 at iter 16033 **\n",
            "  ** New min smooth loss: 1.8999 at iter 16039 **\n",
            "  ** New min smooth loss: 1.8995 at iter 16040 **\n",
            "  ** New min smooth loss: 1.8995 at iter 16042 **\n",
            "  ** New min smooth loss: 1.8995 at iter 16043 **\n",
            "  ** New min smooth loss: 1.8993 at iter 16057 **\n",
            "  ** New min smooth loss: 1.8989 at iter 16059 **\n",
            "  ** New min smooth loss: 1.8987 at iter 16060 **\n",
            "  ** New min smooth loss: 1.8984 at iter 16061 **\n",
            "  ** New min smooth loss: 1.8984 at iter 16062 **\n",
            "  ** New min smooth loss: 1.8983 at iter 16065 **\n",
            "  ** New min smooth loss: 1.8982 at iter 16069 **\n",
            "  ** New min smooth loss: 1.8980 at iter 16071 **\n",
            "  ** New min smooth loss: 1.8979 at iter 16072 **\n",
            "  ** New min smooth loss: 1.8974 at iter 16073 **\n",
            "  ** New min smooth loss: 1.8971 at iter 16088 **\n",
            "  ** New min smooth loss: 1.8970 at iter 16089 **\n",
            "  ** New min smooth loss: 1.8969 at iter 16090 **\n",
            "  ** New min smooth loss: 1.8968 at iter 16092 **\n",
            "  ** New min smooth loss: 1.8968 at iter 16096 **\n",
            "  ** New min smooth loss: 1.8967 at iter 16097 **\n",
            "  ** New min smooth loss: 1.8965 at iter 16098 **\n",
            "  ** New min smooth loss: 1.8964 at iter 16099 **\n",
            "Iter: 16100/100000, Smooth Loss: 1.8969, Min Smooth Loss: 1.8964 (at iter 16099), Time: 79.07s\n",
            "Iter: 16200/100000, Smooth Loss: 1.8977, Min Smooth Loss: 1.8964 (at iter 16099), Time: 79.53s\n",
            "  ** New min smooth loss: 1.8963 at iter 16213 **\n",
            "  ** New min smooth loss: 1.8958 at iter 16215 **\n",
            "  ** New min smooth loss: 1.8956 at iter 16216 **\n",
            "  ** New min smooth loss: 1.8953 at iter 16224 **\n",
            "  ** New min smooth loss: 1.8951 at iter 16226 **\n",
            "  ** New min smooth loss: 1.8950 at iter 16229 **\n",
            "  ** New min smooth loss: 1.8944 at iter 16230 **\n",
            "  ** New min smooth loss: 1.8941 at iter 16233 **\n",
            "  ** New min smooth loss: 1.8940 at iter 16234 **\n",
            "  ** New min smooth loss: 1.8937 at iter 16235 **\n",
            "  ** New min smooth loss: 1.8936 at iter 16237 **\n",
            "  ** New min smooth loss: 1.8935 at iter 16245 **\n",
            "  ** New min smooth loss: 1.8929 at iter 16247 **\n",
            "  ** New min smooth loss: 1.8924 at iter 16249 **\n",
            "  ** New min smooth loss: 1.8916 at iter 16250 **\n",
            "  ** New min smooth loss: 1.8909 at iter 16251 **\n",
            "  ** New min smooth loss: 1.8909 at iter 16266 **\n",
            "  ** New min smooth loss: 1.8907 at iter 16267 **\n",
            "  ** New min smooth loss: 1.8905 at iter 16275 **\n",
            "  ** New min smooth loss: 1.8904 at iter 16276 **\n",
            "  ** New min smooth loss: 1.8902 at iter 16277 **\n",
            "  ** New min smooth loss: 1.8895 at iter 16278 **\n",
            "  ** New min smooth loss: 1.8894 at iter 16280 **\n",
            "  ** New min smooth loss: 1.8893 at iter 16283 **\n",
            "  ** New min smooth loss: 1.8888 at iter 16287 **\n",
            "  ** New min smooth loss: 1.8882 at iter 16288 **\n",
            "  ** New min smooth loss: 1.8880 at iter 16289 **\n",
            "  ** New min smooth loss: 1.8876 at iter 16290 **\n",
            "  ** New min smooth loss: 1.8874 at iter 16291 **\n",
            "  ** New min smooth loss: 1.8872 at iter 16297 **\n",
            "  ** New min smooth loss: 1.8867 at iter 16298 **\n",
            "  ** New min smooth loss: 1.8866 at iter 16300 **\n",
            "Iter: 16300/100000, Smooth Loss: 1.8866, Min Smooth Loss: 1.8866 (at iter 16300), Time: 80.10s\n",
            "  ** New min smooth loss: 1.8864 at iter 16313 **\n",
            "  ** New min smooth loss: 1.8860 at iter 16314 **\n",
            "  ** New min smooth loss: 1.8858 at iter 16315 **\n",
            "  ** New min smooth loss: 1.8853 at iter 16316 **\n",
            "  ** New min smooth loss: 1.8844 at iter 16317 **\n",
            "  ** New min smooth loss: 1.8844 at iter 16318 **\n",
            "  ** New min smooth loss: 1.8842 at iter 16319 **\n",
            "  ** New min smooth loss: 1.8836 at iter 16320 **\n",
            "  ** New min smooth loss: 1.8833 at iter 16321 **\n",
            "  ** New min smooth loss: 1.8830 at iter 16322 **\n",
            "  ** New min smooth loss: 1.8827 at iter 16327 **\n",
            "  ** New min smooth loss: 1.8823 at iter 16328 **\n",
            "  ** New min smooth loss: 1.8817 at iter 16329 **\n",
            "  ** New min smooth loss: 1.8813 at iter 16330 **\n",
            "  ** New min smooth loss: 1.8809 at iter 16338 **\n",
            "  ** New min smooth loss: 1.8808 at iter 16340 **\n",
            "  ** New min smooth loss: 1.8805 at iter 16341 **\n",
            "  ** New min smooth loss: 1.8805 at iter 16344 **\n",
            "  ** New min smooth loss: 1.8803 at iter 16345 **\n",
            "  ** New min smooth loss: 1.8802 at iter 16371 **\n",
            "  ** New min smooth loss: 1.8801 at iter 16373 **\n",
            "  ** New min smooth loss: 1.8799 at iter 16375 **\n",
            "  ** New min smooth loss: 1.8797 at iter 16377 **\n",
            "Iter: 16400/100000, Smooth Loss: 1.8847, Min Smooth Loss: 1.8797 (at iter 16377), Time: 80.80s\n",
            "Iter: 16500/100000, Smooth Loss: 1.8938, Min Smooth Loss: 1.8797 (at iter 16377), Time: 81.45s\n",
            "Iter: 16600/100000, Smooth Loss: 1.8864, Min Smooth Loss: 1.8797 (at iter 16377), Time: 82.18s\n",
            "Iter: 16700/100000, Smooth Loss: 1.8937, Min Smooth Loss: 1.8797 (at iter 16377), Time: 82.85s\n",
            "Iter: 16800/100000, Smooth Loss: 1.8931, Min Smooth Loss: 1.8797 (at iter 16377), Time: 83.49s\n",
            "Iter: 16900/100000, Smooth Loss: 1.8863, Min Smooth Loss: 1.8797 (at iter 16377), Time: 84.20s\n",
            "Iter: 17000/100000, Smooth Loss: 1.8819, Min Smooth Loss: 1.8797 (at iter 16377), Time: 84.90s\n",
            "Iter: 17100/100000, Smooth Loss: 1.8820, Min Smooth Loss: 1.8797 (at iter 16377), Time: 85.75s\n",
            "  ** New min smooth loss: 1.8795 at iter 17124 **\n",
            "  ** New min smooth loss: 1.8795 at iter 17125 **\n",
            "  ** New min smooth loss: 1.8793 at iter 17126 **\n",
            "  ** New min smooth loss: 1.8792 at iter 17127 **\n",
            "  ** New min smooth loss: 1.8789 at iter 17128 **\n",
            "  ** New min smooth loss: 1.8788 at iter 17132 **\n",
            "  ** New min smooth loss: 1.8784 at iter 17133 **\n",
            "  ** New min smooth loss: 1.8781 at iter 17134 **\n",
            "  ** New min smooth loss: 1.8780 at iter 17142 **\n",
            "  ** New min smooth loss: 1.8780 at iter 17146 **\n",
            "  ** New min smooth loss: 1.8780 at iter 17155 **\n",
            "  ** New min smooth loss: 1.8779 at iter 17157 **\n",
            "  ** New min smooth loss: 1.8778 at iter 17158 **\n",
            "  ** New min smooth loss: 1.8775 at iter 17159 **\n",
            "  ** New min smooth loss: 1.8771 at iter 17160 **\n",
            "  ** New min smooth loss: 1.8771 at iter 17164 **\n",
            "  ** New min smooth loss: 1.8767 at iter 17165 **\n",
            "  ** New min smooth loss: 1.8767 at iter 17177 **\n",
            "  ** New min smooth loss: 1.8765 at iter 17178 **\n",
            "  ** New min smooth loss: 1.8763 at iter 17188 **\n",
            "  ** New min smooth loss: 1.8762 at iter 17189 **\n",
            "  ** New min smooth loss: 1.8759 at iter 17190 **\n",
            "  ** New min smooth loss: 1.8757 at iter 17195 **\n",
            "  ** New min smooth loss: 1.8756 at iter 17196 **\n",
            "  ** New min smooth loss: 1.8754 at iter 17197 **\n",
            "  ** New min smooth loss: 1.8753 at iter 17199 **\n",
            "  ** New min smooth loss: 1.8745 at iter 17200 **\n",
            "Iter: 17200/100000, Smooth Loss: 1.8745, Min Smooth Loss: 1.8745 (at iter 17200), Time: 86.23s\n",
            "  ** New min smooth loss: 1.8743 at iter 17201 **\n",
            "  ** New min smooth loss: 1.8740 at iter 17204 **\n",
            "  ** New min smooth loss: 1.8738 at iter 17286 **\n",
            "  ** New min smooth loss: 1.8732 at iter 17287 **\n",
            "  ** New min smooth loss: 1.8729 at iter 17288 **\n",
            "  ** New min smooth loss: 1.8725 at iter 17289 **\n",
            "  ** New min smooth loss: 1.8724 at iter 17290 **\n",
            "  ** New min smooth loss: 1.8723 at iter 17293 **\n",
            "  ** New min smooth loss: 1.8721 at iter 17294 **\n",
            "  ** New min smooth loss: 1.8720 at iter 17295 **\n",
            "  ** New min smooth loss: 1.8715 at iter 17296 **\n",
            "  ** New min smooth loss: 1.8712 at iter 17297 **\n",
            "  ** New min smooth loss: 1.8710 at iter 17299 **\n",
            "Iter: 17300/100000, Smooth Loss: 1.8713, Min Smooth Loss: 1.8710 (at iter 17299), Time: 86.67s\n",
            "  ** New min smooth loss: 1.8709 at iter 17302 **\n",
            "  ** New min smooth loss: 1.8707 at iter 17306 **\n",
            "  ** New min smooth loss: 1.8706 at iter 17307 **\n",
            "  ** New min smooth loss: 1.8705 at iter 17321 **\n",
            "  ** New min smooth loss: 1.8705 at iter 17323 **\n",
            "  ** New min smooth loss: 1.8704 at iter 17324 **\n",
            "  ** New min smooth loss: 1.8701 at iter 17325 **\n",
            "  ** New min smooth loss: 1.8699 at iter 17327 **\n",
            "  ** New min smooth loss: 1.8696 at iter 17329 **\n",
            "  ** New min smooth loss: 1.8696 at iter 17330 **\n",
            "  ** New min smooth loss: 1.8696 at iter 17334 **\n",
            "  ** New min smooth loss: 1.8693 at iter 17335 **\n",
            "  ** New min smooth loss: 1.8693 at iter 17338 **\n",
            "  ** New min smooth loss: 1.8690 at iter 17380 **\n",
            "  ** New min smooth loss: 1.8687 at iter 17381 **\n",
            "  ** New min smooth loss: 1.8686 at iter 17383 **\n",
            "  ** New min smooth loss: 1.8685 at iter 17385 **\n",
            "  ** New min smooth loss: 1.8682 at iter 17392 **\n",
            "  ** New min smooth loss: 1.8678 at iter 17393 **\n",
            "  ** New min smooth loss: 1.8676 at iter 17394 **\n",
            "  ** New min smooth loss: 1.8672 at iter 17395 **\n",
            "  ** New min smooth loss: 1.8672 at iter 17396 **\n",
            "  ** New min smooth loss: 1.8663 at iter 17397 **\n",
            "  ** New min smooth loss: 1.8661 at iter 17398 **\n",
            "  ** New min smooth loss: 1.8658 at iter 17399 **\n",
            "  ** New min smooth loss: 1.8656 at iter 17400 **\n",
            "Iter: 17400/100000, Smooth Loss: 1.8656, Min Smooth Loss: 1.8656 (at iter 17400), Time: 87.16s\n",
            "  ** New min smooth loss: 1.8656 at iter 17403 **\n",
            "  ** New min smooth loss: 1.8653 at iter 17404 **\n",
            "  ** New min smooth loss: 1.8653 at iter 17422 **\n",
            "  ** New min smooth loss: 1.8651 at iter 17427 **\n",
            "  ** New min smooth loss: 1.8649 at iter 17428 **\n",
            "  ** New min smooth loss: 1.8644 at iter 17431 **\n",
            "  ** New min smooth loss: 1.8641 at iter 17432 **\n",
            "  ** New min smooth loss: 1.8640 at iter 17439 **\n",
            "  ** New min smooth loss: 1.8640 at iter 17440 **\n",
            "  ** New min smooth loss: 1.8638 at iter 17453 **\n",
            "Iter: 17500/100000, Smooth Loss: 1.8644, Min Smooth Loss: 1.8638 (at iter 17453), Time: 87.61s\n",
            "  ** New min smooth loss: 1.8635 at iter 17506 **\n",
            "  ** New min smooth loss: 1.8631 at iter 17507 **\n",
            "  ** New min smooth loss: 1.8624 at iter 17508 **\n",
            "  ** New min smooth loss: 1.8619 at iter 17509 **\n",
            "  ** New min smooth loss: 1.8618 at iter 17510 **\n",
            "  ** New min smooth loss: 1.8616 at iter 17511 **\n",
            "Iter: 17600/100000, Smooth Loss: 1.8723, Min Smooth Loss: 1.8616 (at iter 17511), Time: 88.08s\n",
            "Iter: 17700/100000, Smooth Loss: 1.8723, Min Smooth Loss: 1.8616 (at iter 17511), Time: 88.53s\n",
            "Iter: 17800/100000, Smooth Loss: 1.8759, Min Smooth Loss: 1.8616 (at iter 17511), Time: 88.97s\n",
            "Iter: 17900/100000, Smooth Loss: 1.8811, Min Smooth Loss: 1.8616 (at iter 17511), Time: 89.45s\n",
            "Iter: 18000/100000, Smooth Loss: 1.8791, Min Smooth Loss: 1.8616 (at iter 17511), Time: 89.91s\n",
            "Iter: 18100/100000, Smooth Loss: 1.8729, Min Smooth Loss: 1.8616 (at iter 17511), Time: 90.40s\n",
            "Iter: 18200/100000, Smooth Loss: 1.8808, Min Smooth Loss: 1.8616 (at iter 17511), Time: 90.85s\n",
            "Iter: 18300/100000, Smooth Loss: 1.8853, Min Smooth Loss: 1.8616 (at iter 17511), Time: 91.33s\n",
            "Iter: 18400/100000, Smooth Loss: 1.8778, Min Smooth Loss: 1.8616 (at iter 17511), Time: 91.80s\n",
            "Iter: 18500/100000, Smooth Loss: 1.8797, Min Smooth Loss: 1.8616 (at iter 17511), Time: 92.28s\n",
            "Iter: 18600/100000, Smooth Loss: 1.8781, Min Smooth Loss: 1.8616 (at iter 17511), Time: 92.71s\n",
            "Iter: 18700/100000, Smooth Loss: 1.8741, Min Smooth Loss: 1.8616 (at iter 17511), Time: 93.17s\n",
            "Iter: 18800/100000, Smooth Loss: 1.8774, Min Smooth Loss: 1.8616 (at iter 17511), Time: 93.63s\n",
            "Iter: 18900/100000, Smooth Loss: 1.8778, Min Smooth Loss: 1.8616 (at iter 17511), Time: 94.08s\n",
            "Iter: 19000/100000, Smooth Loss: 1.8689, Min Smooth Loss: 1.8616 (at iter 17511), Time: 94.87s\n",
            "Iter: 19100/100000, Smooth Loss: 1.8698, Min Smooth Loss: 1.8616 (at iter 17511), Time: 95.48s\n",
            "Iter: 19200/100000, Smooth Loss: 1.8666, Min Smooth Loss: 1.8616 (at iter 17511), Time: 96.14s\n",
            "  ** New min smooth loss: 1.8612 at iter 19292 **\n",
            "Iter: 19300/100000, Smooth Loss: 1.8620, Min Smooth Loss: 1.8612 (at iter 19292), Time: 96.84s\n",
            "  ** New min smooth loss: 1.8612 at iter 19322 **\n",
            "  ** New min smooth loss: 1.8612 at iter 19323 **\n",
            "  ** New min smooth loss: 1.8611 at iter 19324 **\n",
            "  ** New min smooth loss: 1.8603 at iter 19326 **\n",
            "  ** New min smooth loss: 1.8601 at iter 19327 **\n",
            "  ** New min smooth loss: 1.8594 at iter 19329 **\n",
            "  ** New min smooth loss: 1.8592 at iter 19330 **\n",
            "  ** New min smooth loss: 1.8586 at iter 19331 **\n",
            "  ** New min smooth loss: 1.8580 at iter 19332 **\n",
            "  ** New min smooth loss: 1.8576 at iter 19334 **\n",
            "  ** New min smooth loss: 1.8570 at iter 19335 **\n",
            "  ** New min smooth loss: 1.8569 at iter 19336 **\n",
            "  ** New min smooth loss: 1.8567 at iter 19361 **\n",
            "  ** New min smooth loss: 1.8564 at iter 19372 **\n",
            "  ** New min smooth loss: 1.8561 at iter 19373 **\n",
            "  ** New min smooth loss: 1.8561 at iter 19375 **\n",
            "  ** New min smooth loss: 1.8559 at iter 19376 **\n",
            "  ** New min smooth loss: 1.8557 at iter 19377 **\n",
            "  ** New min smooth loss: 1.8554 at iter 19378 **\n",
            "  ** New min smooth loss: 1.8554 at iter 19380 **\n",
            "  ** New min smooth loss: 1.8550 at iter 19381 **\n",
            "  ** New min smooth loss: 1.8547 at iter 19382 **\n",
            "  ** New min smooth loss: 1.8546 at iter 19383 **\n",
            "  ** New min smooth loss: 1.8544 at iter 19386 **\n",
            "  ** New min smooth loss: 1.8544 at iter 19388 **\n",
            "  ** New min smooth loss: 1.8544 at iter 19389 **\n",
            "  ** New min smooth loss: 1.8542 at iter 19390 **\n",
            "  ** New min smooth loss: 1.8539 at iter 19391 **\n",
            "  ** New min smooth loss: 1.8535 at iter 19392 **\n",
            "  ** New min smooth loss: 1.8530 at iter 19393 **\n",
            "  ** New min smooth loss: 1.8527 at iter 19394 **\n",
            "  ** New min smooth loss: 1.8523 at iter 19395 **\n",
            "  ** New min smooth loss: 1.8520 at iter 19396 **\n",
            "  ** New min smooth loss: 1.8520 at iter 19399 **\n",
            "Iter: 19400/100000, Smooth Loss: 1.8521, Min Smooth Loss: 1.8520 (at iter 19399), Time: 97.54s\n",
            "  ** New min smooth loss: 1.8514 at iter 19438 **\n",
            "  ** New min smooth loss: 1.8513 at iter 19439 **\n",
            "  ** New min smooth loss: 1.8509 at iter 19440 **\n",
            "  ** New min smooth loss: 1.8503 at iter 19441 **\n",
            "  ** New min smooth loss: 1.8499 at iter 19442 **\n",
            "  ** New min smooth loss: 1.8495 at iter 19443 **\n",
            "  ** New min smooth loss: 1.8493 at iter 19444 **\n",
            "  ** New min smooth loss: 1.8493 at iter 19446 **\n",
            "  ** New min smooth loss: 1.8490 at iter 19448 **\n",
            "  ** New min smooth loss: 1.8489 at iter 19449 **\n",
            "  ** New min smooth loss: 1.8487 at iter 19486 **\n",
            "  ** New min smooth loss: 1.8487 at iter 19487 **\n",
            "  ** New min smooth loss: 1.8482 at iter 19488 **\n",
            "  ** New min smooth loss: 1.8478 at iter 19499 **\n",
            "  ** New min smooth loss: 1.8477 at iter 19500 **\n",
            "Iter: 19500/100000, Smooth Loss: 1.8477, Min Smooth Loss: 1.8477 (at iter 19500), Time: 98.23s\n",
            "  ** New min smooth loss: 1.8474 at iter 19501 **\n",
            "  ** New min smooth loss: 1.8472 at iter 19506 **\n",
            "  ** New min smooth loss: 1.8470 at iter 19507 **\n",
            "  ** New min smooth loss: 1.8465 at iter 19508 **\n",
            "  ** New min smooth loss: 1.8464 at iter 19509 **\n",
            "  ** New min smooth loss: 1.8463 at iter 19510 **\n",
            "  ** New min smooth loss: 1.8456 at iter 19512 **\n",
            "  ** New min smooth loss: 1.8452 at iter 19513 **\n",
            "  ** New min smooth loss: 1.8451 at iter 19514 **\n",
            "  ** New min smooth loss: 1.8449 at iter 19545 **\n",
            "  ** New min smooth loss: 1.8446 at iter 19546 **\n",
            "  ** New min smooth loss: 1.8446 at iter 19548 **\n",
            "  ** New min smooth loss: 1.8445 at iter 19557 **\n",
            "  ** New min smooth loss: 1.8443 at iter 19558 **\n",
            "Iter: 19600/100000, Smooth Loss: 1.8502, Min Smooth Loss: 1.8443 (at iter 19558), Time: 98.89s\n",
            "Iter: 19700/100000, Smooth Loss: 1.8539, Min Smooth Loss: 1.8443 (at iter 19558), Time: 99.59s\n",
            "Iter: 19800/100000, Smooth Loss: 1.8553, Min Smooth Loss: 1.8443 (at iter 19558), Time: 100.24s\n",
            "Iter: 19900/100000, Smooth Loss: 1.8483, Min Smooth Loss: 1.8443 (at iter 19558), Time: 100.69s\n",
            "  ** New min smooth loss: 1.8442 at iter 19977 **\n",
            "Iter: 20000/100000, Smooth Loss: 1.8457, Min Smooth Loss: 1.8442 (at iter 19977), Time: 101.16s\n",
            "--- Synthesized text at iter 20000 ---\n",
            "\n",
            "\"Yem of rage cor chernionr. . .\"\n",
            "\"Lowe he you ghuin. Ske beared. S\" horsed fizards klfted enf tull me forn leak, buck.\n",
            "\"At - they, about Tiscous was on milady got rigrtered, whinngring's ffriep!\"  sh\n",
            "---\n",
            "  ** New min smooth loss: 1.8442 at iter 20024 **\n",
            "  ** New min smooth loss: 1.8441 at iter 20028 **\n",
            "  ** New min smooth loss: 1.8438 at iter 20030 **\n",
            "  ** New min smooth loss: 1.8434 at iter 20031 **\n",
            "  ** New min smooth loss: 1.8432 at iter 20033 **\n",
            "  ** New min smooth loss: 1.8432 at iter 20037 **\n",
            "  ** New min smooth loss: 1.8430 at iter 20038 **\n",
            "  ** New min smooth loss: 1.8430 at iter 20099 **\n",
            "  ** New min smooth loss: 1.8424 at iter 20100 **\n",
            "Iter: 20100/100000, Smooth Loss: 1.8424, Min Smooth Loss: 1.8424 (at iter 20100), Time: 101.64s\n",
            "  ** New min smooth loss: 1.8421 at iter 20102 **\n",
            "  ** New min smooth loss: 1.8420 at iter 20103 **\n",
            "  ** New min smooth loss: 1.8415 at iter 20104 **\n",
            "  ** New min smooth loss: 1.8412 at iter 20125 **\n",
            "  ** New min smooth loss: 1.8409 at iter 20126 **\n",
            "  ** New min smooth loss: 1.8407 at iter 20127 **\n",
            "  ** New min smooth loss: 1.8401 at iter 20138 **\n",
            "  ** New min smooth loss: 1.8399 at iter 20139 **\n",
            "  ** New min smooth loss: 1.8398 at iter 20141 **\n",
            "  ** New min smooth loss: 1.8397 at iter 20142 **\n",
            "  ** New min smooth loss: 1.8396 at iter 20144 **\n",
            "  ** New min smooth loss: 1.8395 at iter 20146 **\n",
            "  ** New min smooth loss: 1.8394 at iter 20163 **\n",
            "  ** New min smooth loss: 1.8393 at iter 20177 **\n",
            "  ** New min smooth loss: 1.8389 at iter 20178 **\n",
            "  ** New min smooth loss: 1.8388 at iter 20179 **\n",
            "  ** New min smooth loss: 1.8385 at iter 20180 **\n",
            "  ** New min smooth loss: 1.8384 at iter 20183 **\n",
            "  ** New min smooth loss: 1.8380 at iter 20184 **\n",
            "  ** New min smooth loss: 1.8376 at iter 20185 **\n",
            "  ** New min smooth loss: 1.8372 at iter 20186 **\n",
            "  ** New min smooth loss: 1.8371 at iter 20190 **\n",
            "  ** New min smooth loss: 1.8369 at iter 20191 **\n",
            "  ** New min smooth loss: 1.8364 at iter 20192 **\n",
            "  ** New min smooth loss: 1.8363 at iter 20193 **\n",
            "Iter: 20200/100000, Smooth Loss: 1.8367, Min Smooth Loss: 1.8363 (at iter 20193), Time: 102.12s\n",
            "  ** New min smooth loss: 1.8362 at iter 20222 **\n",
            "  ** New min smooth loss: 1.8359 at iter 20258 **\n",
            "  ** New min smooth loss: 1.8353 at iter 20259 **\n",
            "  ** New min smooth loss: 1.8353 at iter 20268 **\n",
            "  ** New min smooth loss: 1.8351 at iter 20277 **\n",
            "Iter: 20300/100000, Smooth Loss: 1.8390, Min Smooth Loss: 1.8351 (at iter 20277), Time: 102.58s\n",
            "Iter: 20400/100000, Smooth Loss: 1.8414, Min Smooth Loss: 1.8351 (at iter 20277), Time: 103.04s\n",
            "Iter: 20500/100000, Smooth Loss: 1.8362, Min Smooth Loss: 1.8351 (at iter 20277), Time: 103.50s\n",
            "  ** New min smooth loss: 1.8350 at iter 20503 **\n",
            "  ** New min smooth loss: 1.8345 at iter 20504 **\n",
            "  ** New min smooth loss: 1.8342 at iter 20540 **\n",
            "  ** New min smooth loss: 1.8341 at iter 20541 **\n",
            "  ** New min smooth loss: 1.8335 at iter 20548 **\n",
            "  ** New min smooth loss: 1.8332 at iter 20553 **\n",
            "  ** New min smooth loss: 1.8326 at iter 20554 **\n",
            "  ** New min smooth loss: 1.8325 at iter 20555 **\n",
            "  ** New min smooth loss: 1.8324 at iter 20556 **\n",
            "  ** New min smooth loss: 1.8323 at iter 20557 **\n",
            "  ** New min smooth loss: 1.8317 at iter 20558 **\n",
            "  ** New min smooth loss: 1.8314 at iter 20559 **\n",
            "  ** New min smooth loss: 1.8313 at iter 20560 **\n",
            "Iter: 20600/100000, Smooth Loss: 1.8379, Min Smooth Loss: 1.8313 (at iter 20560), Time: 103.97s\n",
            "Iter: 20700/100000, Smooth Loss: 1.8348, Min Smooth Loss: 1.8313 (at iter 20560), Time: 104.44s\n",
            "Iter: 20800/100000, Smooth Loss: 1.8364, Min Smooth Loss: 1.8313 (at iter 20560), Time: 104.90s\n",
            "  ** New min smooth loss: 1.8312 at iter 20872 **\n",
            "  ** New min smooth loss: 1.8307 at iter 20885 **\n",
            "  ** New min smooth loss: 1.8307 at iter 20889 **\n",
            "Iter: 20900/100000, Smooth Loss: 1.8308, Min Smooth Loss: 1.8307 (at iter 20889), Time: 105.37s\n",
            "  ** New min smooth loss: 1.8305 at iter 20910 **\n",
            "  ** New min smooth loss: 1.8301 at iter 20916 **\n",
            "  ** New min smooth loss: 1.8298 at iter 20917 **\n",
            "  ** New min smooth loss: 1.8295 at iter 20918 **\n",
            "  ** New min smooth loss: 1.8293 at iter 20920 **\n",
            "  ** New min smooth loss: 1.8292 at iter 20921 **\n",
            "  ** New min smooth loss: 1.8289 at iter 20922 **\n",
            "  ** New min smooth loss: 1.8287 at iter 20923 **\n",
            "  ** New min smooth loss: 1.8284 at iter 20924 **\n",
            "  ** New min smooth loss: 1.8277 at iter 20926 **\n",
            "  ** New min smooth loss: 1.8276 at iter 20928 **\n",
            "  ** New min smooth loss: 1.8273 at iter 20929 **\n",
            "  ** New min smooth loss: 1.8272 at iter 20930 **\n",
            "  ** New min smooth loss: 1.8269 at iter 20933 **\n",
            "  ** New min smooth loss: 1.8267 at iter 20934 **\n",
            "  ** New min smooth loss: 1.8265 at iter 20935 **\n",
            "  ** New min smooth loss: 1.8263 at iter 20948 **\n",
            "  ** New min smooth loss: 1.8260 at iter 20950 **\n",
            "  ** New min smooth loss: 1.8255 at iter 20951 **\n",
            "  ** New min smooth loss: 1.8250 at iter 20952 **\n",
            "  ** New min smooth loss: 1.8249 at iter 20953 **\n",
            "  ** New min smooth loss: 1.8248 at iter 20954 **\n",
            "  ** New min smooth loss: 1.8246 at iter 20962 **\n",
            "  ** New min smooth loss: 1.8239 at iter 20963 **\n",
            "  ** New min smooth loss: 1.8237 at iter 20964 **\n",
            "  ** New min smooth loss: 1.8232 at iter 20971 **\n",
            "  ** New min smooth loss: 1.8232 at iter 20974 **\n",
            "  ** New min smooth loss: 1.8231 at iter 20990 **\n",
            "  ** New min smooth loss: 1.8230 at iter 20991 **\n",
            "  ** New min smooth loss: 1.8230 at iter 20992 **\n",
            "  ** New min smooth loss: 1.8229 at iter 20994 **\n",
            "  ** New min smooth loss: 1.8228 at iter 20995 **\n",
            "Iter: 21000/100000, Smooth Loss: 1.8238, Min Smooth Loss: 1.8228 (at iter 20995), Time: 105.80s\n",
            "Iter: 21100/100000, Smooth Loss: 1.8243, Min Smooth Loss: 1.8228 (at iter 20995), Time: 106.25s\n",
            "  ** New min smooth loss: 1.8226 at iter 21105 **\n",
            "  ** New min smooth loss: 1.8221 at iter 21106 **\n",
            "  ** New min smooth loss: 1.8220 at iter 21109 **\n",
            "  ** New min smooth loss: 1.8220 at iter 21110 **\n",
            "  ** New min smooth loss: 1.8218 at iter 21111 **\n",
            "  ** New min smooth loss: 1.8217 at iter 21113 **\n",
            "  ** New min smooth loss: 1.8213 at iter 21128 **\n",
            "  ** New min smooth loss: 1.8208 at iter 21129 **\n",
            "  ** New min smooth loss: 1.8201 at iter 21130 **\n",
            "  ** New min smooth loss: 1.8196 at iter 21133 **\n",
            "  ** New min smooth loss: 1.8194 at iter 21134 **\n",
            "  ** New min smooth loss: 1.8192 at iter 21140 **\n",
            "  ** New min smooth loss: 1.8191 at iter 21143 **\n",
            "  ** New min smooth loss: 1.8189 at iter 21144 **\n",
            "  ** New min smooth loss: 1.8185 at iter 21145 **\n",
            "  ** New min smooth loss: 1.8182 at iter 21146 **\n",
            "  ** New min smooth loss: 1.8182 at iter 21148 **\n",
            "  ** New min smooth loss: 1.8181 at iter 21152 **\n",
            "  ** New min smooth loss: 1.8180 at iter 21158 **\n",
            "  ** New min smooth loss: 1.8180 at iter 21173 **\n",
            "Iter: 21200/100000, Smooth Loss: 1.8189, Min Smooth Loss: 1.8180 (at iter 21173), Time: 106.70s\n",
            "  ** New min smooth loss: 1.8178 at iter 21237 **\n",
            "  ** New min smooth loss: 1.8174 at iter 21238 **\n",
            "  ** New min smooth loss: 1.8168 at iter 21240 **\n",
            "  ** New min smooth loss: 1.8165 at iter 21265 **\n",
            "  ** New min smooth loss: 1.8165 at iter 21269 **\n",
            "  ** New min smooth loss: 1.8160 at iter 21270 **\n",
            "  ** New min smooth loss: 1.8158 at iter 21273 **\n",
            "Iter: 21300/100000, Smooth Loss: 1.8166, Min Smooth Loss: 1.8158 (at iter 21273), Time: 107.12s\n",
            "  ** New min smooth loss: 1.8154 at iter 21313 **\n",
            "  ** New min smooth loss: 1.8153 at iter 21316 **\n",
            "  ** New min smooth loss: 1.8151 at iter 21317 **\n",
            "  ** New min smooth loss: 1.8145 at iter 21318 **\n",
            "  ** New min smooth loss: 1.8139 at iter 21319 **\n",
            "  ** New min smooth loss: 1.8136 at iter 21320 **\n",
            "Iter: 21400/100000, Smooth Loss: 1.8164, Min Smooth Loss: 1.8136 (at iter 21320), Time: 107.56s\n",
            "Iter: 21500/100000, Smooth Loss: 1.8140, Min Smooth Loss: 1.8136 (at iter 21320), Time: 107.99s\n",
            "  ** New min smooth loss: 1.8134 at iter 21525 **\n",
            "  ** New min smooth loss: 1.8128 at iter 21533 **\n",
            "  ** New min smooth loss: 1.8126 at iter 21534 **\n",
            "  ** New min smooth loss: 1.8125 at iter 21536 **\n",
            "  ** New min smooth loss: 1.8123 at iter 21537 **\n",
            "  ** New min smooth loss: 1.8121 at iter 21540 **\n",
            "  ** New min smooth loss: 1.8119 at iter 21541 **\n",
            "  ** New min smooth loss: 1.8117 at iter 21563 **\n",
            "  ** New min smooth loss: 1.8114 at iter 21576 **\n",
            "  ** New min smooth loss: 1.8110 at iter 21577 **\n",
            "  ** New min smooth loss: 1.8110 at iter 21578 **\n",
            "  ** New min smooth loss: 1.8109 at iter 21579 **\n",
            "  ** New min smooth loss: 1.8108 at iter 21580 **\n",
            "  ** New min smooth loss: 1.8106 at iter 21588 **\n",
            "  ** New min smooth loss: 1.8100 at iter 21589 **\n",
            "  ** New min smooth loss: 1.8096 at iter 21590 **\n",
            "  ** New min smooth loss: 1.8090 at iter 21591 **\n",
            "  ** New min smooth loss: 1.8089 at iter 21592 **\n",
            "  ** New min smooth loss: 1.8086 at iter 21593 **\n",
            "Iter: 21600/100000, Smooth Loss: 1.8088, Min Smooth Loss: 1.8086 (at iter 21593), Time: 108.42s\n",
            "  ** New min smooth loss: 1.8085 at iter 21601 **\n",
            "  ** New min smooth loss: 1.8084 at iter 21602 **\n",
            "  ** New min smooth loss: 1.8081 at iter 21604 **\n",
            "  ** New min smooth loss: 1.8080 at iter 21606 **\n",
            "  ** New min smooth loss: 1.8078 at iter 21607 **\n",
            "  ** New min smooth loss: 1.8077 at iter 21608 **\n",
            "  ** New min smooth loss: 1.8075 at iter 21610 **\n",
            "  ** New min smooth loss: 1.8075 at iter 21614 **\n",
            "  ** New min smooth loss: 1.8072 at iter 21615 **\n",
            "  ** New min smooth loss: 1.8071 at iter 21620 **\n",
            "  ** New min smooth loss: 1.8068 at iter 21622 **\n",
            "  ** New min smooth loss: 1.8066 at iter 21623 **\n",
            "  ** New min smooth loss: 1.8065 at iter 21640 **\n",
            "  ** New min smooth loss: 1.8062 at iter 21644 **\n",
            "  ** New min smooth loss: 1.8061 at iter 21645 **\n",
            "  ** New min smooth loss: 1.8060 at iter 21651 **\n",
            "  ** New min smooth loss: 1.8060 at iter 21652 **\n",
            "  ** New min smooth loss: 1.8059 at iter 21653 **\n",
            "  ** New min smooth loss: 1.8056 at iter 21682 **\n",
            "  ** New min smooth loss: 1.8055 at iter 21691 **\n",
            "  ** New min smooth loss: 1.8049 at iter 21692 **\n",
            "  ** New min smooth loss: 1.8049 at iter 21693 **\n",
            "  ** New min smooth loss: 1.8044 at iter 21694 **\n",
            "Iter: 21700/100000, Smooth Loss: 1.8047, Min Smooth Loss: 1.8044 (at iter 21694), Time: 108.86s\n",
            "  ** New min smooth loss: 1.8044 at iter 21702 **\n",
            "  ** New min smooth loss: 1.8043 at iter 21710 **\n",
            "  ** New min smooth loss: 1.8040 at iter 21714 **\n",
            "  ** New min smooth loss: 1.8037 at iter 21751 **\n",
            "  ** New min smooth loss: 1.8037 at iter 21752 **\n",
            "  ** New min smooth loss: 1.8033 at iter 21753 **\n",
            "  ** New min smooth loss: 1.8032 at iter 21754 **\n",
            "  ** New min smooth loss: 1.8032 at iter 21755 **\n",
            "  ** New min smooth loss: 1.8027 at iter 21756 **\n",
            "  ** New min smooth loss: 1.8026 at iter 21759 **\n",
            "  ** New min smooth loss: 1.8024 at iter 21760 **\n",
            "  ** New min smooth loss: 1.8022 at iter 21780 **\n",
            "  ** New min smooth loss: 1.8020 at iter 21792 **\n",
            "  ** New min smooth loss: 1.8018 at iter 21793 **\n",
            "  ** New min smooth loss: 1.8017 at iter 21794 **\n",
            "  ** New min smooth loss: 1.8012 at iter 21795 **\n",
            "  ** New min smooth loss: 1.8010 at iter 21796 **\n",
            "  ** New min smooth loss: 1.8008 at iter 21797 **\n",
            "  ** New min smooth loss: 1.8004 at iter 21798 **\n",
            "  ** New min smooth loss: 1.8000 at iter 21799 **\n",
            "Iter: 21800/100000, Smooth Loss: 1.8002, Min Smooth Loss: 1.8000 (at iter 21799), Time: 109.28s\n",
            "  ** New min smooth loss: 1.7998 at iter 21824 **\n",
            "  ** New min smooth loss: 1.7993 at iter 21825 **\n",
            "  ** New min smooth loss: 1.7989 at iter 21826 **\n",
            "  ** New min smooth loss: 1.7987 at iter 21828 **\n",
            "  ** New min smooth loss: 1.7984 at iter 21829 **\n",
            "  ** New min smooth loss: 1.7983 at iter 21830 **\n",
            "Iter: 21900/100000, Smooth Loss: 1.8003, Min Smooth Loss: 1.7983 (at iter 21830), Time: 109.72s\n",
            "Iter: 22000/100000, Smooth Loss: 1.8013, Min Smooth Loss: 1.7983 (at iter 21830), Time: 110.14s\n",
            "Iter: 22100/100000, Smooth Loss: 1.8092, Min Smooth Loss: 1.7983 (at iter 21830), Time: 110.78s\n",
            "Iter: 22200/100000, Smooth Loss: 1.8200, Min Smooth Loss: 1.7983 (at iter 21830), Time: 111.41s\n",
            "Iter: 22300/100000, Smooth Loss: 1.8167, Min Smooth Loss: 1.7983 (at iter 21830), Time: 112.06s\n",
            "Iter: 22400/100000, Smooth Loss: 1.8223, Min Smooth Loss: 1.7983 (at iter 21830), Time: 112.71s\n",
            "Iter: 22500/100000, Smooth Loss: 1.8286, Min Smooth Loss: 1.7983 (at iter 21830), Time: 113.33s\n",
            "Iter: 22600/100000, Smooth Loss: 1.8223, Min Smooth Loss: 1.7983 (at iter 21830), Time: 113.98s\n",
            "Iter: 22700/100000, Smooth Loss: 1.8244, Min Smooth Loss: 1.7983 (at iter 21830), Time: 114.65s\n",
            "Iter: 22800/100000, Smooth Loss: 1.8159, Min Smooth Loss: 1.7983 (at iter 21830), Time: 115.13s\n",
            "Iter: 22900/100000, Smooth Loss: 1.8115, Min Smooth Loss: 1.7983 (at iter 21830), Time: 115.55s\n",
            "Iter: 23000/100000, Smooth Loss: 1.8039, Min Smooth Loss: 1.7983 (at iter 21830), Time: 115.99s\n",
            "  ** New min smooth loss: 1.7981 at iter 23082 **\n",
            "  ** New min smooth loss: 1.7978 at iter 23083 **\n",
            "  ** New min smooth loss: 1.7972 at iter 23084 **\n",
            "  ** New min smooth loss: 1.7970 at iter 23085 **\n",
            "  ** New min smooth loss: 1.7969 at iter 23086 **\n",
            "  ** New min smooth loss: 1.7969 at iter 23087 **\n",
            "  ** New min smooth loss: 1.7967 at iter 23091 **\n",
            "  ** New min smooth loss: 1.7966 at iter 23095 **\n",
            "  ** New min smooth loss: 1.7963 at iter 23096 **\n",
            "  ** New min smooth loss: 1.7960 at iter 23097 **\n",
            "  ** New min smooth loss: 1.7952 at iter 23098 **\n",
            "  ** New min smooth loss: 1.7946 at iter 23099 **\n",
            "Iter: 23100/100000, Smooth Loss: 1.7947, Min Smooth Loss: 1.7946 (at iter 23099), Time: 116.42s\n",
            "  ** New min smooth loss: 1.7944 at iter 23101 **\n",
            "  ** New min smooth loss: 1.7943 at iter 23102 **\n",
            "  ** New min smooth loss: 1.7943 at iter 23105 **\n",
            "  ** New min smooth loss: 1.7942 at iter 23107 **\n",
            "  ** New min smooth loss: 1.7938 at iter 23108 **\n",
            "  ** New min smooth loss: 1.7938 at iter 23109 **\n",
            "  ** New min smooth loss: 1.7927 at iter 23110 **\n",
            "  ** New min smooth loss: 1.7926 at iter 23122 **\n",
            "  ** New min smooth loss: 1.7925 at iter 23125 **\n",
            "  ** New min smooth loss: 1.7921 at iter 23127 **\n",
            "  ** New min smooth loss: 1.7921 at iter 23130 **\n",
            "  ** New min smooth loss: 1.7917 at iter 23131 **\n",
            "  ** New min smooth loss: 1.7916 at iter 23132 **\n",
            "  ** New min smooth loss: 1.7915 at iter 23136 **\n",
            "  ** New min smooth loss: 1.7914 at iter 23138 **\n",
            "  ** New min smooth loss: 1.7908 at iter 23140 **\n",
            "Iter: 23200/100000, Smooth Loss: 1.8050, Min Smooth Loss: 1.7908 (at iter 23140), Time: 116.87s\n",
            "Iter: 23300/100000, Smooth Loss: 1.8037, Min Smooth Loss: 1.7908 (at iter 23140), Time: 117.33s\n",
            "Iter: 23400/100000, Smooth Loss: 1.8051, Min Smooth Loss: 1.7908 (at iter 23140), Time: 117.76s\n",
            "Iter: 23500/100000, Smooth Loss: 1.8097, Min Smooth Loss: 1.7908 (at iter 23140), Time: 118.20s\n",
            "Iter: 23600/100000, Smooth Loss: 1.8147, Min Smooth Loss: 1.7908 (at iter 23140), Time: 118.62s\n",
            "Iter: 23700/100000, Smooth Loss: 1.8140, Min Smooth Loss: 1.7908 (at iter 23140), Time: 119.07s\n",
            "Iter: 23800/100000, Smooth Loss: 1.8174, Min Smooth Loss: 1.7908 (at iter 23140), Time: 119.49s\n",
            "Iter: 23900/100000, Smooth Loss: 1.8198, Min Smooth Loss: 1.7908 (at iter 23140), Time: 119.91s\n",
            "Iter: 24000/100000, Smooth Loss: 1.8090, Min Smooth Loss: 1.7908 (at iter 23140), Time: 120.37s\n",
            "Iter: 24100/100000, Smooth Loss: 1.7942, Min Smooth Loss: 1.7908 (at iter 23140), Time: 120.81s\n",
            "  ** New min smooth loss: 1.7907 at iter 24139 **\n",
            "  ** New min smooth loss: 1.7906 at iter 24140 **\n",
            "  ** New min smooth loss: 1.7904 at iter 24141 **\n",
            "  ** New min smooth loss: 1.7900 at iter 24142 **\n",
            "  ** New min smooth loss: 1.7898 at iter 24143 **\n",
            "  ** New min smooth loss: 1.7895 at iter 24144 **\n",
            "  ** New min smooth loss: 1.7890 at iter 24145 **\n",
            "  ** New min smooth loss: 1.7884 at iter 24146 **\n",
            "  ** New min smooth loss: 1.7883 at iter 24152 **\n",
            "  ** New min smooth loss: 1.7880 at iter 24160 **\n",
            "  ** New min smooth loss: 1.7877 at iter 24161 **\n",
            "  ** New min smooth loss: 1.7877 at iter 24162 **\n",
            "  ** New min smooth loss: 1.7875 at iter 24163 **\n",
            "Iter: 24200/100000, Smooth Loss: 1.7953, Min Smooth Loss: 1.7875 (at iter 24163), Time: 121.27s\n",
            "Iter: 24300/100000, Smooth Loss: 1.8104, Min Smooth Loss: 1.7875 (at iter 24163), Time: 121.70s\n",
            "Iter: 24400/100000, Smooth Loss: 1.8203, Min Smooth Loss: 1.7875 (at iter 24163), Time: 122.13s\n",
            "Iter: 24500/100000, Smooth Loss: 1.8202, Min Smooth Loss: 1.7875 (at iter 24163), Time: 122.57s\n",
            "Iter: 24600/100000, Smooth Loss: 1.8218, Min Smooth Loss: 1.7875 (at iter 24163), Time: 122.99s\n",
            "Iter: 24700/100000, Smooth Loss: 1.8295, Min Smooth Loss: 1.7875 (at iter 24163), Time: 123.43s\n",
            "Iter: 24800/100000, Smooth Loss: 1.8274, Min Smooth Loss: 1.7875 (at iter 24163), Time: 123.86s\n",
            "Iter: 24900/100000, Smooth Loss: 1.8255, Min Smooth Loss: 1.7875 (at iter 24163), Time: 124.31s\n",
            "Iter: 25000/100000, Smooth Loss: 1.8191, Min Smooth Loss: 1.7875 (at iter 24163), Time: 124.73s\n",
            "Iter: 25100/100000, Smooth Loss: 1.8239, Min Smooth Loss: 1.7875 (at iter 24163), Time: 125.39s\n",
            "Iter: 25200/100000, Smooth Loss: 1.8358, Min Smooth Loss: 1.7875 (at iter 24163), Time: 126.01s\n",
            "Iter: 25300/100000, Smooth Loss: 1.8327, Min Smooth Loss: 1.7875 (at iter 24163), Time: 126.69s\n",
            "Iter: 25400/100000, Smooth Loss: 1.8214, Min Smooth Loss: 1.7875 (at iter 24163), Time: 127.33s\n",
            "Iter: 25500/100000, Smooth Loss: 1.8132, Min Smooth Loss: 1.7875 (at iter 24163), Time: 127.98s\n",
            "Iter: 25600/100000, Smooth Loss: 1.8139, Min Smooth Loss: 1.7875 (at iter 24163), Time: 128.65s\n",
            "Iter: 25700/100000, Smooth Loss: 1.8208, Min Smooth Loss: 1.7875 (at iter 24163), Time: 129.32s\n",
            "Iter: 25800/100000, Smooth Loss: 1.8259, Min Smooth Loss: 1.7875 (at iter 24163), Time: 129.76s\n",
            "Iter: 25900/100000, Smooth Loss: 1.8194, Min Smooth Loss: 1.7875 (at iter 24163), Time: 130.20s\n",
            "Iter: 26000/100000, Smooth Loss: 1.8109, Min Smooth Loss: 1.7875 (at iter 24163), Time: 130.62s\n",
            "Iter: 26100/100000, Smooth Loss: 1.8128, Min Smooth Loss: 1.7875 (at iter 24163), Time: 131.05s\n",
            "Iter: 26200/100000, Smooth Loss: 1.8070, Min Smooth Loss: 1.7875 (at iter 24163), Time: 131.49s\n",
            "Iter: 26300/100000, Smooth Loss: 1.8037, Min Smooth Loss: 1.7875 (at iter 24163), Time: 131.94s\n",
            "Iter: 26400/100000, Smooth Loss: 1.8281, Min Smooth Loss: 1.7875 (at iter 24163), Time: 132.38s\n",
            "Iter: 26500/100000, Smooth Loss: 1.8428, Min Smooth Loss: 1.7875 (at iter 24163), Time: 132.81s\n",
            "Iter: 26600/100000, Smooth Loss: 1.8273, Min Smooth Loss: 1.7875 (at iter 24163), Time: 133.24s\n",
            "Iter: 26700/100000, Smooth Loss: 1.8199, Min Smooth Loss: 1.7875 (at iter 24163), Time: 133.66s\n",
            "Iter: 26800/100000, Smooth Loss: 1.8073, Min Smooth Loss: 1.7875 (at iter 24163), Time: 134.10s\n",
            "Iter: 26900/100000, Smooth Loss: 1.8055, Min Smooth Loss: 1.7875 (at iter 24163), Time: 134.52s\n",
            "Iter: 27000/100000, Smooth Loss: 1.7983, Min Smooth Loss: 1.7875 (at iter 24163), Time: 134.96s\n",
            "Iter: 27100/100000, Smooth Loss: 1.8005, Min Smooth Loss: 1.7875 (at iter 24163), Time: 135.37s\n",
            "Iter: 27200/100000, Smooth Loss: 1.7965, Min Smooth Loss: 1.7875 (at iter 24163), Time: 135.80s\n",
            "  ** New min smooth loss: 1.7874 at iter 27279 **\n",
            "  ** New min smooth loss: 1.7871 at iter 27284 **\n",
            "  ** New min smooth loss: 1.7868 at iter 27285 **\n",
            "  ** New min smooth loss: 1.7862 at iter 27286 **\n",
            "  ** New min smooth loss: 1.7862 at iter 27287 **\n",
            "  ** New min smooth loss: 1.7858 at iter 27288 **\n",
            "  ** New min smooth loss: 1.7856 at iter 27289 **\n",
            "  ** New min smooth loss: 1.7853 at iter 27291 **\n",
            "  ** New min smooth loss: 1.7845 at iter 27292 **\n",
            "  ** New min smooth loss: 1.7839 at iter 27293 **\n",
            "  ** New min smooth loss: 1.7838 at iter 27294 **\n",
            "  ** New min smooth loss: 1.7837 at iter 27296 **\n",
            "  ** New min smooth loss: 1.7832 at iter 27297 **\n",
            "  ** New min smooth loss: 1.7827 at iter 27300 **\n",
            "Iter: 27300/100000, Smooth Loss: 1.7827, Min Smooth Loss: 1.7827 (at iter 27300), Time: 136.25s\n",
            "  ** New min smooth loss: 1.7826 at iter 27302 **\n",
            "  ** New min smooth loss: 1.7823 at iter 27303 **\n",
            "  ** New min smooth loss: 1.7817 at iter 27314 **\n",
            "  ** New min smooth loss: 1.7815 at iter 27315 **\n",
            "  ** New min smooth loss: 1.7811 at iter 27316 **\n",
            "  ** New min smooth loss: 1.7808 at iter 27317 **\n",
            "  ** New min smooth loss: 1.7808 at iter 27323 **\n",
            "  ** New min smooth loss: 1.7808 at iter 27325 **\n",
            "Iter: 27400/100000, Smooth Loss: 1.7856, Min Smooth Loss: 1.7808 (at iter 27325), Time: 136.67s\n",
            "Iter: 27500/100000, Smooth Loss: 1.7975, Min Smooth Loss: 1.7808 (at iter 27325), Time: 137.12s\n",
            "Iter: 27600/100000, Smooth Loss: 1.8066, Min Smooth Loss: 1.7808 (at iter 27325), Time: 137.54s\n",
            "Iter: 27700/100000, Smooth Loss: 1.8119, Min Smooth Loss: 1.7808 (at iter 27325), Time: 137.97s\n",
            "Iter: 27800/100000, Smooth Loss: 1.8121, Min Smooth Loss: 1.7808 (at iter 27325), Time: 138.41s\n",
            "Iter: 27900/100000, Smooth Loss: 1.8129, Min Smooth Loss: 1.7808 (at iter 27325), Time: 138.83s\n",
            "Iter: 28000/100000, Smooth Loss: 1.8114, Min Smooth Loss: 1.7808 (at iter 27325), Time: 139.27s\n",
            "Iter: 28100/100000, Smooth Loss: 1.8084, Min Smooth Loss: 1.7808 (at iter 27325), Time: 139.90s\n",
            "Iter: 28200/100000, Smooth Loss: 1.8141, Min Smooth Loss: 1.7808 (at iter 27325), Time: 140.54s\n",
            "Iter: 28300/100000, Smooth Loss: 1.8128, Min Smooth Loss: 1.7808 (at iter 27325), Time: 141.21s\n",
            "Iter: 28400/100000, Smooth Loss: 1.8119, Min Smooth Loss: 1.7808 (at iter 27325), Time: 141.84s\n",
            "Iter: 28500/100000, Smooth Loss: 1.8082, Min Smooth Loss: 1.7808 (at iter 27325), Time: 142.48s\n",
            "Iter: 28600/100000, Smooth Loss: 1.7956, Min Smooth Loss: 1.7808 (at iter 27325), Time: 143.11s\n",
            "Iter: 28700/100000, Smooth Loss: 1.7829, Min Smooth Loss: 1.7808 (at iter 27325), Time: 143.80s\n",
            "  ** New min smooth loss: 1.7806 at iter 28749 **\n",
            "  ** New min smooth loss: 1.7804 at iter 28751 **\n",
            "  ** New min smooth loss: 1.7798 at iter 28752 **\n",
            "  ** New min smooth loss: 1.7796 at iter 28759 **\n",
            "  ** New min smooth loss: 1.7792 at iter 28761 **\n",
            "  ** New min smooth loss: 1.7790 at iter 28762 **\n",
            "  ** New min smooth loss: 1.7788 at iter 28763 **\n",
            "  ** New min smooth loss: 1.7783 at iter 28764 **\n",
            "  ** New min smooth loss: 1.7780 at iter 28765 **\n",
            "  ** New min smooth loss: 1.7777 at iter 28766 **\n",
            "  ** New min smooth loss: 1.7772 at iter 28767 **\n",
            "  ** New min smooth loss: 1.7771 at iter 28768 **\n",
            "  ** New min smooth loss: 1.7765 at iter 28769 **\n",
            "  ** New min smooth loss: 1.7762 at iter 28770 **\n",
            "  ** New min smooth loss: 1.7755 at iter 28771 **\n",
            "  ** New min smooth loss: 1.7752 at iter 28773 **\n",
            "  ** New min smooth loss: 1.7751 at iter 28774 **\n",
            "  ** New min smooth loss: 1.7746 at iter 28775 **\n",
            "  ** New min smooth loss: 1.7741 at iter 28776 **\n",
            "  ** New min smooth loss: 1.7739 at iter 28777 **\n",
            "  ** New min smooth loss: 1.7735 at iter 28785 **\n",
            "  ** New min smooth loss: 1.7735 at iter 28787 **\n",
            "  ** New min smooth loss: 1.7733 at iter 28790 **\n",
            "  ** New min smooth loss: 1.7728 at iter 28791 **\n",
            "  ** New min smooth loss: 1.7722 at iter 28792 **\n",
            "  ** New min smooth loss: 1.7719 at iter 28794 **\n",
            "  ** New min smooth loss: 1.7716 at iter 28795 **\n",
            "  ** New min smooth loss: 1.7711 at iter 28797 **\n",
            "  ** New min smooth loss: 1.7707 at iter 28798 **\n",
            "  ** New min smooth loss: 1.7705 at iter 28799 **\n",
            "  ** New min smooth loss: 1.7705 at iter 28800 **\n",
            "Iter: 28800/100000, Smooth Loss: 1.7705, Min Smooth Loss: 1.7705 (at iter 28800), Time: 144.27s\n",
            "  ** New min smooth loss: 1.7703 at iter 28801 **\n",
            "  ** New min smooth loss: 1.7700 at iter 28802 **\n",
            "  ** New min smooth loss: 1.7699 at iter 28804 **\n",
            "  ** New min smooth loss: 1.7694 at iter 28805 **\n",
            "  ** New min smooth loss: 1.7692 at iter 28806 **\n",
            "Iter: 28900/100000, Smooth Loss: 1.7729, Min Smooth Loss: 1.7692 (at iter 28806), Time: 144.71s\n",
            "  ** New min smooth loss: 1.7691 at iter 28920 **\n",
            "  ** New min smooth loss: 1.7690 at iter 28921 **\n",
            "  ** New min smooth loss: 1.7687 at iter 28932 **\n",
            "  ** New min smooth loss: 1.7686 at iter 28933 **\n",
            "  ** New min smooth loss: 1.7682 at iter 28934 **\n",
            "  ** New min smooth loss: 1.7679 at iter 28936 **\n",
            "  ** New min smooth loss: 1.7678 at iter 28947 **\n",
            "  ** New min smooth loss: 1.7678 at iter 28953 **\n",
            "  ** New min smooth loss: 1.7674 at iter 28954 **\n",
            "  ** New min smooth loss: 1.7673 at iter 28955 **\n",
            "  ** New min smooth loss: 1.7670 at iter 28956 **\n",
            "  ** New min smooth loss: 1.7669 at iter 28957 **\n",
            "  ** New min smooth loss: 1.7664 at iter 28958 **\n",
            "  ** New min smooth loss: 1.7659 at iter 28959 **\n",
            "  ** New min smooth loss: 1.7650 at iter 28960 **\n",
            "  ** New min smooth loss: 1.7646 at iter 28963 **\n",
            "  ** New min smooth loss: 1.7644 at iter 28964 **\n",
            "  ** New min smooth loss: 1.7644 at iter 28965 **\n",
            "  ** New min smooth loss: 1.7638 at iter 28966 **\n",
            "  ** New min smooth loss: 1.7636 at iter 28967 **\n",
            "  ** New min smooth loss: 1.7634 at iter 28977 **\n",
            "  ** New min smooth loss: 1.7628 at iter 28978 **\n",
            "  ** New min smooth loss: 1.7623 at iter 28980 **\n",
            "  ** New min smooth loss: 1.7623 at iter 28981 **\n",
            "Iter: 29000/100000, Smooth Loss: 1.7671, Min Smooth Loss: 1.7623 (at iter 28981), Time: 145.13s\n",
            "Iter: 29100/100000, Smooth Loss: 1.7771, Min Smooth Loss: 1.7623 (at iter 28981), Time: 145.56s\n",
            "Iter: 29200/100000, Smooth Loss: 1.7861, Min Smooth Loss: 1.7623 (at iter 28981), Time: 146.00s\n",
            "Iter: 29300/100000, Smooth Loss: 1.7871, Min Smooth Loss: 1.7623 (at iter 28981), Time: 146.42s\n",
            "Iter: 29400/100000, Smooth Loss: 1.7957, Min Smooth Loss: 1.7623 (at iter 28981), Time: 146.88s\n",
            "Iter: 29500/100000, Smooth Loss: 1.7966, Min Smooth Loss: 1.7623 (at iter 28981), Time: 147.32s\n",
            "Iter: 29600/100000, Smooth Loss: 1.7907, Min Smooth Loss: 1.7623 (at iter 28981), Time: 147.77s\n",
            "Iter: 29700/100000, Smooth Loss: 1.7829, Min Smooth Loss: 1.7623 (at iter 28981), Time: 148.20s\n",
            "Iter: 29800/100000, Smooth Loss: 1.7800, Min Smooth Loss: 1.7623 (at iter 28981), Time: 148.61s\n",
            "Iter: 29900/100000, Smooth Loss: 1.7803, Min Smooth Loss: 1.7623 (at iter 28981), Time: 149.06s\n",
            "Iter: 30000/100000, Smooth Loss: 1.7801, Min Smooth Loss: 1.7623 (at iter 28981), Time: 149.49s\n",
            "--- Synthesized text at iter 30000 ---\n",
            "op stle baring shrione, goted wat live of it? nuter wates. . ....\n",
            "Aly of reothee stoposs the coill scavisel chaupingen o show It heartef wereiny hadpes.\n",
            "\"What you to cand ans of the gamer the rount yo\n",
            "---\n",
            "Iter: 30100/100000, Smooth Loss: 1.7766, Min Smooth Loss: 1.7623 (at iter 28981), Time: 149.97s\n",
            "Iter: 30200/100000, Smooth Loss: 1.7704, Min Smooth Loss: 1.7623 (at iter 28981), Time: 150.40s\n",
            "  ** New min smooth loss: 1.7622 at iter 30241 **\n",
            "  ** New min smooth loss: 1.7621 at iter 30245 **\n",
            "  ** New min smooth loss: 1.7619 at iter 30246 **\n",
            "  ** New min smooth loss: 1.7616 at iter 30247 **\n",
            "  ** New min smooth loss: 1.7615 at iter 30248 **\n",
            "  ** New min smooth loss: 1.7614 at iter 30249 **\n",
            "  ** New min smooth loss: 1.7614 at iter 30266 **\n",
            "  ** New min smooth loss: 1.7614 at iter 30267 **\n",
            "  ** New min smooth loss: 1.7611 at iter 30276 **\n",
            "  ** New min smooth loss: 1.7609 at iter 30277 **\n",
            "  ** New min smooth loss: 1.7606 at iter 30278 **\n",
            "  ** New min smooth loss: 1.7602 at iter 30279 **\n",
            "  ** New min smooth loss: 1.7598 at iter 30280 **\n",
            "  ** New min smooth loss: 1.7595 at iter 30281 **\n",
            "  ** New min smooth loss: 1.7594 at iter 30290 **\n",
            "  ** New min smooth loss: 1.7593 at iter 30293 **\n",
            "  ** New min smooth loss: 1.7591 at iter 30294 **\n",
            "  ** New min smooth loss: 1.7590 at iter 30295 **\n",
            "  ** New min smooth loss: 1.7584 at iter 30296 **\n",
            "Iter: 30300/100000, Smooth Loss: 1.7587, Min Smooth Loss: 1.7584 (at iter 30296), Time: 150.85s\n",
            "  ** New min smooth loss: 1.7582 at iter 30301 **\n",
            "  ** New min smooth loss: 1.7578 at iter 30302 **\n",
            "  ** New min smooth loss: 1.7574 at iter 30303 **\n",
            "  ** New min smooth loss: 1.7570 at iter 30305 **\n",
            "  ** New min smooth loss: 1.7567 at iter 30306 **\n",
            "  ** New min smooth loss: 1.7565 at iter 30307 **\n",
            "  ** New min smooth loss: 1.7565 at iter 30308 **\n",
            "  ** New min smooth loss: 1.7562 at iter 30310 **\n",
            "  ** New min smooth loss: 1.7559 at iter 30311 **\n",
            "  ** New min smooth loss: 1.7558 at iter 30312 **\n",
            "  ** New min smooth loss: 1.7555 at iter 30313 **\n",
            "  ** New min smooth loss: 1.7555 at iter 30314 **\n",
            "  ** New min smooth loss: 1.7554 at iter 30315 **\n",
            "  ** New min smooth loss: 1.7551 at iter 30328 **\n",
            "  ** New min smooth loss: 1.7547 at iter 30329 **\n",
            "  ** New min smooth loss: 1.7547 at iter 30330 **\n",
            "  ** New min smooth loss: 1.7543 at iter 30332 **\n",
            "  ** New min smooth loss: 1.7537 at iter 30333 **\n",
            "  ** New min smooth loss: 1.7534 at iter 30334 **\n",
            "  ** New min smooth loss: 1.7531 at iter 30335 **\n",
            "  ** New min smooth loss: 1.7530 at iter 30337 **\n",
            "  ** New min smooth loss: 1.7527 at iter 30338 **\n",
            "  ** New min smooth loss: 1.7525 at iter 30339 **\n",
            "  ** New min smooth loss: 1.7521 at iter 30340 **\n",
            "  ** New min smooth loss: 1.7515 at iter 30341 **\n",
            "  ** New min smooth loss: 1.7512 at iter 30342 **\n",
            "  ** New min smooth loss: 1.7509 at iter 30343 **\n",
            "  ** New min smooth loss: 1.7509 at iter 30344 **\n",
            "  ** New min smooth loss: 1.7506 at iter 30345 **\n",
            "  ** New min smooth loss: 1.7505 at iter 30347 **\n",
            "  ** New min smooth loss: 1.7502 at iter 30348 **\n",
            "  ** New min smooth loss: 1.7501 at iter 30349 **\n",
            "  ** New min smooth loss: 1.7500 at iter 30350 **\n",
            "  ** New min smooth loss: 1.7497 at iter 30366 **\n",
            "  ** New min smooth loss: 1.7495 at iter 30381 **\n",
            "  ** New min smooth loss: 1.7490 at iter 30382 **\n",
            "  ** New min smooth loss: 1.7489 at iter 30383 **\n",
            "Iter: 30400/100000, Smooth Loss: 1.7503, Min Smooth Loss: 1.7489 (at iter 30383), Time: 151.29s\n",
            "  ** New min smooth loss: 1.7489 at iter 30486 **\n",
            "  ** New min smooth loss: 1.7488 at iter 30496 **\n",
            "Iter: 30500/100000, Smooth Loss: 1.7491, Min Smooth Loss: 1.7488 (at iter 30496), Time: 151.73s\n",
            "  ** New min smooth loss: 1.7486 at iter 30523 **\n",
            "Iter: 30600/100000, Smooth Loss: 1.7577, Min Smooth Loss: 1.7486 (at iter 30523), Time: 152.17s\n",
            "Iter: 30700/100000, Smooth Loss: 1.7563, Min Smooth Loss: 1.7486 (at iter 30523), Time: 152.60s\n",
            "Iter: 30800/100000, Smooth Loss: 1.7639, Min Smooth Loss: 1.7486 (at iter 30523), Time: 153.04s\n",
            "Iter: 30900/100000, Smooth Loss: 1.7806, Min Smooth Loss: 1.7486 (at iter 30523), Time: 153.45s\n",
            "Iter: 31000/100000, Smooth Loss: 1.7755, Min Smooth Loss: 1.7486 (at iter 30523), Time: 153.86s\n",
            "Iter: 31100/100000, Smooth Loss: 1.7768, Min Smooth Loss: 1.7486 (at iter 30523), Time: 154.51s\n",
            "Iter: 31200/100000, Smooth Loss: 1.7726, Min Smooth Loss: 1.7486 (at iter 30523), Time: 155.14s\n",
            "Iter: 31300/100000, Smooth Loss: 1.7644, Min Smooth Loss: 1.7486 (at iter 30523), Time: 155.79s\n",
            "Iter: 31400/100000, Smooth Loss: 1.7622, Min Smooth Loss: 1.7486 (at iter 30523), Time: 156.44s\n",
            "Iter: 31500/100000, Smooth Loss: 1.7705, Min Smooth Loss: 1.7486 (at iter 30523), Time: 157.07s\n",
            "Iter: 31600/100000, Smooth Loss: 1.7661, Min Smooth Loss: 1.7486 (at iter 30523), Time: 157.72s\n",
            "Iter: 31700/100000, Smooth Loss: 1.7554, Min Smooth Loss: 1.7486 (at iter 30523), Time: 158.39s\n",
            "Iter: 31800/100000, Smooth Loss: 1.7534, Min Smooth Loss: 1.7486 (at iter 30523), Time: 158.86s\n",
            "Iter: 31900/100000, Smooth Loss: 1.7512, Min Smooth Loss: 1.7486 (at iter 30523), Time: 159.28s\n",
            "  ** New min smooth loss: 1.7486 at iter 31914 **\n",
            "Iter: 32000/100000, Smooth Loss: 1.7510, Min Smooth Loss: 1.7486 (at iter 31914), Time: 159.72s\n",
            "  ** New min smooth loss: 1.7485 at iter 32027 **\n",
            "  ** New min smooth loss: 1.7476 at iter 32029 **\n",
            "  ** New min smooth loss: 1.7473 at iter 32030 **\n",
            "  ** New min smooth loss: 1.7472 at iter 32032 **\n",
            "  ** New min smooth loss: 1.7471 at iter 32034 **\n",
            "  ** New min smooth loss: 1.7469 at iter 32087 **\n",
            "  ** New min smooth loss: 1.7468 at iter 32088 **\n",
            "  ** New min smooth loss: 1.7468 at iter 32089 **\n",
            "  ** New min smooth loss: 1.7467 at iter 32090 **\n",
            "  ** New min smooth loss: 1.7466 at iter 32097 **\n",
            "  ** New min smooth loss: 1.7464 at iter 32098 **\n",
            "  ** New min smooth loss: 1.7462 at iter 32100 **\n",
            "Iter: 32100/100000, Smooth Loss: 1.7462, Min Smooth Loss: 1.7462 (at iter 32100), Time: 160.16s\n",
            "  ** New min smooth loss: 1.7457 at iter 32101 **\n",
            "  ** New min smooth loss: 1.7449 at iter 32102 **\n",
            "  ** New min smooth loss: 1.7449 at iter 32104 **\n",
            "  ** New min smooth loss: 1.7446 at iter 32106 **\n",
            "  ** New min smooth loss: 1.7445 at iter 32107 **\n",
            "  ** New min smooth loss: 1.7441 at iter 32109 **\n",
            "  ** New min smooth loss: 1.7439 at iter 32110 **\n",
            "  ** New min smooth loss: 1.7437 at iter 32111 **\n",
            "  ** New min smooth loss: 1.7429 at iter 32112 **\n",
            "  ** New min smooth loss: 1.7426 at iter 32113 **\n",
            "  ** New min smooth loss: 1.7421 at iter 32131 **\n",
            "  ** New min smooth loss: 1.7419 at iter 32143 **\n",
            "  ** New min smooth loss: 1.7417 at iter 32144 **\n",
            "  ** New min smooth loss: 1.7417 at iter 32145 **\n",
            "  ** New min smooth loss: 1.7412 at iter 32146 **\n",
            "  ** New min smooth loss: 1.7410 at iter 32152 **\n",
            "  ** New min smooth loss: 1.7406 at iter 32172 **\n",
            "  ** New min smooth loss: 1.7404 at iter 32173 **\n",
            "Iter: 32200/100000, Smooth Loss: 1.7416, Min Smooth Loss: 1.7404 (at iter 32173), Time: 160.60s\n",
            "  ** New min smooth loss: 1.7403 at iter 32204 **\n",
            "  ** New min smooth loss: 1.7403 at iter 32205 **\n",
            "  ** New min smooth loss: 1.7398 at iter 32206 **\n",
            "  ** New min smooth loss: 1.7396 at iter 32207 **\n",
            "  ** New min smooth loss: 1.7395 at iter 32208 **\n",
            "  ** New min smooth loss: 1.7393 at iter 32212 **\n",
            "  ** New min smooth loss: 1.7392 at iter 32213 **\n",
            "  ** New min smooth loss: 1.7392 at iter 32254 **\n",
            "  ** New min smooth loss: 1.7387 at iter 32255 **\n",
            "  ** New min smooth loss: 1.7387 at iter 32256 **\n",
            "  ** New min smooth loss: 1.7381 at iter 32266 **\n",
            "  ** New min smooth loss: 1.7373 at iter 32267 **\n",
            "  ** New min smooth loss: 1.7370 at iter 32268 **\n",
            "  ** New min smooth loss: 1.7366 at iter 32269 **\n",
            "  ** New min smooth loss: 1.7362 at iter 32272 **\n",
            "  ** New min smooth loss: 1.7360 at iter 32273 **\n",
            "  ** New min smooth loss: 1.7359 at iter 32274 **\n",
            "  ** New min smooth loss: 1.7359 at iter 32280 **\n",
            "Iter: 32300/100000, Smooth Loss: 1.7388, Min Smooth Loss: 1.7359 (at iter 32280), Time: 161.03s\n",
            "  ** New min smooth loss: 1.7358 at iter 32342 **\n",
            "  ** New min smooth loss: 1.7355 at iter 32345 **\n",
            "  ** New min smooth loss: 1.7355 at iter 32346 **\n",
            "  ** New min smooth loss: 1.7354 at iter 32347 **\n",
            "  ** New min smooth loss: 1.7353 at iter 32376 **\n",
            "  ** New min smooth loss: 1.7350 at iter 32378 **\n",
            "  ** New min smooth loss: 1.7344 at iter 32380 **\n",
            "  ** New min smooth loss: 1.7342 at iter 32382 **\n",
            "  ** New min smooth loss: 1.7339 at iter 32384 **\n",
            "Iter: 32400/100000, Smooth Loss: 1.7360, Min Smooth Loss: 1.7339 (at iter 32384), Time: 161.46s\n",
            "  ** New min smooth loss: 1.7336 at iter 32438 **\n",
            "  ** New min smooth loss: 1.7333 at iter 32439 **\n",
            "  ** New min smooth loss: 1.7331 at iter 32442 **\n",
            "  ** New min smooth loss: 1.7326 at iter 32443 **\n",
            "  ** New min smooth loss: 1.7324 at iter 32445 **\n",
            "  ** New min smooth loss: 1.7321 at iter 32446 **\n",
            "  ** New min smooth loss: 1.7319 at iter 32447 **\n",
            "Iter: 32500/100000, Smooth Loss: 1.7335, Min Smooth Loss: 1.7319 (at iter 32447), Time: 161.92s\n",
            "  ** New min smooth loss: 1.7317 at iter 32522 **\n",
            "  ** New min smooth loss: 1.7314 at iter 32523 **\n",
            "  ** New min smooth loss: 1.7313 at iter 32528 **\n",
            "  ** New min smooth loss: 1.7311 at iter 32529 **\n",
            "  ** New min smooth loss: 1.7310 at iter 32530 **\n",
            "  ** New min smooth loss: 1.7305 at iter 32531 **\n",
            "  ** New min smooth loss: 1.7304 at iter 32532 **\n",
            "  ** New min smooth loss: 1.7303 at iter 32536 **\n",
            "  ** New min smooth loss: 1.7301 at iter 32537 **\n",
            "  ** New min smooth loss: 1.7300 at iter 32539 **\n",
            "  ** New min smooth loss: 1.7300 at iter 32540 **\n",
            "  ** New min smooth loss: 1.7297 at iter 32541 **\n",
            "Iter: 32600/100000, Smooth Loss: 1.7316, Min Smooth Loss: 1.7297 (at iter 32541), Time: 162.34s\n",
            "Iter: 32700/100000, Smooth Loss: 1.7388, Min Smooth Loss: 1.7297 (at iter 32541), Time: 162.78s\n",
            "Iter: 32800/100000, Smooth Loss: 1.7478, Min Smooth Loss: 1.7297 (at iter 32541), Time: 163.20s\n",
            "Iter: 32900/100000, Smooth Loss: 1.7493, Min Smooth Loss: 1.7297 (at iter 32541), Time: 163.64s\n",
            "Iter: 33000/100000, Smooth Loss: 1.7506, Min Smooth Loss: 1.7297 (at iter 32541), Time: 164.07s\n",
            "Iter: 33100/100000, Smooth Loss: 1.7529, Min Smooth Loss: 1.7297 (at iter 32541), Time: 164.49s\n",
            "Iter: 33200/100000, Smooth Loss: 1.7572, Min Smooth Loss: 1.7297 (at iter 32541), Time: 164.92s\n",
            "Iter: 33300/100000, Smooth Loss: 1.7532, Min Smooth Loss: 1.7297 (at iter 32541), Time: 165.33s\n",
            "Iter: 33400/100000, Smooth Loss: 1.7484, Min Smooth Loss: 1.7297 (at iter 32541), Time: 165.77s\n",
            "Iter: 33500/100000, Smooth Loss: 1.7403, Min Smooth Loss: 1.7297 (at iter 32541), Time: 166.19s\n",
            "  ** New min smooth loss: 1.7297 at iter 33590 **\n",
            "  ** New min smooth loss: 1.7293 at iter 33591 **\n",
            "  ** New min smooth loss: 1.7290 at iter 33592 **\n",
            "  ** New min smooth loss: 1.7289 at iter 33595 **\n",
            "Iter: 33600/100000, Smooth Loss: 1.7296, Min Smooth Loss: 1.7289 (at iter 33595), Time: 166.62s\n",
            "  ** New min smooth loss: 1.7283 at iter 33650 **\n",
            "  ** New min smooth loss: 1.7280 at iter 33653 **\n",
            "  ** New min smooth loss: 1.7276 at iter 33654 **\n",
            "  ** New min smooth loss: 1.7273 at iter 33655 **\n",
            "  ** New min smooth loss: 1.7269 at iter 33663 **\n",
            "  ** New min smooth loss: 1.7268 at iter 33668 **\n",
            "  ** New min smooth loss: 1.7267 at iter 33669 **\n",
            "  ** New min smooth loss: 1.7264 at iter 33670 **\n",
            "  ** New min smooth loss: 1.7257 at iter 33674 **\n",
            "  ** New min smooth loss: 1.7253 at iter 33680 **\n",
            "  ** New min smooth loss: 1.7249 at iter 33683 **\n",
            "  ** New min smooth loss: 1.7242 at iter 33684 **\n",
            "  ** New min smooth loss: 1.7239 at iter 33685 **\n",
            "  ** New min smooth loss: 1.7235 at iter 33686 **\n",
            "  ** New min smooth loss: 1.7232 at iter 33689 **\n",
            "  ** New min smooth loss: 1.7232 at iter 33690 **\n",
            "  ** New min smooth loss: 1.7232 at iter 33691 **\n",
            "  ** New min smooth loss: 1.7228 at iter 33692 **\n",
            "  ** New min smooth loss: 1.7224 at iter 33693 **\n",
            "  ** New min smooth loss: 1.7216 at iter 33697 **\n",
            "  ** New min smooth loss: 1.7214 at iter 33698 **\n",
            "  ** New min smooth loss: 1.7212 at iter 33699 **\n",
            "Iter: 33700/100000, Smooth Loss: 1.7212, Min Smooth Loss: 1.7212 (at iter 33699), Time: 167.07s\n",
            "  ** New min smooth loss: 1.7211 at iter 33701 **\n",
            "  ** New min smooth loss: 1.7209 at iter 33702 **\n",
            "  ** New min smooth loss: 1.7206 at iter 33703 **\n",
            "  ** New min smooth loss: 1.7205 at iter 33715 **\n",
            "  ** New min smooth loss: 1.7205 at iter 33716 **\n",
            "  ** New min smooth loss: 1.7205 at iter 33718 **\n",
            "  ** New min smooth loss: 1.7203 at iter 33719 **\n",
            "  ** New min smooth loss: 1.7203 at iter 33720 **\n",
            "  ** New min smooth loss: 1.7195 at iter 33722 **\n",
            "  ** New min smooth loss: 1.7195 at iter 33723 **\n",
            "  ** New min smooth loss: 1.7191 at iter 33729 **\n",
            "  ** New min smooth loss: 1.7186 at iter 33730 **\n",
            "  ** New min smooth loss: 1.7186 at iter 33731 **\n",
            "  ** New min smooth loss: 1.7184 at iter 33732 **\n",
            "  ** New min smooth loss: 1.7180 at iter 33733 **\n",
            "  ** New min smooth loss: 1.7176 at iter 33734 **\n",
            "  ** New min smooth loss: 1.7172 at iter 33735 **\n",
            "  ** New min smooth loss: 1.7163 at iter 33736 **\n",
            "  ** New min smooth loss: 1.7163 at iter 33737 **\n",
            "  ** New min smooth loss: 1.7160 at iter 33739 **\n",
            "  ** New min smooth loss: 1.7156 at iter 33740 **\n",
            "  ** New min smooth loss: 1.7147 at iter 33741 **\n",
            "  ** New min smooth loss: 1.7142 at iter 33742 **\n",
            "  ** New min smooth loss: 1.7139 at iter 33743 **\n",
            "  ** New min smooth loss: 1.7139 at iter 33744 **\n",
            "  ** New min smooth loss: 1.7136 at iter 33745 **\n",
            "  ** New min smooth loss: 1.7133 at iter 33746 **\n",
            "  ** New min smooth loss: 1.7132 at iter 33748 **\n",
            "  ** New min smooth loss: 1.7129 at iter 33749 **\n",
            "  ** New min smooth loss: 1.7123 at iter 33751 **\n",
            "  ** New min smooth loss: 1.7114 at iter 33752 **\n",
            "  ** New min smooth loss: 1.7114 at iter 33753 **\n",
            "  ** New min smooth loss: 1.7110 at iter 33754 **\n",
            "  ** New min smooth loss: 1.7105 at iter 33755 **\n",
            "  ** New min smooth loss: 1.7102 at iter 33756 **\n",
            "  ** New min smooth loss: 1.7101 at iter 33781 **\n",
            "  ** New min smooth loss: 1.7097 at iter 33783 **\n",
            "  ** New min smooth loss: 1.7096 at iter 33785 **\n",
            "  ** New min smooth loss: 1.7094 at iter 33787 **\n",
            "  ** New min smooth loss: 1.7091 at iter 33788 **\n",
            "  ** New min smooth loss: 1.7086 at iter 33789 **\n",
            "  ** New min smooth loss: 1.7082 at iter 33793 **\n",
            "  ** New min smooth loss: 1.7078 at iter 33794 **\n",
            "  ** New min smooth loss: 1.7075 at iter 33795 **\n",
            "Iter: 33800/100000, Smooth Loss: 1.7083, Min Smooth Loss: 1.7075 (at iter 33795), Time: 167.52s\n",
            "  ** New min smooth loss: 1.7069 at iter 33805 **\n",
            "  ** New min smooth loss: 1.7064 at iter 33811 **\n",
            "  ** New min smooth loss: 1.7062 at iter 33812 **\n",
            "  ** New min smooth loss: 1.7055 at iter 33816 **\n",
            "  ** New min smooth loss: 1.7054 at iter 33817 **\n",
            "  ** New min smooth loss: 1.7051 at iter 33818 **\n",
            "  ** New min smooth loss: 1.7046 at iter 33830 **\n",
            "  ** New min smooth loss: 1.7039 at iter 33832 **\n",
            "  ** New min smooth loss: 1.7035 at iter 33835 **\n",
            "  ** New min smooth loss: 1.7034 at iter 33836 **\n",
            "  ** New min smooth loss: 1.7032 at iter 33837 **\n",
            "  ** New min smooth loss: 1.7031 at iter 33838 **\n",
            "  ** New min smooth loss: 1.7027 at iter 33839 **\n",
            "  ** New min smooth loss: 1.7024 at iter 33840 **\n",
            "  ** New min smooth loss: 1.7021 at iter 33845 **\n",
            "  ** New min smooth loss: 1.7018 at iter 33846 **\n",
            "  ** New min smooth loss: 1.7016 at iter 33847 **\n",
            "  ** New min smooth loss: 1.7014 at iter 33852 **\n",
            "  ** New min smooth loss: 1.7013 at iter 33854 **\n",
            "  ** New min smooth loss: 1.7013 at iter 33855 **\n",
            "  ** New min smooth loss: 1.7007 at iter 33857 **\n",
            "  ** New min smooth loss: 1.7000 at iter 33858 **\n",
            "  ** New min smooth loss: 1.6996 at iter 33859 **\n",
            "  ** New min smooth loss: 1.6994 at iter 33860 **\n",
            "  ** New min smooth loss: 1.6992 at iter 33861 **\n",
            "  ** New min smooth loss: 1.6986 at iter 33862 **\n",
            "  ** New min smooth loss: 1.6983 at iter 33865 **\n",
            "  ** New min smooth loss: 1.6981 at iter 33870 **\n",
            "  ** New min smooth loss: 1.6979 at iter 33872 **\n",
            "Iter: 33900/100000, Smooth Loss: 1.6998, Min Smooth Loss: 1.6979 (at iter 33872), Time: 167.97s\n",
            "Iter: 34000/100000, Smooth Loss: 1.7025, Min Smooth Loss: 1.6979 (at iter 33872), Time: 168.38s\n",
            "  ** New min smooth loss: 1.6973 at iter 34062 **\n",
            "  ** New min smooth loss: 1.6966 at iter 34063 **\n",
            "  ** New min smooth loss: 1.6966 at iter 34064 **\n",
            "  ** New min smooth loss: 1.6964 at iter 34066 **\n",
            "  ** New min smooth loss: 1.6963 at iter 34070 **\n",
            "  ** New min smooth loss: 1.6958 at iter 34071 **\n",
            "  ** New min smooth loss: 1.6956 at iter 34072 **\n",
            "  ** New min smooth loss: 1.6950 at iter 34073 **\n",
            "  ** New min smooth loss: 1.6948 at iter 34074 **\n",
            "  ** New min smooth loss: 1.6946 at iter 34075 **\n",
            "  ** New min smooth loss: 1.6943 at iter 34077 **\n",
            "  ** New min smooth loss: 1.6942 at iter 34078 **\n",
            "  ** New min smooth loss: 1.6942 at iter 34079 **\n",
            "  ** New min smooth loss: 1.6938 at iter 34080 **\n",
            "  ** New min smooth loss: 1.6937 at iter 34092 **\n",
            "  ** New min smooth loss: 1.6935 at iter 34093 **\n",
            "  ** New min smooth loss: 1.6928 at iter 34094 **\n",
            "  ** New min smooth loss: 1.6924 at iter 34095 **\n",
            "  ** New min smooth loss: 1.6920 at iter 34096 **\n",
            "  ** New min smooth loss: 1.6914 at iter 34099 **\n",
            "  ** New min smooth loss: 1.6909 at iter 34100 **\n",
            "Iter: 34100/100000, Smooth Loss: 1.6909, Min Smooth Loss: 1.6909 (at iter 34100), Time: 169.00s\n",
            "  ** New min smooth loss: 1.6904 at iter 34101 **\n",
            "  ** New min smooth loss: 1.6897 at iter 34103 **\n",
            "  ** New min smooth loss: 1.6896 at iter 34104 **\n",
            "  ** New min smooth loss: 1.6894 at iter 34105 **\n",
            "  ** New min smooth loss: 1.6892 at iter 34133 **\n",
            "  ** New min smooth loss: 1.6886 at iter 34134 **\n",
            "  ** New min smooth loss: 1.6878 at iter 34135 **\n",
            "  ** New min smooth loss: 1.6876 at iter 34136 **\n",
            "  ** New min smooth loss: 1.6871 at iter 34137 **\n",
            "  ** New min smooth loss: 1.6871 at iter 34142 **\n",
            "  ** New min smooth loss: 1.6870 at iter 34145 **\n",
            "  ** New min smooth loss: 1.6868 at iter 34148 **\n",
            "Iter: 34200/100000, Smooth Loss: 1.6880, Min Smooth Loss: 1.6868 (at iter 34148), Time: 169.65s\n",
            "  ** New min smooth loss: 1.6868 at iter 34212 **\n",
            "  ** New min smooth loss: 1.6864 at iter 34213 **\n",
            "  ** New min smooth loss: 1.6860 at iter 34214 **\n",
            "  ** New min smooth loss: 1.6855 at iter 34215 **\n",
            "  ** New min smooth loss: 1.6855 at iter 34228 **\n",
            "  ** New min smooth loss: 1.6854 at iter 34229 **\n",
            "  ** New min smooth loss: 1.6847 at iter 34232 **\n",
            "  ** New min smooth loss: 1.6844 at iter 34233 **\n",
            "  ** New min smooth loss: 1.6841 at iter 34234 **\n",
            "  ** New min smooth loss: 1.6834 at iter 34235 **\n",
            "  ** New min smooth loss: 1.6832 at iter 34236 **\n",
            "  ** New min smooth loss: 1.6831 at iter 34237 **\n",
            "  ** New min smooth loss: 1.6827 at iter 34238 **\n",
            "  ** New min smooth loss: 1.6821 at iter 34239 **\n",
            "  ** New min smooth loss: 1.6819 at iter 34240 **\n",
            "  ** New min smooth loss: 1.6817 at iter 34242 **\n",
            "  ** New min smooth loss: 1.6816 at iter 34243 **\n",
            "  ** New min smooth loss: 1.6813 at iter 34244 **\n",
            "  ** New min smooth loss: 1.6809 at iter 34245 **\n",
            "  ** New min smooth loss: 1.6805 at iter 34246 **\n",
            "  ** New min smooth loss: 1.6804 at iter 34253 **\n",
            "  ** New min smooth loss: 1.6802 at iter 34254 **\n",
            "  ** New min smooth loss: 1.6799 at iter 34258 **\n",
            "  ** New min smooth loss: 1.6796 at iter 34259 **\n",
            "  ** New min smooth loss: 1.6794 at iter 34265 **\n",
            "  ** New min smooth loss: 1.6792 at iter 34266 **\n",
            "  ** New min smooth loss: 1.6789 at iter 34269 **\n",
            "  ** New min smooth loss: 1.6788 at iter 34270 **\n",
            "  ** New min smooth loss: 1.6785 at iter 34271 **\n",
            "  ** New min smooth loss: 1.6782 at iter 34272 **\n",
            "  ** New min smooth loss: 1.6782 at iter 34273 **\n",
            "  ** New min smooth loss: 1.6773 at iter 34274 **\n",
            "  ** New min smooth loss: 1.6771 at iter 34279 **\n",
            "  ** New min smooth loss: 1.6768 at iter 34280 **\n",
            "  ** New min smooth loss: 1.6767 at iter 34297 **\n",
            "  ** New min smooth loss: 1.6765 at iter 34299 **\n",
            "Iter: 34300/100000, Smooth Loss: 1.6766, Min Smooth Loss: 1.6765 (at iter 34299), Time: 170.32s\n",
            "  ** New min smooth loss: 1.6759 at iter 34354 **\n",
            "  ** New min smooth loss: 1.6759 at iter 34374 **\n",
            "  ** New min smooth loss: 1.6757 at iter 34376 **\n",
            "  ** New min smooth loss: 1.6755 at iter 34377 **\n",
            "  ** New min smooth loss: 1.6753 at iter 34378 **\n",
            "  ** New min smooth loss: 1.6753 at iter 34384 **\n",
            "  ** New min smooth loss: 1.6749 at iter 34388 **\n",
            "  ** New min smooth loss: 1.6748 at iter 34393 **\n",
            "  ** New min smooth loss: 1.6742 at iter 34394 **\n",
            "  ** New min smooth loss: 1.6740 at iter 34395 **\n",
            "Iter: 34400/100000, Smooth Loss: 1.6746, Min Smooth Loss: 1.6740 (at iter 34395), Time: 170.95s\n",
            "  ** New min smooth loss: 1.6739 at iter 34421 **\n",
            "  ** New min smooth loss: 1.6733 at iter 34422 **\n",
            "  ** New min smooth loss: 1.6728 at iter 34423 **\n",
            "  ** New min smooth loss: 1.6726 at iter 34428 **\n",
            "  ** New min smooth loss: 1.6719 at iter 34429 **\n",
            "  ** New min smooth loss: 1.6717 at iter 34430 **\n",
            "  ** New min smooth loss: 1.6714 at iter 34432 **\n",
            "  ** New min smooth loss: 1.6712 at iter 34433 **\n",
            "  ** New min smooth loss: 1.6711 at iter 34434 **\n",
            "Iter: 34500/100000, Smooth Loss: 1.6742, Min Smooth Loss: 1.6711 (at iter 34434), Time: 171.58s\n",
            "  ** New min smooth loss: 1.6706 at iter 34562 **\n",
            "  ** New min smooth loss: 1.6706 at iter 34563 **\n",
            "  ** New min smooth loss: 1.6705 at iter 34564 **\n",
            "  ** New min smooth loss: 1.6699 at iter 34566 **\n",
            "  ** New min smooth loss: 1.6696 at iter 34567 **\n",
            "  ** New min smooth loss: 1.6696 at iter 34571 **\n",
            "  ** New min smooth loss: 1.6694 at iter 34572 **\n",
            "  ** New min smooth loss: 1.6691 at iter 34573 **\n",
            "  ** New min smooth loss: 1.6689 at iter 34574 **\n",
            "  ** New min smooth loss: 1.6688 at iter 34577 **\n",
            "  ** New min smooth loss: 1.6688 at iter 34578 **\n",
            "  ** New min smooth loss: 1.6688 at iter 34580 **\n",
            "  ** New min smooth loss: 1.6685 at iter 34581 **\n",
            "Iter: 34600/100000, Smooth Loss: 1.6717, Min Smooth Loss: 1.6685 (at iter 34581), Time: 172.22s\n",
            "Iter: 34700/100000, Smooth Loss: 1.6826, Min Smooth Loss: 1.6685 (at iter 34581), Time: 172.87s\n",
            "Iter: 34800/100000, Smooth Loss: 1.6946, Min Smooth Loss: 1.6685 (at iter 34581), Time: 173.40s\n",
            "Iter: 34900/100000, Smooth Loss: 1.6892, Min Smooth Loss: 1.6685 (at iter 34581), Time: 173.82s\n",
            "Iter: 35000/100000, Smooth Loss: 1.7085, Min Smooth Loss: 1.6685 (at iter 34581), Time: 174.26s\n",
            "Iter: 35100/100000, Smooth Loss: 1.7081, Min Smooth Loss: 1.6685 (at iter 34581), Time: 174.68s\n",
            "Iter: 35200/100000, Smooth Loss: 1.7073, Min Smooth Loss: 1.6685 (at iter 34581), Time: 175.10s\n",
            "Iter: 35300/100000, Smooth Loss: 1.6980, Min Smooth Loss: 1.6685 (at iter 34581), Time: 175.53s\n",
            "Iter: 35400/100000, Smooth Loss: 1.6877, Min Smooth Loss: 1.6685 (at iter 34581), Time: 175.96s\n",
            "Iter: 35500/100000, Smooth Loss: 1.6886, Min Smooth Loss: 1.6685 (at iter 34581), Time: 176.39s\n",
            "Iter: 35600/100000, Smooth Loss: 1.6953, Min Smooth Loss: 1.6685 (at iter 34581), Time: 176.82s\n",
            "Iter: 35700/100000, Smooth Loss: 1.6989, Min Smooth Loss: 1.6685 (at iter 34581), Time: 177.25s\n",
            "Iter: 35800/100000, Smooth Loss: 1.7013, Min Smooth Loss: 1.6685 (at iter 34581), Time: 177.70s\n",
            "Iter: 35900/100000, Smooth Loss: 1.6978, Min Smooth Loss: 1.6685 (at iter 34581), Time: 178.12s\n",
            "Iter: 36000/100000, Smooth Loss: 1.7057, Min Smooth Loss: 1.6685 (at iter 34581), Time: 178.55s\n",
            "Iter: 36100/100000, Smooth Loss: 1.6971, Min Smooth Loss: 1.6685 (at iter 34581), Time: 178.98s\n",
            "Iter: 36200/100000, Smooth Loss: 1.6991, Min Smooth Loss: 1.6685 (at iter 34581), Time: 179.41s\n",
            "Iter: 36300/100000, Smooth Loss: 1.6906, Min Smooth Loss: 1.6685 (at iter 34581), Time: 179.86s\n",
            "Iter: 36400/100000, Smooth Loss: 1.6863, Min Smooth Loss: 1.6685 (at iter 34581), Time: 180.29s\n",
            "Iter: 36500/100000, Smooth Loss: 1.6748, Min Smooth Loss: 1.6685 (at iter 34581), Time: 180.73s\n",
            "Iter: 36600/100000, Smooth Loss: 1.6739, Min Smooth Loss: 1.6685 (at iter 34581), Time: 181.15s\n",
            "Iter: 36700/100000, Smooth Loss: 1.6776, Min Smooth Loss: 1.6685 (at iter 34581), Time: 181.59s\n",
            "Iter: 36800/100000, Smooth Loss: 1.6859, Min Smooth Loss: 1.6685 (at iter 34581), Time: 182.03s\n",
            "Iter: 36900/100000, Smooth Loss: 1.7010, Min Smooth Loss: 1.6685 (at iter 34581), Time: 182.45s\n",
            "Iter: 37000/100000, Smooth Loss: 1.7175, Min Smooth Loss: 1.6685 (at iter 34581), Time: 182.91s\n",
            "Iter: 37100/100000, Smooth Loss: 1.7158, Min Smooth Loss: 1.6685 (at iter 34581), Time: 183.44s\n",
            "Iter: 37200/100000, Smooth Loss: 1.7135, Min Smooth Loss: 1.6685 (at iter 34581), Time: 184.12s\n",
            "Iter: 37300/100000, Smooth Loss: 1.7079, Min Smooth Loss: 1.6685 (at iter 34581), Time: 184.76s\n",
            "Iter: 37400/100000, Smooth Loss: 1.7020, Min Smooth Loss: 1.6685 (at iter 34581), Time: 185.41s\n",
            "Iter: 37500/100000, Smooth Loss: 1.7059, Min Smooth Loss: 1.6685 (at iter 34581), Time: 186.06s\n",
            "Iter: 37600/100000, Smooth Loss: 1.7027, Min Smooth Loss: 1.6685 (at iter 34581), Time: 186.69s\n",
            "Iter: 37700/100000, Smooth Loss: 1.7059, Min Smooth Loss: 1.6685 (at iter 34581), Time: 187.39s\n",
            "Iter: 37800/100000, Smooth Loss: 1.7073, Min Smooth Loss: 1.6685 (at iter 34581), Time: 187.97s\n",
            "Iter: 37900/100000, Smooth Loss: 1.6989, Min Smooth Loss: 1.6685 (at iter 34581), Time: 188.39s\n",
            "Iter: 38000/100000, Smooth Loss: 1.7140, Min Smooth Loss: 1.6685 (at iter 34581), Time: 188.80s\n",
            "Iter: 38100/100000, Smooth Loss: 1.7171, Min Smooth Loss: 1.6685 (at iter 34581), Time: 189.24s\n",
            "Iter: 38200/100000, Smooth Loss: 1.7148, Min Smooth Loss: 1.6685 (at iter 34581), Time: 189.66s\n",
            "Iter: 38300/100000, Smooth Loss: 1.7014, Min Smooth Loss: 1.6685 (at iter 34581), Time: 190.11s\n",
            "Iter: 38400/100000, Smooth Loss: 1.7002, Min Smooth Loss: 1.6685 (at iter 34581), Time: 190.52s\n",
            "Iter: 38500/100000, Smooth Loss: 1.6968, Min Smooth Loss: 1.6685 (at iter 34581), Time: 190.95s\n",
            "Iter: 38600/100000, Smooth Loss: 1.6886, Min Smooth Loss: 1.6685 (at iter 34581), Time: 191.39s\n",
            "Iter: 38700/100000, Smooth Loss: 1.6849, Min Smooth Loss: 1.6685 (at iter 34581), Time: 191.81s\n",
            "Iter: 38800/100000, Smooth Loss: 1.6838, Min Smooth Loss: 1.6685 (at iter 34581), Time: 192.26s\n",
            "Iter: 38900/100000, Smooth Loss: 1.6884, Min Smooth Loss: 1.6685 (at iter 34581), Time: 192.67s\n",
            "Iter: 39000/100000, Smooth Loss: 1.6963, Min Smooth Loss: 1.6685 (at iter 34581), Time: 193.10s\n",
            "Iter: 39100/100000, Smooth Loss: 1.7025, Min Smooth Loss: 1.6685 (at iter 34581), Time: 193.53s\n",
            "Iter: 39200/100000, Smooth Loss: 1.7074, Min Smooth Loss: 1.6685 (at iter 34581), Time: 193.95s\n",
            "Iter: 39300/100000, Smooth Loss: 1.7064, Min Smooth Loss: 1.6685 (at iter 34581), Time: 194.38s\n",
            "Iter: 39400/100000, Smooth Loss: 1.7126, Min Smooth Loss: 1.6685 (at iter 34581), Time: 194.79s\n",
            "Iter: 39500/100000, Smooth Loss: 1.7118, Min Smooth Loss: 1.6685 (at iter 34581), Time: 195.22s\n",
            "Iter: 39600/100000, Smooth Loss: 1.7168, Min Smooth Loss: 1.6685 (at iter 34581), Time: 195.66s\n",
            "Iter: 39700/100000, Smooth Loss: 1.7074, Min Smooth Loss: 1.6685 (at iter 34581), Time: 196.09s\n",
            "Iter: 39800/100000, Smooth Loss: 1.6981, Min Smooth Loss: 1.6685 (at iter 34581), Time: 196.53s\n",
            "Iter: 39900/100000, Smooth Loss: 1.6912, Min Smooth Loss: 1.6685 (at iter 34581), Time: 196.98s\n",
            "Iter: 40000/100000, Smooth Loss: 1.6835, Min Smooth Loss: 1.6685 (at iter 34581), Time: 197.42s\n",
            "--- Synthesized text at iter 40000 ---\n",
            "he gand, whithand-inrierlyed ave, Harry becand fliend, thit, thought thes nook it Snough lith and have lookend - they wan the thavegs.  Hf wint Lapble doing to gazing and sheter.  And he retereded, wh\n",
            "---\n",
            "Iter: 40100/100000, Smooth Loss: 1.6728, Min Smooth Loss: 1.6685 (at iter 34581), Time: 197.95s\n",
            "  ** New min smooth loss: 1.6682 at iter 40116 **\n",
            "  ** New min smooth loss: 1.6675 at iter 40117 **\n",
            "  ** New min smooth loss: 1.6671 at iter 40118 **\n",
            "  ** New min smooth loss: 1.6671 at iter 40119 **\n",
            "  ** New min smooth loss: 1.6670 at iter 40120 **\n",
            "  ** New min smooth loss: 1.6665 at iter 40121 **\n",
            "  ** New min smooth loss: 1.6662 at iter 40122 **\n",
            "  ** New min smooth loss: 1.6659 at iter 40123 **\n",
            "  ** New min smooth loss: 1.6656 at iter 40125 **\n",
            "  ** New min smooth loss: 1.6652 at iter 40132 **\n",
            "  ** New min smooth loss: 1.6651 at iter 40133 **\n",
            "  ** New min smooth loss: 1.6651 at iter 40135 **\n",
            "  ** New min smooth loss: 1.6644 at iter 40137 **\n",
            "  ** New min smooth loss: 1.6644 at iter 40140 **\n",
            "  ** New min smooth loss: 1.6643 at iter 40144 **\n",
            "  ** New min smooth loss: 1.6637 at iter 40145 **\n",
            "  ** New min smooth loss: 1.6635 at iter 40148 **\n",
            "  ** New min smooth loss: 1.6632 at iter 40167 **\n",
            "  ** New min smooth loss: 1.6630 at iter 40168 **\n",
            "  ** New min smooth loss: 1.6623 at iter 40169 **\n",
            "  ** New min smooth loss: 1.6622 at iter 40171 **\n",
            "  ** New min smooth loss: 1.6617 at iter 40172 **\n",
            "  ** New min smooth loss: 1.6611 at iter 40173 **\n",
            "  ** New min smooth loss: 1.6608 at iter 40175 **\n",
            "  ** New min smooth loss: 1.6601 at iter 40176 **\n",
            "  ** New min smooth loss: 1.6599 at iter 40185 **\n",
            "  ** New min smooth loss: 1.6597 at iter 40186 **\n",
            "  ** New min smooth loss: 1.6593 at iter 40190 **\n",
            "  ** New min smooth loss: 1.6592 at iter 40197 **\n",
            "  ** New min smooth loss: 1.6591 at iter 40198 **\n",
            "Iter: 40200/100000, Smooth Loss: 1.6593, Min Smooth Loss: 1.6591 (at iter 40198), Time: 198.63s\n",
            "  ** New min smooth loss: 1.6590 at iter 40201 **\n",
            "  ** New min smooth loss: 1.6590 at iter 40209 **\n",
            "  ** New min smooth loss: 1.6584 at iter 40233 **\n",
            "  ** New min smooth loss: 1.6578 at iter 40234 **\n",
            "  ** New min smooth loss: 1.6577 at iter 40235 **\n",
            "  ** New min smooth loss: 1.6572 at iter 40236 **\n",
            "  ** New min smooth loss: 1.6567 at iter 40237 **\n",
            "  ** New min smooth loss: 1.6566 at iter 40238 **\n",
            "  ** New min smooth loss: 1.6558 at iter 40239 **\n",
            "  ** New min smooth loss: 1.6556 at iter 40241 **\n",
            "  ** New min smooth loss: 1.6553 at iter 40242 **\n",
            "  ** New min smooth loss: 1.6549 at iter 40243 **\n",
            "  ** New min smooth loss: 1.6549 at iter 40248 **\n",
            "  ** New min smooth loss: 1.6546 at iter 40249 **\n",
            "  ** New min smooth loss: 1.6545 at iter 40250 **\n",
            "  ** New min smooth loss: 1.6542 at iter 40251 **\n",
            "  ** New min smooth loss: 1.6539 at iter 40253 **\n",
            "  ** New min smooth loss: 1.6537 at iter 40254 **\n",
            "  ** New min smooth loss: 1.6537 at iter 40255 **\n",
            "  ** New min smooth loss: 1.6535 at iter 40264 **\n",
            "  ** New min smooth loss: 1.6531 at iter 40267 **\n",
            "  ** New min smooth loss: 1.6529 at iter 40268 **\n",
            "  ** New min smooth loss: 1.6527 at iter 40269 **\n",
            "  ** New min smooth loss: 1.6526 at iter 40270 **\n",
            "  ** New min smooth loss: 1.6519 at iter 40271 **\n",
            "  ** New min smooth loss: 1.6518 at iter 40274 **\n",
            "  ** New min smooth loss: 1.6518 at iter 40275 **\n",
            "  ** New min smooth loss: 1.6516 at iter 40278 **\n",
            "  ** New min smooth loss: 1.6516 at iter 40286 **\n",
            "  ** New min smooth loss: 1.6514 at iter 40287 **\n",
            "  ** New min smooth loss: 1.6511 at iter 40288 **\n",
            "  ** New min smooth loss: 1.6511 at iter 40292 **\n",
            "  ** New min smooth loss: 1.6509 at iter 40294 **\n",
            "  ** New min smooth loss: 1.6509 at iter 40295 **\n",
            "Iter: 40300/100000, Smooth Loss: 1.6516, Min Smooth Loss: 1.6509 (at iter 40295), Time: 199.29s\n",
            "Iter: 40400/100000, Smooth Loss: 1.6523, Min Smooth Loss: 1.6509 (at iter 40295), Time: 199.96s\n",
            "  ** New min smooth loss: 1.6508 at iter 40412 **\n",
            "  ** New min smooth loss: 1.6508 at iter 40414 **\n",
            "  ** New min smooth loss: 1.6503 at iter 40415 **\n",
            "  ** New min smooth loss: 1.6503 at iter 40416 **\n",
            "  ** New min smooth loss: 1.6500 at iter 40447 **\n",
            "  ** New min smooth loss: 1.6498 at iter 40448 **\n",
            "  ** New min smooth loss: 1.6494 at iter 40449 **\n",
            "  ** New min smooth loss: 1.6493 at iter 40450 **\n",
            "  ** New min smooth loss: 1.6489 at iter 40451 **\n",
            "  ** New min smooth loss: 1.6488 at iter 40455 **\n",
            "Iter: 40500/100000, Smooth Loss: 1.6490, Min Smooth Loss: 1.6488 (at iter 40455), Time: 200.61s\n",
            "  ** New min smooth loss: 1.6487 at iter 40544 **\n",
            "  ** New min smooth loss: 1.6487 at iter 40549 **\n",
            "  ** New min smooth loss: 1.6485 at iter 40550 **\n",
            "  ** New min smooth loss: 1.6483 at iter 40551 **\n",
            "  ** New min smooth loss: 1.6480 at iter 40575 **\n",
            "  ** New min smooth loss: 1.6476 at iter 40582 **\n",
            "  ** New min smooth loss: 1.6473 at iter 40583 **\n",
            "  ** New min smooth loss: 1.6470 at iter 40584 **\n",
            "  ** New min smooth loss: 1.6468 at iter 40585 **\n",
            "  ** New min smooth loss: 1.6465 at iter 40586 **\n",
            "Iter: 40600/100000, Smooth Loss: 1.6487, Min Smooth Loss: 1.6465 (at iter 40586), Time: 201.25s\n",
            "  ** New min smooth loss: 1.6462 at iter 40628 **\n",
            "  ** New min smooth loss: 1.6459 at iter 40629 **\n",
            "  ** New min smooth loss: 1.6456 at iter 40632 **\n",
            "  ** New min smooth loss: 1.6456 at iter 40633 **\n",
            "  ** New min smooth loss: 1.6452 at iter 40634 **\n",
            "  ** New min smooth loss: 1.6448 at iter 40635 **\n",
            "  ** New min smooth loss: 1.6447 at iter 40637 **\n",
            "  ** New min smooth loss: 1.6442 at iter 40638 **\n",
            "  ** New min smooth loss: 1.6442 at iter 40643 **\n",
            "  ** New min smooth loss: 1.6441 at iter 40644 **\n",
            "  ** New min smooth loss: 1.6433 at iter 40645 **\n",
            "  ** New min smooth loss: 1.6432 at iter 40646 **\n",
            "  ** New min smooth loss: 1.6429 at iter 40647 **\n",
            "  ** New min smooth loss: 1.6427 at iter 40670 **\n",
            "  ** New min smooth loss: 1.6424 at iter 40673 **\n",
            "  ** New min smooth loss: 1.6424 at iter 40675 **\n",
            "  ** New min smooth loss: 1.6421 at iter 40676 **\n",
            "  ** New min smooth loss: 1.6420 at iter 40677 **\n",
            "  ** New min smooth loss: 1.6419 at iter 40678 **\n",
            "  ** New min smooth loss: 1.6415 at iter 40679 **\n",
            "  ** New min smooth loss: 1.6412 at iter 40680 **\n",
            "  ** New min smooth loss: 1.6409 at iter 40681 **\n",
            "  ** New min smooth loss: 1.6401 at iter 40682 **\n",
            "  ** New min smooth loss: 1.6400 at iter 40686 **\n",
            "  ** New min smooth loss: 1.6394 at iter 40687 **\n",
            "  ** New min smooth loss: 1.6386 at iter 40690 **\n",
            "Iter: 40700/100000, Smooth Loss: 1.6411, Min Smooth Loss: 1.6386 (at iter 40690), Time: 201.98s\n",
            "Iter: 40800/100000, Smooth Loss: 1.6463, Min Smooth Loss: 1.6386 (at iter 40690), Time: 202.54s\n",
            "Iter: 40900/100000, Smooth Loss: 1.6489, Min Smooth Loss: 1.6386 (at iter 40690), Time: 203.00s\n",
            "Iter: 41000/100000, Smooth Loss: 1.6444, Min Smooth Loss: 1.6386 (at iter 40690), Time: 203.42s\n",
            "Iter: 41100/100000, Smooth Loss: 1.6527, Min Smooth Loss: 1.6386 (at iter 40690), Time: 203.86s\n",
            "Iter: 41200/100000, Smooth Loss: 1.6547, Min Smooth Loss: 1.6386 (at iter 40690), Time: 204.29s\n",
            "Iter: 41300/100000, Smooth Loss: 1.6621, Min Smooth Loss: 1.6386 (at iter 40690), Time: 204.71s\n",
            "Iter: 41400/100000, Smooth Loss: 1.6617, Min Smooth Loss: 1.6386 (at iter 40690), Time: 205.17s\n",
            "Iter: 41500/100000, Smooth Loss: 1.6556, Min Smooth Loss: 1.6386 (at iter 40690), Time: 205.59s\n",
            "Iter: 41600/100000, Smooth Loss: 1.6568, Min Smooth Loss: 1.6386 (at iter 40690), Time: 206.07s\n",
            "Iter: 41700/100000, Smooth Loss: 1.6409, Min Smooth Loss: 1.6386 (at iter 40690), Time: 206.50s\n",
            "  ** New min smooth loss: 1.6381 at iter 41707 **\n",
            "  ** New min smooth loss: 1.6378 at iter 41708 **\n",
            "  ** New min smooth loss: 1.6371 at iter 41709 **\n",
            "  ** New min smooth loss: 1.6371 at iter 41710 **\n",
            "  ** New min smooth loss: 1.6369 at iter 41711 **\n",
            "Iter: 41800/100000, Smooth Loss: 1.6413, Min Smooth Loss: 1.6369 (at iter 41711), Time: 206.97s\n",
            "Iter: 41900/100000, Smooth Loss: 1.6404, Min Smooth Loss: 1.6369 (at iter 41711), Time: 207.42s\n",
            "  ** New min smooth loss: 1.6366 at iter 41968 **\n",
            "  ** New min smooth loss: 1.6364 at iter 41969 **\n",
            "  ** New min smooth loss: 1.6360 at iter 41970 **\n",
            "  ** New min smooth loss: 1.6359 at iter 41971 **\n",
            "  ** New min smooth loss: 1.6359 at iter 41973 **\n",
            "  ** New min smooth loss: 1.6356 at iter 41974 **\n",
            "  ** New min smooth loss: 1.6354 at iter 41977 **\n",
            "  ** New min smooth loss: 1.6353 at iter 41984 **\n",
            "  ** New min smooth loss: 1.6352 at iter 41985 **\n",
            "  ** New min smooth loss: 1.6351 at iter 41988 **\n",
            "  ** New min smooth loss: 1.6346 at iter 41990 **\n",
            "  ** New min smooth loss: 1.6343 at iter 41991 **\n",
            "  ** New min smooth loss: 1.6342 at iter 41992 **\n",
            "  ** New min smooth loss: 1.6341 at iter 41993 **\n",
            "  ** New min smooth loss: 1.6340 at iter 41995 **\n",
            "Iter: 42000/100000, Smooth Loss: 1.6344, Min Smooth Loss: 1.6340 (at iter 41995), Time: 207.87s\n",
            "  ** New min smooth loss: 1.6339 at iter 42003 **\n",
            "  ** New min smooth loss: 1.6335 at iter 42004 **\n",
            "  ** New min smooth loss: 1.6332 at iter 42005 **\n",
            "  ** New min smooth loss: 1.6328 at iter 42006 **\n",
            "  ** New min smooth loss: 1.6321 at iter 42007 **\n",
            "  ** New min smooth loss: 1.6320 at iter 42009 **\n",
            "  ** New min smooth loss: 1.6317 at iter 42011 **\n",
            "  ** New min smooth loss: 1.6313 at iter 42012 **\n",
            "  ** New min smooth loss: 1.6310 at iter 42016 **\n",
            "  ** New min smooth loss: 1.6306 at iter 42017 **\n",
            "  ** New min smooth loss: 1.6303 at iter 42018 **\n",
            "  ** New min smooth loss: 1.6302 at iter 42019 **\n",
            "  ** New min smooth loss: 1.6297 at iter 42022 **\n",
            "  ** New min smooth loss: 1.6296 at iter 42025 **\n",
            "  ** New min smooth loss: 1.6289 at iter 42026 **\n",
            "  ** New min smooth loss: 1.6284 at iter 42027 **\n",
            "  ** New min smooth loss: 1.6278 at iter 42028 **\n",
            "Iter: 42100/100000, Smooth Loss: 1.6299, Min Smooth Loss: 1.6278 (at iter 42028), Time: 208.34s\n",
            "  ** New min smooth loss: 1.6277 at iter 42110 **\n",
            "  ** New min smooth loss: 1.6271 at iter 42112 **\n",
            "  ** New min smooth loss: 1.6267 at iter 42133 **\n",
            "  ** New min smooth loss: 1.6265 at iter 42134 **\n",
            "  ** New min smooth loss: 1.6261 at iter 42176 **\n",
            "  ** New min smooth loss: 1.6259 at iter 42189 **\n",
            "  ** New min smooth loss: 1.6253 at iter 42190 **\n",
            "  ** New min smooth loss: 1.6253 at iter 42200 **\n",
            "Iter: 42200/100000, Smooth Loss: 1.6253, Min Smooth Loss: 1.6253 (at iter 42200), Time: 208.78s\n",
            "  ** New min smooth loss: 1.6252 at iter 42207 **\n",
            "  ** New min smooth loss: 1.6248 at iter 42208 **\n",
            "  ** New min smooth loss: 1.6245 at iter 42245 **\n",
            "  ** New min smooth loss: 1.6241 at iter 42246 **\n",
            "Iter: 42300/100000, Smooth Loss: 1.6272, Min Smooth Loss: 1.6241 (at iter 42246), Time: 209.24s\n",
            "Iter: 42400/100000, Smooth Loss: 1.6293, Min Smooth Loss: 1.6241 (at iter 42246), Time: 209.66s\n",
            "Iter: 42500/100000, Smooth Loss: 1.6336, Min Smooth Loss: 1.6241 (at iter 42246), Time: 210.17s\n",
            "Iter: 42600/100000, Smooth Loss: 1.6319, Min Smooth Loss: 1.6241 (at iter 42246), Time: 210.62s\n",
            "Iter: 42700/100000, Smooth Loss: 1.6427, Min Smooth Loss: 1.6241 (at iter 42246), Time: 211.08s\n",
            "Iter: 42800/100000, Smooth Loss: 1.6465, Min Smooth Loss: 1.6241 (at iter 42246), Time: 211.56s\n",
            "Iter: 42900/100000, Smooth Loss: 1.6473, Min Smooth Loss: 1.6241 (at iter 42246), Time: 212.03s\n",
            "Iter: 43000/100000, Smooth Loss: 1.6451, Min Smooth Loss: 1.6241 (at iter 42246), Time: 212.56s\n",
            "Iter: 43100/100000, Smooth Loss: 1.6423, Min Smooth Loss: 1.6241 (at iter 42246), Time: 213.23s\n",
            "Iter: 43200/100000, Smooth Loss: 1.6371, Min Smooth Loss: 1.6241 (at iter 42246), Time: 213.89s\n",
            "Iter: 43300/100000, Smooth Loss: 1.6343, Min Smooth Loss: 1.6241 (at iter 42246), Time: 214.58s\n",
            "Iter: 43400/100000, Smooth Loss: 1.6450, Min Smooth Loss: 1.6241 (at iter 42246), Time: 215.22s\n",
            "Iter: 43500/100000, Smooth Loss: 1.6402, Min Smooth Loss: 1.6241 (at iter 42246), Time: 215.85s\n",
            "Iter: 43600/100000, Smooth Loss: 1.6389, Min Smooth Loss: 1.6241 (at iter 42246), Time: 216.52s\n",
            "Iter: 43700/100000, Smooth Loss: 1.6434, Min Smooth Loss: 1.6241 (at iter 42246), Time: 217.09s\n",
            "Iter: 43800/100000, Smooth Loss: 1.6533, Min Smooth Loss: 1.6241 (at iter 42246), Time: 217.53s\n",
            "Iter: 43900/100000, Smooth Loss: 1.6514, Min Smooth Loss: 1.6241 (at iter 42246), Time: 217.98s\n",
            "Iter: 44000/100000, Smooth Loss: 1.6511, Min Smooth Loss: 1.6241 (at iter 42246), Time: 218.40s\n",
            "Iter: 44100/100000, Smooth Loss: 1.6539, Min Smooth Loss: 1.6241 (at iter 42246), Time: 218.83s\n",
            "Iter: 44200/100000, Smooth Loss: 1.6582, Min Smooth Loss: 1.6241 (at iter 42246), Time: 219.26s\n",
            "Iter: 44300/100000, Smooth Loss: 1.6561, Min Smooth Loss: 1.6241 (at iter 42246), Time: 219.67s\n",
            "Iter: 44400/100000, Smooth Loss: 1.6839, Min Smooth Loss: 1.6241 (at iter 42246), Time: 220.11s\n",
            "Iter: 44500/100000, Smooth Loss: 1.6918, Min Smooth Loss: 1.6241 (at iter 42246), Time: 220.53s\n",
            "Iter: 44600/100000, Smooth Loss: 1.6962, Min Smooth Loss: 1.6241 (at iter 42246), Time: 220.97s\n",
            "Iter: 44700/100000, Smooth Loss: 1.6938, Min Smooth Loss: 1.6241 (at iter 42246), Time: 221.38s\n",
            "Iter: 44800/100000, Smooth Loss: 1.6976, Min Smooth Loss: 1.6241 (at iter 42246), Time: 221.80s\n",
            "Iter: 44900/100000, Smooth Loss: 1.7030, Min Smooth Loss: 1.6241 (at iter 42246), Time: 222.24s\n",
            "Iter: 45000/100000, Smooth Loss: 1.7016, Min Smooth Loss: 1.6241 (at iter 42246), Time: 222.68s\n",
            "Iter: 45100/100000, Smooth Loss: 1.6996, Min Smooth Loss: 1.6241 (at iter 42246), Time: 223.12s\n",
            "Iter: 45200/100000, Smooth Loss: 1.6930, Min Smooth Loss: 1.6241 (at iter 42246), Time: 223.54s\n",
            "Iter: 45300/100000, Smooth Loss: 1.6908, Min Smooth Loss: 1.6241 (at iter 42246), Time: 224.01s\n",
            "Iter: 45400/100000, Smooth Loss: 1.6882, Min Smooth Loss: 1.6241 (at iter 42246), Time: 224.42s\n",
            "Iter: 45500/100000, Smooth Loss: 1.6849, Min Smooth Loss: 1.6241 (at iter 42246), Time: 224.85s\n",
            "Iter: 45600/100000, Smooth Loss: 1.6807, Min Smooth Loss: 1.6241 (at iter 42246), Time: 225.30s\n",
            "Iter: 45700/100000, Smooth Loss: 1.6873, Min Smooth Loss: 1.6241 (at iter 42246), Time: 225.73s\n",
            "Iter: 45800/100000, Smooth Loss: 1.6828, Min Smooth Loss: 1.6241 (at iter 42246), Time: 226.18s\n",
            "Iter: 45900/100000, Smooth Loss: 1.7027, Min Smooth Loss: 1.6241 (at iter 42246), Time: 226.62s\n",
            "Iter: 46000/100000, Smooth Loss: 1.7157, Min Smooth Loss: 1.6241 (at iter 42246), Time: 227.16s\n",
            "Iter: 46100/100000, Smooth Loss: 1.7178, Min Smooth Loss: 1.6241 (at iter 42246), Time: 227.80s\n",
            "Iter: 46200/100000, Smooth Loss: 1.7206, Min Smooth Loss: 1.6241 (at iter 42246), Time: 228.45s\n",
            "Iter: 46300/100000, Smooth Loss: 1.7135, Min Smooth Loss: 1.6241 (at iter 42246), Time: 229.11s\n",
            "Iter: 46400/100000, Smooth Loss: 1.7060, Min Smooth Loss: 1.6241 (at iter 42246), Time: 229.77s\n",
            "Iter: 46500/100000, Smooth Loss: 1.7023, Min Smooth Loss: 1.6241 (at iter 42246), Time: 230.43s\n",
            "Iter: 46600/100000, Smooth Loss: 1.7121, Min Smooth Loss: 1.6241 (at iter 42246), Time: 231.09s\n",
            "Iter: 46700/100000, Smooth Loss: 1.7123, Min Smooth Loss: 1.6241 (at iter 42246), Time: 231.69s\n",
            "Iter: 46800/100000, Smooth Loss: 1.7103, Min Smooth Loss: 1.6241 (at iter 42246), Time: 232.11s\n",
            "Iter: 46900/100000, Smooth Loss: 1.7149, Min Smooth Loss: 1.6241 (at iter 42246), Time: 232.53s\n",
            "Iter: 47000/100000, Smooth Loss: 1.7077, Min Smooth Loss: 1.6241 (at iter 42246), Time: 232.98s\n",
            "Iter: 47100/100000, Smooth Loss: 1.6983, Min Smooth Loss: 1.6241 (at iter 42246), Time: 233.40s\n",
            "Iter: 47200/100000, Smooth Loss: 1.6851, Min Smooth Loss: 1.6241 (at iter 42246), Time: 233.84s\n",
            "Iter: 47300/100000, Smooth Loss: 1.6791, Min Smooth Loss: 1.6241 (at iter 42246), Time: 234.27s\n",
            "Iter: 47400/100000, Smooth Loss: 1.6863, Min Smooth Loss: 1.6241 (at iter 42246), Time: 234.72s\n",
            "Iter: 47500/100000, Smooth Loss: 1.6805, Min Smooth Loss: 1.6241 (at iter 42246), Time: 235.14s\n",
            "Iter: 47600/100000, Smooth Loss: 1.6872, Min Smooth Loss: 1.6241 (at iter 42246), Time: 235.56s\n",
            "Iter: 47700/100000, Smooth Loss: 1.6932, Min Smooth Loss: 1.6241 (at iter 42246), Time: 236.01s\n",
            "Iter: 47800/100000, Smooth Loss: 1.6913, Min Smooth Loss: 1.6241 (at iter 42246), Time: 236.44s\n",
            "Iter: 47900/100000, Smooth Loss: 1.6924, Min Smooth Loss: 1.6241 (at iter 42246), Time: 236.93s\n",
            "Iter: 48000/100000, Smooth Loss: 1.7001, Min Smooth Loss: 1.6241 (at iter 42246), Time: 237.38s\n",
            "Iter: 48100/100000, Smooth Loss: 1.7073, Min Smooth Loss: 1.6241 (at iter 42246), Time: 237.83s\n",
            "Iter: 48200/100000, Smooth Loss: 1.7094, Min Smooth Loss: 1.6241 (at iter 42246), Time: 238.25s\n",
            "Iter: 48300/100000, Smooth Loss: 1.7057, Min Smooth Loss: 1.6241 (at iter 42246), Time: 238.69s\n",
            "Iter: 48400/100000, Smooth Loss: 1.7067, Min Smooth Loss: 1.6241 (at iter 42246), Time: 239.13s\n",
            "Iter: 48500/100000, Smooth Loss: 1.7087, Min Smooth Loss: 1.6241 (at iter 42246), Time: 239.55s\n",
            "Iter: 48600/100000, Smooth Loss: 1.7083, Min Smooth Loss: 1.6241 (at iter 42246), Time: 240.01s\n",
            "Iter: 48700/100000, Smooth Loss: 1.7081, Min Smooth Loss: 1.6241 (at iter 42246), Time: 240.45s\n",
            "Iter: 48800/100000, Smooth Loss: 1.7159, Min Smooth Loss: 1.6241 (at iter 42246), Time: 240.88s\n",
            "Iter: 48900/100000, Smooth Loss: 1.7089, Min Smooth Loss: 1.6241 (at iter 42246), Time: 241.33s\n",
            "Iter: 49000/100000, Smooth Loss: 1.7127, Min Smooth Loss: 1.6241 (at iter 42246), Time: 241.96s\n",
            "Iter: 49100/100000, Smooth Loss: 1.7129, Min Smooth Loss: 1.6241 (at iter 42246), Time: 242.61s\n",
            "Iter: 49200/100000, Smooth Loss: 1.7200, Min Smooth Loss: 1.6241 (at iter 42246), Time: 243.31s\n",
            "Iter: 49300/100000, Smooth Loss: 1.7173, Min Smooth Loss: 1.6241 (at iter 42246), Time: 243.94s\n",
            "Iter: 49400/100000, Smooth Loss: 1.7173, Min Smooth Loss: 1.6241 (at iter 42246), Time: 244.56s\n",
            "Iter: 49500/100000, Smooth Loss: 1.7253, Min Smooth Loss: 1.6241 (at iter 42246), Time: 245.21s\n",
            "Iter: 49600/100000, Smooth Loss: 1.7322, Min Smooth Loss: 1.6241 (at iter 42246), Time: 245.89s\n",
            "Iter: 49700/100000, Smooth Loss: 1.7365, Min Smooth Loss: 1.6241 (at iter 42246), Time: 246.36s\n",
            "Iter: 49800/100000, Smooth Loss: 1.7366, Min Smooth Loss: 1.6241 (at iter 42246), Time: 246.81s\n",
            "Iter: 49900/100000, Smooth Loss: 1.7409, Min Smooth Loss: 1.6241 (at iter 42246), Time: 247.24s\n",
            "Iter: 50000/100000, Smooth Loss: 1.7406, Min Smooth Loss: 1.6241 (at iter 42246), Time: 247.68s\n",
            "--- Synthesized text at iter 50000 ---\n",
            "and coulding, and weat chought.  \"You?\"\n",
            "\"Judd.\n",
            "Ag in fill,\" said Ron,\" said Rentals the Mrmimbs Cus forthan inthe the sappover of that wizlot'v it them wairseasly wizard his righted their arment you p\n",
            "---\n",
            "Iter: 50100/100000, Smooth Loss: 1.7498, Min Smooth Loss: 1.6241 (at iter 42246), Time: 248.15s\n",
            "Iter: 50200/100000, Smooth Loss: 1.7457, Min Smooth Loss: 1.6241 (at iter 42246), Time: 248.62s\n",
            "Iter: 50300/100000, Smooth Loss: 1.7417, Min Smooth Loss: 1.6241 (at iter 42246), Time: 249.06s\n",
            "Iter: 50400/100000, Smooth Loss: 1.7401, Min Smooth Loss: 1.6241 (at iter 42246), Time: 249.49s\n",
            "Iter: 50500/100000, Smooth Loss: 1.7473, Min Smooth Loss: 1.6241 (at iter 42246), Time: 249.95s\n",
            "Iter: 50600/100000, Smooth Loss: 1.7533, Min Smooth Loss: 1.6241 (at iter 42246), Time: 250.38s\n",
            "Iter: 50700/100000, Smooth Loss: 1.7766, Min Smooth Loss: 1.6241 (at iter 42246), Time: 250.82s\n",
            "Iter: 50800/100000, Smooth Loss: 1.7830, Min Smooth Loss: 1.6241 (at iter 42246), Time: 251.24s\n",
            "Iter: 50900/100000, Smooth Loss: 1.7819, Min Smooth Loss: 1.6241 (at iter 42246), Time: 251.69s\n",
            "Iter: 51000/100000, Smooth Loss: 1.7740, Min Smooth Loss: 1.6241 (at iter 42246), Time: 252.12s\n",
            "Iter: 51100/100000, Smooth Loss: 1.7731, Min Smooth Loss: 1.6241 (at iter 42246), Time: 252.54s\n",
            "Iter: 51200/100000, Smooth Loss: 1.7708, Min Smooth Loss: 1.6241 (at iter 42246), Time: 252.99s\n",
            "Iter: 51300/100000, Smooth Loss: 1.7613, Min Smooth Loss: 1.6241 (at iter 42246), Time: 253.42s\n",
            "Iter: 51400/100000, Smooth Loss: 1.7582, Min Smooth Loss: 1.6241 (at iter 42246), Time: 253.86s\n",
            "Iter: 51500/100000, Smooth Loss: 1.7483, Min Smooth Loss: 1.6241 (at iter 42246), Time: 254.30s\n",
            "Iter: 51600/100000, Smooth Loss: 1.7331, Min Smooth Loss: 1.6241 (at iter 42246), Time: 254.72s\n",
            "Iter: 51700/100000, Smooth Loss: 1.7263, Min Smooth Loss: 1.6241 (at iter 42246), Time: 255.17s\n",
            "Iter: 51800/100000, Smooth Loss: 1.7202, Min Smooth Loss: 1.6241 (at iter 42246), Time: 255.59s\n",
            "Iter: 51900/100000, Smooth Loss: 1.7199, Min Smooth Loss: 1.6241 (at iter 42246), Time: 256.07s\n",
            "Iter: 52000/100000, Smooth Loss: 1.7078, Min Smooth Loss: 1.6241 (at iter 42246), Time: 256.74s\n",
            "Iter: 52100/100000, Smooth Loss: 1.7037, Min Smooth Loss: 1.6241 (at iter 42246), Time: 257.38s\n",
            "Iter: 52200/100000, Smooth Loss: 1.6916, Min Smooth Loss: 1.6241 (at iter 42246), Time: 258.06s\n",
            "Iter: 52300/100000, Smooth Loss: 1.6875, Min Smooth Loss: 1.6241 (at iter 42246), Time: 258.69s\n",
            "Iter: 52400/100000, Smooth Loss: 1.6844, Min Smooth Loss: 1.6241 (at iter 42246), Time: 259.34s\n",
            "Iter: 52500/100000, Smooth Loss: 1.6750, Min Smooth Loss: 1.6241 (at iter 42246), Time: 260.00s\n",
            "Iter: 52600/100000, Smooth Loss: 1.6674, Min Smooth Loss: 1.6241 (at iter 42246), Time: 260.65s\n",
            "Iter: 52700/100000, Smooth Loss: 1.6598, Min Smooth Loss: 1.6241 (at iter 42246), Time: 261.07s\n",
            "Iter: 52800/100000, Smooth Loss: 1.6466, Min Smooth Loss: 1.6241 (at iter 42246), Time: 261.51s\n",
            "Iter: 52900/100000, Smooth Loss: 1.6412, Min Smooth Loss: 1.6241 (at iter 42246), Time: 261.96s\n",
            "Iter: 53000/100000, Smooth Loss: 1.6424, Min Smooth Loss: 1.6241 (at iter 42246), Time: 262.40s\n",
            "Iter: 53100/100000, Smooth Loss: 1.6564, Min Smooth Loss: 1.6241 (at iter 42246), Time: 262.82s\n",
            "Iter: 53200/100000, Smooth Loss: 1.6637, Min Smooth Loss: 1.6241 (at iter 42246), Time: 263.24s\n",
            "Iter: 53300/100000, Smooth Loss: 1.6607, Min Smooth Loss: 1.6241 (at iter 42246), Time: 263.69s\n",
            "Iter: 53400/100000, Smooth Loss: 1.6685, Min Smooth Loss: 1.6241 (at iter 42246), Time: 264.10s\n",
            "Iter: 53500/100000, Smooth Loss: 1.6788, Min Smooth Loss: 1.6241 (at iter 42246), Time: 264.54s\n",
            "Iter: 53600/100000, Smooth Loss: 1.6767, Min Smooth Loss: 1.6241 (at iter 42246), Time: 264.96s\n",
            "Iter: 53700/100000, Smooth Loss: 1.6819, Min Smooth Loss: 1.6241 (at iter 42246), Time: 265.39s\n",
            "Iter: 53800/100000, Smooth Loss: 1.6815, Min Smooth Loss: 1.6241 (at iter 42246), Time: 265.84s\n",
            "Iter: 53900/100000, Smooth Loss: 1.6782, Min Smooth Loss: 1.6241 (at iter 42246), Time: 266.26s\n",
            "Iter: 54000/100000, Smooth Loss: 1.6746, Min Smooth Loss: 1.6241 (at iter 42246), Time: 266.71s\n",
            "Iter: 54100/100000, Smooth Loss: 1.6858, Min Smooth Loss: 1.6241 (at iter 42246), Time: 267.14s\n",
            "Iter: 54200/100000, Smooth Loss: 1.6750, Min Smooth Loss: 1.6241 (at iter 42246), Time: 267.56s\n",
            "Iter: 54300/100000, Smooth Loss: 1.6816, Min Smooth Loss: 1.6241 (at iter 42246), Time: 268.01s\n",
            "Iter: 54400/100000, Smooth Loss: 1.6948, Min Smooth Loss: 1.6241 (at iter 42246), Time: 268.43s\n",
            "Iter: 54500/100000, Smooth Loss: 1.6968, Min Smooth Loss: 1.6241 (at iter 42246), Time: 268.86s\n",
            "Iter: 54600/100000, Smooth Loss: 1.7044, Min Smooth Loss: 1.6241 (at iter 42246), Time: 269.28s\n",
            "Iter: 54700/100000, Smooth Loss: 1.7121, Min Smooth Loss: 1.6241 (at iter 42246), Time: 269.73s\n",
            "Iter: 54800/100000, Smooth Loss: 1.7099, Min Smooth Loss: 1.6241 (at iter 42246), Time: 270.17s\n",
            "Iter: 54900/100000, Smooth Loss: 1.7260, Min Smooth Loss: 1.6241 (at iter 42246), Time: 270.62s\n",
            "Iter: 55000/100000, Smooth Loss: 1.7415, Min Smooth Loss: 1.6241 (at iter 42246), Time: 271.30s\n",
            "Iter: 55100/100000, Smooth Loss: 1.7494, Min Smooth Loss: 1.6241 (at iter 42246), Time: 271.98s\n",
            "Iter: 55200/100000, Smooth Loss: 1.7487, Min Smooth Loss: 1.6241 (at iter 42246), Time: 272.62s\n",
            "Iter: 55300/100000, Smooth Loss: 1.7522, Min Smooth Loss: 1.6241 (at iter 42246), Time: 273.30s\n",
            "Iter: 55400/100000, Smooth Loss: 1.7401, Min Smooth Loss: 1.6241 (at iter 42246), Time: 273.93s\n",
            "Iter: 55500/100000, Smooth Loss: 1.7401, Min Smooth Loss: 1.6241 (at iter 42246), Time: 274.58s\n",
            "Iter: 55600/100000, Smooth Loss: 1.7381, Min Smooth Loss: 1.6241 (at iter 42246), Time: 275.23s\n",
            "Iter: 55700/100000, Smooth Loss: 1.7283, Min Smooth Loss: 1.6241 (at iter 42246), Time: 275.66s\n",
            "Iter: 55800/100000, Smooth Loss: 1.7366, Min Smooth Loss: 1.6241 (at iter 42246), Time: 276.08s\n",
            "Iter: 55900/100000, Smooth Loss: 1.7358, Min Smooth Loss: 1.6241 (at iter 42246), Time: 276.52s\n",
            "Iter: 56000/100000, Smooth Loss: 1.7444, Min Smooth Loss: 1.6241 (at iter 42246), Time: 276.95s\n",
            "Iter: 56100/100000, Smooth Loss: 1.7546, Min Smooth Loss: 1.6241 (at iter 42246), Time: 277.40s\n",
            "Iter: 56200/100000, Smooth Loss: 1.7495, Min Smooth Loss: 1.6241 (at iter 42246), Time: 277.85s\n",
            "Iter: 56300/100000, Smooth Loss: 1.7463, Min Smooth Loss: 1.6241 (at iter 42246), Time: 278.31s\n",
            "Iter: 56400/100000, Smooth Loss: 1.7458, Min Smooth Loss: 1.6241 (at iter 42246), Time: 278.73s\n",
            "Iter: 56500/100000, Smooth Loss: 1.7492, Min Smooth Loss: 1.6241 (at iter 42246), Time: 279.16s\n",
            "Iter: 56600/100000, Smooth Loss: 1.7413, Min Smooth Loss: 1.6241 (at iter 42246), Time: 279.60s\n",
            "Iter: 56700/100000, Smooth Loss: 1.7268, Min Smooth Loss: 1.6241 (at iter 42246), Time: 280.03s\n",
            "Iter: 56800/100000, Smooth Loss: 1.7230, Min Smooth Loss: 1.6241 (at iter 42246), Time: 280.48s\n",
            "Iter: 56900/100000, Smooth Loss: 1.7225, Min Smooth Loss: 1.6241 (at iter 42246), Time: 280.91s\n",
            "Iter: 57000/100000, Smooth Loss: 1.7185, Min Smooth Loss: 1.6241 (at iter 42246), Time: 281.33s\n",
            "Iter: 57100/100000, Smooth Loss: 1.7121, Min Smooth Loss: 1.6241 (at iter 42246), Time: 281.78s\n",
            "Iter: 57200/100000, Smooth Loss: 1.7047, Min Smooth Loss: 1.6241 (at iter 42246), Time: 282.21s\n",
            "Iter: 57300/100000, Smooth Loss: 1.7078, Min Smooth Loss: 1.6241 (at iter 42246), Time: 282.65s\n",
            "Iter: 57400/100000, Smooth Loss: 1.6936, Min Smooth Loss: 1.6241 (at iter 42246), Time: 283.08s\n",
            "Iter: 57500/100000, Smooth Loss: 1.6836, Min Smooth Loss: 1.6241 (at iter 42246), Time: 283.52s\n",
            "Iter: 57600/100000, Smooth Loss: 1.6857, Min Smooth Loss: 1.6241 (at iter 42246), Time: 283.94s\n",
            "Iter: 57700/100000, Smooth Loss: 1.6821, Min Smooth Loss: 1.6241 (at iter 42246), Time: 284.36s\n",
            "Iter: 57800/100000, Smooth Loss: 1.6914, Min Smooth Loss: 1.6241 (at iter 42246), Time: 284.80s\n",
            "Iter: 57900/100000, Smooth Loss: 1.6899, Min Smooth Loss: 1.6241 (at iter 42246), Time: 285.34s\n",
            "Iter: 58000/100000, Smooth Loss: 1.6920, Min Smooth Loss: 1.6241 (at iter 42246), Time: 286.01s\n",
            "Iter: 58100/100000, Smooth Loss: 1.6931, Min Smooth Loss: 1.6241 (at iter 42246), Time: 286.65s\n",
            "Iter: 58200/100000, Smooth Loss: 1.6938, Min Smooth Loss: 1.6241 (at iter 42246), Time: 287.32s\n",
            "Iter: 58300/100000, Smooth Loss: 1.7055, Min Smooth Loss: 1.6241 (at iter 42246), Time: 287.98s\n",
            "Iter: 58400/100000, Smooth Loss: 1.7246, Min Smooth Loss: 1.6241 (at iter 42246), Time: 288.60s\n",
            "Iter: 58500/100000, Smooth Loss: 1.7238, Min Smooth Loss: 1.6241 (at iter 42246), Time: 289.30s\n",
            "Iter: 58600/100000, Smooth Loss: 1.7281, Min Smooth Loss: 1.6241 (at iter 42246), Time: 289.85s\n",
            "Iter: 58700/100000, Smooth Loss: 1.7228, Min Smooth Loss: 1.6241 (at iter 42246), Time: 290.29s\n",
            "Iter: 58800/100000, Smooth Loss: 1.7233, Min Smooth Loss: 1.6241 (at iter 42246), Time: 290.70s\n",
            "Iter: 58900/100000, Smooth Loss: 1.7258, Min Smooth Loss: 1.6241 (at iter 42246), Time: 291.13s\n",
            "Iter: 59000/100000, Smooth Loss: 1.7197, Min Smooth Loss: 1.6241 (at iter 42246), Time: 291.55s\n",
            "Iter: 59100/100000, Smooth Loss: 1.7146, Min Smooth Loss: 1.6241 (at iter 42246), Time: 292.00s\n",
            "Iter: 59200/100000, Smooth Loss: 1.7063, Min Smooth Loss: 1.6241 (at iter 42246), Time: 292.43s\n",
            "Iter: 59300/100000, Smooth Loss: 1.6978, Min Smooth Loss: 1.6241 (at iter 42246), Time: 292.85s\n",
            "Iter: 59400/100000, Smooth Loss: 1.6917, Min Smooth Loss: 1.6241 (at iter 42246), Time: 293.28s\n",
            "Iter: 59500/100000, Smooth Loss: 1.6863, Min Smooth Loss: 1.6241 (at iter 42246), Time: 293.70s\n",
            "Iter: 59600/100000, Smooth Loss: 1.6809, Min Smooth Loss: 1.6241 (at iter 42246), Time: 294.14s\n",
            "Iter: 59700/100000, Smooth Loss: 1.6726, Min Smooth Loss: 1.6241 (at iter 42246), Time: 294.56s\n",
            "Iter: 59800/100000, Smooth Loss: 1.6611, Min Smooth Loss: 1.6241 (at iter 42246), Time: 294.99s\n",
            "Iter: 59900/100000, Smooth Loss: 1.6578, Min Smooth Loss: 1.6241 (at iter 42246), Time: 295.43s\n",
            "Iter: 60000/100000, Smooth Loss: 1.6550, Min Smooth Loss: 1.6241 (at iter 42246), Time: 295.85s\n",
            "--- Synthesized text at iter 60000 ---\n",
            "r student, Lodd the gottel, cone, veal outt back fred grinnishin; you they can't as And expite called once,\" said Maxable.\n",
            "\"With,\" said Heree Joyffet?\"  \"I was right, her Harry, like said steds takned\n",
            "---\n",
            "Iter: 60100/100000, Smooth Loss: 1.6574, Min Smooth Loss: 1.6241 (at iter 42246), Time: 296.32s\n",
            "Iter: 60200/100000, Smooth Loss: 1.6660, Min Smooth Loss: 1.6241 (at iter 42246), Time: 296.74s\n",
            "Iter: 60300/100000, Smooth Loss: 1.6591, Min Smooth Loss: 1.6241 (at iter 42246), Time: 297.17s\n",
            "Iter: 60400/100000, Smooth Loss: 1.6496, Min Smooth Loss: 1.6241 (at iter 42246), Time: 297.61s\n",
            "Iter: 60500/100000, Smooth Loss: 1.6476, Min Smooth Loss: 1.6241 (at iter 42246), Time: 298.04s\n",
            "Iter: 60600/100000, Smooth Loss: 1.6343, Min Smooth Loss: 1.6241 (at iter 42246), Time: 298.48s\n",
            "Iter: 60700/100000, Smooth Loss: 1.6329, Min Smooth Loss: 1.6241 (at iter 42246), Time: 298.91s\n",
            "Iter: 60800/100000, Smooth Loss: 1.6422, Min Smooth Loss: 1.6241 (at iter 42246), Time: 299.32s\n",
            "Iter: 60900/100000, Smooth Loss: 1.6353, Min Smooth Loss: 1.6241 (at iter 42246), Time: 299.82s\n",
            "Iter: 61000/100000, Smooth Loss: 1.6424, Min Smooth Loss: 1.6241 (at iter 42246), Time: 300.51s\n",
            "Iter: 61100/100000, Smooth Loss: 1.6435, Min Smooth Loss: 1.6241 (at iter 42246), Time: 301.17s\n",
            "Iter: 61200/100000, Smooth Loss: 1.6381, Min Smooth Loss: 1.6241 (at iter 42246), Time: 301.88s\n",
            "Iter: 61300/100000, Smooth Loss: 1.6322, Min Smooth Loss: 1.6241 (at iter 42246), Time: 302.53s\n",
            "Iter: 61400/100000, Smooth Loss: 1.6339, Min Smooth Loss: 1.6241 (at iter 42246), Time: 303.15s\n",
            "Iter: 61500/100000, Smooth Loss: 1.6292, Min Smooth Loss: 1.6241 (at iter 42246), Time: 303.86s\n",
            "Iter: 61600/100000, Smooth Loss: 1.6278, Min Smooth Loss: 1.6241 (at iter 42246), Time: 304.44s\n",
            "Iter: 61700/100000, Smooth Loss: 1.6253, Min Smooth Loss: 1.6241 (at iter 42246), Time: 304.87s\n",
            "  ** New min smooth loss: 1.6237 at iter 61732 **\n",
            "  ** New min smooth loss: 1.6235 at iter 61733 **\n",
            "  ** New min smooth loss: 1.6232 at iter 61740 **\n",
            "  ** New min smooth loss: 1.6231 at iter 61748 **\n",
            "  ** New min smooth loss: 1.6229 at iter 61754 **\n",
            "Iter: 61800/100000, Smooth Loss: 1.6260, Min Smooth Loss: 1.6229 (at iter 61754), Time: 305.31s\n",
            "  ** New min smooth loss: 1.6225 at iter 61812 **\n",
            "  ** New min smooth loss: 1.6221 at iter 61816 **\n",
            "Iter: 61900/100000, Smooth Loss: 1.6364, Min Smooth Loss: 1.6221 (at iter 61816), Time: 305.73s\n",
            "Iter: 62000/100000, Smooth Loss: 1.6414, Min Smooth Loss: 1.6221 (at iter 61816), Time: 306.17s\n",
            "Iter: 62100/100000, Smooth Loss: 1.6456, Min Smooth Loss: 1.6221 (at iter 61816), Time: 306.60s\n",
            "Iter: 62200/100000, Smooth Loss: 1.6550, Min Smooth Loss: 1.6221 (at iter 61816), Time: 307.07s\n",
            "Iter: 62300/100000, Smooth Loss: 1.6568, Min Smooth Loss: 1.6221 (at iter 61816), Time: 307.50s\n",
            "Iter: 62400/100000, Smooth Loss: 1.6523, Min Smooth Loss: 1.6221 (at iter 61816), Time: 307.95s\n",
            "Iter: 62500/100000, Smooth Loss: 1.6613, Min Smooth Loss: 1.6221 (at iter 61816), Time: 308.37s\n",
            "Iter: 62600/100000, Smooth Loss: 1.6662, Min Smooth Loss: 1.6221 (at iter 61816), Time: 308.79s\n",
            "Iter: 62700/100000, Smooth Loss: 1.6585, Min Smooth Loss: 1.6221 (at iter 61816), Time: 309.23s\n",
            "Iter: 62800/100000, Smooth Loss: 1.6635, Min Smooth Loss: 1.6221 (at iter 61816), Time: 309.67s\n",
            "Iter: 62900/100000, Smooth Loss: 1.6656, Min Smooth Loss: 1.6221 (at iter 61816), Time: 310.11s\n",
            "Iter: 63000/100000, Smooth Loss: 1.6629, Min Smooth Loss: 1.6221 (at iter 61816), Time: 310.54s\n",
            "Iter: 63100/100000, Smooth Loss: 1.6647, Min Smooth Loss: 1.6221 (at iter 61816), Time: 310.96s\n",
            "Iter: 63200/100000, Smooth Loss: 1.6664, Min Smooth Loss: 1.6221 (at iter 61816), Time: 311.39s\n",
            "Iter: 63300/100000, Smooth Loss: 1.6605, Min Smooth Loss: 1.6221 (at iter 61816), Time: 311.80s\n",
            "Iter: 63400/100000, Smooth Loss: 1.6631, Min Smooth Loss: 1.6221 (at iter 61816), Time: 312.25s\n",
            "Iter: 63500/100000, Smooth Loss: 1.6609, Min Smooth Loss: 1.6221 (at iter 61816), Time: 312.67s\n",
            "Iter: 63600/100000, Smooth Loss: 1.6566, Min Smooth Loss: 1.6221 (at iter 61816), Time: 313.10s\n",
            "Iter: 63700/100000, Smooth Loss: 1.6479, Min Smooth Loss: 1.6221 (at iter 61816), Time: 313.54s\n",
            "Iter: 63800/100000, Smooth Loss: 1.6458, Min Smooth Loss: 1.6221 (at iter 61816), Time: 313.98s\n",
            "Iter: 63900/100000, Smooth Loss: 1.6515, Min Smooth Loss: 1.6221 (at iter 61816), Time: 314.52s\n",
            "Iter: 64000/100000, Smooth Loss: 1.6576, Min Smooth Loss: 1.6221 (at iter 61816), Time: 315.17s\n",
            "Iter: 64100/100000, Smooth Loss: 1.6616, Min Smooth Loss: 1.6221 (at iter 61816), Time: 315.83s\n",
            "Iter: 64200/100000, Smooth Loss: 1.6535, Min Smooth Loss: 1.6221 (at iter 61816), Time: 316.50s\n",
            "Iter: 64300/100000, Smooth Loss: 1.6476, Min Smooth Loss: 1.6221 (at iter 61816), Time: 317.14s\n",
            "Iter: 64400/100000, Smooth Loss: 1.6388, Min Smooth Loss: 1.6221 (at iter 61816), Time: 317.78s\n",
            "Iter: 64500/100000, Smooth Loss: 1.6346, Min Smooth Loss: 1.6221 (at iter 61816), Time: 318.47s\n",
            "Iter: 64600/100000, Smooth Loss: 1.6401, Min Smooth Loss: 1.6221 (at iter 61816), Time: 319.02s\n",
            "Iter: 64700/100000, Smooth Loss: 1.6433, Min Smooth Loss: 1.6221 (at iter 61816), Time: 319.45s\n",
            "Iter: 64800/100000, Smooth Loss: 1.6354, Min Smooth Loss: 1.6221 (at iter 61816), Time: 319.89s\n",
            "Iter: 64900/100000, Smooth Loss: 1.6359, Min Smooth Loss: 1.6221 (at iter 61816), Time: 320.32s\n",
            "Iter: 65000/100000, Smooth Loss: 1.6392, Min Smooth Loss: 1.6221 (at iter 61816), Time: 320.75s\n",
            "Iter: 65100/100000, Smooth Loss: 1.6415, Min Smooth Loss: 1.6221 (at iter 61816), Time: 321.18s\n",
            "Iter: 65200/100000, Smooth Loss: 1.6369, Min Smooth Loss: 1.6221 (at iter 61816), Time: 321.65s\n",
            "Iter: 65300/100000, Smooth Loss: 1.6289, Min Smooth Loss: 1.6221 (at iter 61816), Time: 322.13s\n",
            "Iter: 65400/100000, Smooth Loss: 1.6321, Min Smooth Loss: 1.6221 (at iter 61816), Time: 322.60s\n",
            "Iter: 65500/100000, Smooth Loss: 1.6292, Min Smooth Loss: 1.6221 (at iter 61816), Time: 323.09s\n",
            "Iter: 65600/100000, Smooth Loss: 1.6303, Min Smooth Loss: 1.6221 (at iter 61816), Time: 323.55s\n",
            "Iter: 65700/100000, Smooth Loss: 1.6291, Min Smooth Loss: 1.6221 (at iter 61816), Time: 324.05s\n",
            "Iter: 65800/100000, Smooth Loss: 1.6288, Min Smooth Loss: 1.6221 (at iter 61816), Time: 324.54s\n",
            "Iter: 65900/100000, Smooth Loss: 1.6245, Min Smooth Loss: 1.6221 (at iter 61816), Time: 325.04s\n",
            "  ** New min smooth loss: 1.6217 at iter 65993 **\n",
            "  ** New min smooth loss: 1.6216 at iter 65994 **\n",
            "  ** New min smooth loss: 1.6210 at iter 65995 **\n",
            "Iter: 66000/100000, Smooth Loss: 1.6211, Min Smooth Loss: 1.6210 (at iter 65995), Time: 325.53s\n",
            "  ** New min smooth loss: 1.6210 at iter 66001 **\n",
            "  ** New min smooth loss: 1.6208 at iter 66002 **\n",
            "  ** New min smooth loss: 1.6207 at iter 66003 **\n",
            "  ** New min smooth loss: 1.6207 at iter 66010 **\n",
            "  ** New min smooth loss: 1.6204 at iter 66011 **\n",
            "  ** New min smooth loss: 1.6200 at iter 66050 **\n",
            "  ** New min smooth loss: 1.6199 at iter 66051 **\n",
            "  ** New min smooth loss: 1.6193 at iter 66052 **\n",
            "  ** New min smooth loss: 1.6193 at iter 66053 **\n",
            "  ** New min smooth loss: 1.6192 at iter 66054 **\n",
            "  ** New min smooth loss: 1.6191 at iter 66055 **\n",
            "  ** New min smooth loss: 1.6190 at iter 66056 **\n",
            "  ** New min smooth loss: 1.6186 at iter 66057 **\n",
            "  ** New min smooth loss: 1.6182 at iter 66060 **\n",
            "  ** New min smooth loss: 1.6180 at iter 66061 **\n",
            "  ** New min smooth loss: 1.6177 at iter 66096 **\n",
            "  ** New min smooth loss: 1.6176 at iter 66097 **\n",
            "  ** New min smooth loss: 1.6174 at iter 66098 **\n",
            "  ** New min smooth loss: 1.6170 at iter 66099 **\n",
            "  ** New min smooth loss: 1.6168 at iter 66100 **\n",
            "Iter: 66100/100000, Smooth Loss: 1.6168, Min Smooth Loss: 1.6168 (at iter 66100), Time: 326.03s\n",
            "  ** New min smooth loss: 1.6167 at iter 66103 **\n",
            "  ** New min smooth loss: 1.6166 at iter 66125 **\n",
            "  ** New min smooth loss: 1.6162 at iter 66126 **\n",
            "  ** New min smooth loss: 1.6158 at iter 66127 **\n",
            "  ** New min smooth loss: 1.6154 at iter 66130 **\n",
            "  ** New min smooth loss: 1.6152 at iter 66131 **\n",
            "  ** New min smooth loss: 1.6151 at iter 66169 **\n",
            "  ** New min smooth loss: 1.6150 at iter 66176 **\n",
            "  ** New min smooth loss: 1.6145 at iter 66181 **\n",
            "  ** New min smooth loss: 1.6143 at iter 66182 **\n",
            "  ** New min smooth loss: 1.6140 at iter 66183 **\n",
            "Iter: 66200/100000, Smooth Loss: 1.6149, Min Smooth Loss: 1.6140 (at iter 66183), Time: 326.48s\n",
            "  ** New min smooth loss: 1.6139 at iter 66207 **\n",
            "  ** New min smooth loss: 1.6137 at iter 66212 **\n",
            "  ** New min smooth loss: 1.6136 at iter 66215 **\n",
            "  ** New min smooth loss: 1.6136 at iter 66216 **\n",
            "  ** New min smooth loss: 1.6133 at iter 66217 **\n",
            "  ** New min smooth loss: 1.6131 at iter 66218 **\n",
            "  ** New min smooth loss: 1.6130 at iter 66219 **\n",
            "  ** New min smooth loss: 1.6128 at iter 66220 **\n",
            "Iter: 66300/100000, Smooth Loss: 1.6151, Min Smooth Loss: 1.6128 (at iter 66220), Time: 326.94s\n",
            "Iter: 66400/100000, Smooth Loss: 1.6236, Min Smooth Loss: 1.6128 (at iter 66220), Time: 327.39s\n",
            "Iter: 66500/100000, Smooth Loss: 1.6368, Min Smooth Loss: 1.6128 (at iter 66220), Time: 327.82s\n",
            "Iter: 66600/100000, Smooth Loss: 1.6357, Min Smooth Loss: 1.6128 (at iter 66220), Time: 328.27s\n",
            "Iter: 66700/100000, Smooth Loss: 1.6426, Min Smooth Loss: 1.6128 (at iter 66220), Time: 328.72s\n",
            "Iter: 66800/100000, Smooth Loss: 1.6513, Min Smooth Loss: 1.6128 (at iter 66220), Time: 329.37s\n",
            "Iter: 66900/100000, Smooth Loss: 1.6458, Min Smooth Loss: 1.6128 (at iter 66220), Time: 330.02s\n",
            "Iter: 67000/100000, Smooth Loss: 1.6499, Min Smooth Loss: 1.6128 (at iter 66220), Time: 330.73s\n",
            "Iter: 67100/100000, Smooth Loss: 1.6426, Min Smooth Loss: 1.6128 (at iter 66220), Time: 331.38s\n",
            "Iter: 67200/100000, Smooth Loss: 1.6373, Min Smooth Loss: 1.6128 (at iter 66220), Time: 332.03s\n",
            "Iter: 67300/100000, Smooth Loss: 1.6311, Min Smooth Loss: 1.6128 (at iter 66220), Time: 332.69s\n",
            "Iter: 67400/100000, Smooth Loss: 1.6235, Min Smooth Loss: 1.6128 (at iter 66220), Time: 333.38s\n",
            "Iter: 67500/100000, Smooth Loss: 1.6352, Min Smooth Loss: 1.6128 (at iter 66220), Time: 333.80s\n",
            "Iter: 67600/100000, Smooth Loss: 1.6341, Min Smooth Loss: 1.6128 (at iter 66220), Time: 334.23s\n",
            "Iter: 67700/100000, Smooth Loss: 1.6363, Min Smooth Loss: 1.6128 (at iter 66220), Time: 334.68s\n",
            "Iter: 67800/100000, Smooth Loss: 1.6402, Min Smooth Loss: 1.6128 (at iter 66220), Time: 335.11s\n",
            "Iter: 67900/100000, Smooth Loss: 1.6502, Min Smooth Loss: 1.6128 (at iter 66220), Time: 335.55s\n",
            "Iter: 68000/100000, Smooth Loss: 1.6507, Min Smooth Loss: 1.6128 (at iter 66220), Time: 335.97s\n",
            "Iter: 68100/100000, Smooth Loss: 1.6543, Min Smooth Loss: 1.6128 (at iter 66220), Time: 336.38s\n",
            "Iter: 68200/100000, Smooth Loss: 1.6601, Min Smooth Loss: 1.6128 (at iter 66220), Time: 336.81s\n",
            "Iter: 68300/100000, Smooth Loss: 1.6494, Min Smooth Loss: 1.6128 (at iter 66220), Time: 337.23s\n",
            "Iter: 68400/100000, Smooth Loss: 1.6340, Min Smooth Loss: 1.6128 (at iter 66220), Time: 337.67s\n",
            "Iter: 68500/100000, Smooth Loss: 1.6353, Min Smooth Loss: 1.6128 (at iter 66220), Time: 338.10s\n",
            "Iter: 68600/100000, Smooth Loss: 1.6423, Min Smooth Loss: 1.6128 (at iter 66220), Time: 338.51s\n",
            "Iter: 68700/100000, Smooth Loss: 1.6537, Min Smooth Loss: 1.6128 (at iter 66220), Time: 338.96s\n",
            "Iter: 68800/100000, Smooth Loss: 1.6532, Min Smooth Loss: 1.6128 (at iter 66220), Time: 339.39s\n",
            "Iter: 68900/100000, Smooth Loss: 1.6570, Min Smooth Loss: 1.6128 (at iter 66220), Time: 339.83s\n",
            "Iter: 69000/100000, Smooth Loss: 1.6680, Min Smooth Loss: 1.6128 (at iter 66220), Time: 340.26s\n",
            "Iter: 69100/100000, Smooth Loss: 1.6650, Min Smooth Loss: 1.6128 (at iter 66220), Time: 340.69s\n",
            "Iter: 69200/100000, Smooth Loss: 1.6615, Min Smooth Loss: 1.6128 (at iter 66220), Time: 341.11s\n",
            "Iter: 69300/100000, Smooth Loss: 1.6562, Min Smooth Loss: 1.6128 (at iter 66220), Time: 341.52s\n",
            "Iter: 69400/100000, Smooth Loss: 1.6623, Min Smooth Loss: 1.6128 (at iter 66220), Time: 341.97s\n",
            "Iter: 69500/100000, Smooth Loss: 1.6761, Min Smooth Loss: 1.6128 (at iter 66220), Time: 342.39s\n",
            "Iter: 69600/100000, Smooth Loss: 1.6726, Min Smooth Loss: 1.6128 (at iter 66220), Time: 342.83s\n",
            "Iter: 69700/100000, Smooth Loss: 1.6623, Min Smooth Loss: 1.6128 (at iter 66220), Time: 343.27s\n",
            "Iter: 69800/100000, Smooth Loss: 1.6536, Min Smooth Loss: 1.6128 (at iter 66220), Time: 343.89s\n",
            "Iter: 69900/100000, Smooth Loss: 1.6539, Min Smooth Loss: 1.6128 (at iter 66220), Time: 344.53s\n",
            "Iter: 70000/100000, Smooth Loss: 1.6622, Min Smooth Loss: 1.6128 (at iter 66220), Time: 345.18s\n",
            "--- Synthesized text at iter 70000 ---\n",
            "'me, by her a didine conce to then the Back in fance bessurastred Valins.  Hen.  She welling in his, they let!\n",
            "\n",
            "Then whetly wintly - and Ron, I wisned he came in whon, I woube've in all now much he wi\n",
            "---\n",
            "Iter: 70100/100000, Smooth Loss: 1.6682, Min Smooth Loss: 1.6128 (at iter 66220), Time: 345.85s\n",
            "Iter: 70200/100000, Smooth Loss: 1.6641, Min Smooth Loss: 1.6128 (at iter 66220), Time: 346.52s\n",
            "Iter: 70300/100000, Smooth Loss: 1.6552, Min Smooth Loss: 1.6128 (at iter 66220), Time: 347.24s\n",
            "Iter: 70400/100000, Smooth Loss: 1.6558, Min Smooth Loss: 1.6128 (at iter 66220), Time: 347.93s\n",
            "Iter: 70500/100000, Smooth Loss: 1.6497, Min Smooth Loss: 1.6128 (at iter 66220), Time: 348.39s\n",
            "Iter: 70600/100000, Smooth Loss: 1.6475, Min Smooth Loss: 1.6128 (at iter 66220), Time: 348.82s\n",
            "Iter: 70700/100000, Smooth Loss: 1.6737, Min Smooth Loss: 1.6128 (at iter 66220), Time: 349.27s\n",
            "Iter: 70800/100000, Smooth Loss: 1.6910, Min Smooth Loss: 1.6128 (at iter 66220), Time: 349.69s\n",
            "Iter: 70900/100000, Smooth Loss: 1.6756, Min Smooth Loss: 1.6128 (at iter 66220), Time: 350.12s\n",
            "Iter: 71000/100000, Smooth Loss: 1.6678, Min Smooth Loss: 1.6128 (at iter 66220), Time: 350.56s\n",
            "Iter: 71100/100000, Smooth Loss: 1.6574, Min Smooth Loss: 1.6128 (at iter 66220), Time: 350.99s\n",
            "Iter: 71200/100000, Smooth Loss: 1.6549, Min Smooth Loss: 1.6128 (at iter 66220), Time: 351.44s\n",
            "Iter: 71300/100000, Smooth Loss: 1.6477, Min Smooth Loss: 1.6128 (at iter 66220), Time: 351.89s\n",
            "Iter: 71400/100000, Smooth Loss: 1.6507, Min Smooth Loss: 1.6128 (at iter 66220), Time: 352.33s\n",
            "Iter: 71500/100000, Smooth Loss: 1.6478, Min Smooth Loss: 1.6128 (at iter 66220), Time: 352.77s\n",
            "Iter: 71600/100000, Smooth Loss: 1.6374, Min Smooth Loss: 1.6128 (at iter 66220), Time: 353.21s\n",
            "Iter: 71700/100000, Smooth Loss: 1.6394, Min Smooth Loss: 1.6128 (at iter 66220), Time: 353.65s\n",
            "Iter: 71800/100000, Smooth Loss: 1.6529, Min Smooth Loss: 1.6128 (at iter 66220), Time: 354.07s\n",
            "Iter: 71900/100000, Smooth Loss: 1.6612, Min Smooth Loss: 1.6128 (at iter 66220), Time: 354.52s\n",
            "Iter: 72000/100000, Smooth Loss: 1.6677, Min Smooth Loss: 1.6128 (at iter 66220), Time: 354.94s\n",
            "Iter: 72100/100000, Smooth Loss: 1.6703, Min Smooth Loss: 1.6128 (at iter 66220), Time: 355.36s\n",
            "Iter: 72200/100000, Smooth Loss: 1.6727, Min Smooth Loss: 1.6128 (at iter 66220), Time: 355.80s\n",
            "Iter: 72300/100000, Smooth Loss: 1.6711, Min Smooth Loss: 1.6128 (at iter 66220), Time: 356.22s\n",
            "Iter: 72400/100000, Smooth Loss: 1.6687, Min Smooth Loss: 1.6128 (at iter 66220), Time: 356.68s\n",
            "Iter: 72500/100000, Smooth Loss: 1.6739, Min Smooth Loss: 1.6128 (at iter 66220), Time: 357.12s\n",
            "Iter: 72600/100000, Smooth Loss: 1.6726, Min Smooth Loss: 1.6128 (at iter 66220), Time: 357.54s\n",
            "Iter: 72700/100000, Smooth Loss: 1.6717, Min Smooth Loss: 1.6128 (at iter 66220), Time: 358.03s\n",
            "Iter: 72800/100000, Smooth Loss: 1.6674, Min Smooth Loss: 1.6128 (at iter 66220), Time: 358.69s\n",
            "Iter: 72900/100000, Smooth Loss: 1.6558, Min Smooth Loss: 1.6128 (at iter 66220), Time: 359.34s\n",
            "Iter: 73000/100000, Smooth Loss: 1.6425, Min Smooth Loss: 1.6128 (at iter 66220), Time: 360.02s\n",
            "Iter: 73100/100000, Smooth Loss: 1.6308, Min Smooth Loss: 1.6128 (at iter 66220), Time: 360.66s\n",
            "Iter: 73200/100000, Smooth Loss: 1.6311, Min Smooth Loss: 1.6128 (at iter 66220), Time: 361.29s\n",
            "Iter: 73300/100000, Smooth Loss: 1.6249, Min Smooth Loss: 1.6128 (at iter 66220), Time: 361.93s\n",
            "Iter: 73400/100000, Smooth Loss: 1.6354, Min Smooth Loss: 1.6128 (at iter 66220), Time: 362.58s\n",
            "Iter: 73500/100000, Smooth Loss: 1.6449, Min Smooth Loss: 1.6128 (at iter 66220), Time: 363.02s\n",
            "Iter: 73600/100000, Smooth Loss: 1.6492, Min Smooth Loss: 1.6128 (at iter 66220), Time: 363.48s\n",
            "Iter: 73700/100000, Smooth Loss: 1.6577, Min Smooth Loss: 1.6128 (at iter 66220), Time: 363.94s\n",
            "Iter: 73800/100000, Smooth Loss: 1.6597, Min Smooth Loss: 1.6128 (at iter 66220), Time: 364.40s\n",
            "Iter: 73900/100000, Smooth Loss: 1.6565, Min Smooth Loss: 1.6128 (at iter 66220), Time: 364.83s\n",
            "Iter: 74000/100000, Smooth Loss: 1.6485, Min Smooth Loss: 1.6128 (at iter 66220), Time: 365.30s\n",
            "Iter: 74100/100000, Smooth Loss: 1.6427, Min Smooth Loss: 1.6128 (at iter 66220), Time: 365.75s\n",
            "Iter: 74200/100000, Smooth Loss: 1.6476, Min Smooth Loss: 1.6128 (at iter 66220), Time: 366.21s\n",
            "Iter: 74300/100000, Smooth Loss: 1.6486, Min Smooth Loss: 1.6128 (at iter 66220), Time: 366.64s\n",
            "Iter: 74400/100000, Smooth Loss: 1.6469, Min Smooth Loss: 1.6128 (at iter 66220), Time: 367.08s\n",
            "Iter: 74500/100000, Smooth Loss: 1.6407, Min Smooth Loss: 1.6128 (at iter 66220), Time: 367.53s\n",
            "Iter: 74600/100000, Smooth Loss: 1.6280, Min Smooth Loss: 1.6128 (at iter 66220), Time: 367.96s\n",
            "Iter: 74700/100000, Smooth Loss: 1.6182, Min Smooth Loss: 1.6128 (at iter 66220), Time: 368.40s\n",
            "Iter: 74800/100000, Smooth Loss: 1.6167, Min Smooth Loss: 1.6128 (at iter 66220), Time: 368.85s\n",
            "Iter: 74900/100000, Smooth Loss: 1.6248, Min Smooth Loss: 1.6128 (at iter 66220), Time: 369.29s\n",
            "Iter: 75000/100000, Smooth Loss: 1.6262, Min Smooth Loss: 1.6128 (at iter 66220), Time: 369.75s\n",
            "Iter: 75100/100000, Smooth Loss: 1.6339, Min Smooth Loss: 1.6128 (at iter 66220), Time: 370.20s\n",
            "Iter: 75200/100000, Smooth Loss: 1.6530, Min Smooth Loss: 1.6128 (at iter 66220), Time: 370.68s\n",
            "Iter: 75300/100000, Smooth Loss: 1.6481, Min Smooth Loss: 1.6128 (at iter 66220), Time: 371.12s\n",
            "Iter: 75400/100000, Smooth Loss: 1.6490, Min Smooth Loss: 1.6128 (at iter 66220), Time: 371.58s\n",
            "Iter: 75500/100000, Smooth Loss: 1.6475, Min Smooth Loss: 1.6128 (at iter 66220), Time: 372.03s\n",
            "Iter: 75600/100000, Smooth Loss: 1.6387, Min Smooth Loss: 1.6128 (at iter 66220), Time: 372.48s\n",
            "Iter: 75700/100000, Smooth Loss: 1.6384, Min Smooth Loss: 1.6128 (at iter 66220), Time: 373.13s\n",
            "Iter: 75800/100000, Smooth Loss: 1.6461, Min Smooth Loss: 1.6128 (at iter 66220), Time: 373.80s\n",
            "Iter: 75900/100000, Smooth Loss: 1.6433, Min Smooth Loss: 1.6128 (at iter 66220), Time: 374.48s\n",
            "Iter: 76000/100000, Smooth Loss: 1.6300, Min Smooth Loss: 1.6128 (at iter 66220), Time: 375.16s\n",
            "Iter: 76100/100000, Smooth Loss: 1.6251, Min Smooth Loss: 1.6128 (at iter 66220), Time: 375.92s\n",
            "Iter: 76200/100000, Smooth Loss: 1.6227, Min Smooth Loss: 1.6128 (at iter 66220), Time: 376.64s\n",
            "Iter: 76300/100000, Smooth Loss: 1.6230, Min Smooth Loss: 1.6128 (at iter 66220), Time: 377.37s\n",
            "Iter: 76400/100000, Smooth Loss: 1.6172, Min Smooth Loss: 1.6128 (at iter 66220), Time: 377.83s\n",
            "  ** New min smooth loss: 1.6128 at iter 76426 **\n",
            "  ** New min smooth loss: 1.6127 at iter 76431 **\n",
            "  ** New min smooth loss: 1.6121 at iter 76432 **\n",
            "  ** New min smooth loss: 1.6119 at iter 76440 **\n",
            "  ** New min smooth loss: 1.6118 at iter 76441 **\n",
            "  ** New min smooth loss: 1.6115 at iter 76442 **\n",
            "  ** New min smooth loss: 1.6111 at iter 76444 **\n",
            "  ** New min smooth loss: 1.6108 at iter 76445 **\n",
            "  ** New min smooth loss: 1.6104 at iter 76447 **\n",
            "  ** New min smooth loss: 1.6099 at iter 76473 **\n",
            "  ** New min smooth loss: 1.6099 at iter 76474 **\n",
            "Iter: 76500/100000, Smooth Loss: 1.6112, Min Smooth Loss: 1.6099 (at iter 76474), Time: 378.29s\n",
            "  ** New min smooth loss: 1.6097 at iter 76507 **\n",
            "  ** New min smooth loss: 1.6095 at iter 76508 **\n",
            "  ** New min smooth loss: 1.6094 at iter 76509 **\n",
            "  ** New min smooth loss: 1.6093 at iter 76512 **\n",
            "  ** New min smooth loss: 1.6089 at iter 76513 **\n",
            "  ** New min smooth loss: 1.6087 at iter 76514 **\n",
            "  ** New min smooth loss: 1.6083 at iter 76567 **\n",
            "  ** New min smooth loss: 1.6076 at iter 76568 **\n",
            "  ** New min smooth loss: 1.6075 at iter 76569 **\n",
            "  ** New min smooth loss: 1.6071 at iter 76570 **\n",
            "  ** New min smooth loss: 1.6067 at iter 76573 **\n",
            "  ** New min smooth loss: 1.6065 at iter 76574 **\n",
            "  ** New min smooth loss: 1.6064 at iter 76575 **\n",
            "Iter: 76600/100000, Smooth Loss: 1.6087, Min Smooth Loss: 1.6064 (at iter 76575), Time: 378.73s\n",
            "  ** New min smooth loss: 1.6061 at iter 76643 **\n",
            "  ** New min smooth loss: 1.6061 at iter 76646 **\n",
            "  ** New min smooth loss: 1.6058 at iter 76647 **\n",
            "  ** New min smooth loss: 1.6056 at iter 76681 **\n",
            "  ** New min smooth loss: 1.6055 at iter 76685 **\n",
            "Iter: 76700/100000, Smooth Loss: 1.6072, Min Smooth Loss: 1.6055 (at iter 76685), Time: 379.21s\n",
            "  ** New min smooth loss: 1.6054 at iter 76745 **\n",
            "  ** New min smooth loss: 1.6051 at iter 76746 **\n",
            "  ** New min smooth loss: 1.6048 at iter 76747 **\n",
            "  ** New min smooth loss: 1.6047 at iter 76748 **\n",
            "Iter: 76800/100000, Smooth Loss: 1.6084, Min Smooth Loss: 1.6047 (at iter 76748), Time: 379.65s\n",
            "  ** New min smooth loss: 1.6045 at iter 76830 **\n",
            "  ** New min smooth loss: 1.6044 at iter 76831 **\n",
            "  ** New min smooth loss: 1.6040 at iter 76832 **\n",
            "  ** New min smooth loss: 1.6038 at iter 76833 **\n",
            "  ** New min smooth loss: 1.6038 at iter 76837 **\n",
            "  ** New min smooth loss: 1.6036 at iter 76838 **\n",
            "  ** New min smooth loss: 1.6036 at iter 76840 **\n",
            "  ** New min smooth loss: 1.6034 at iter 76841 **\n",
            "  ** New min smooth loss: 1.6033 at iter 76842 **\n",
            "Iter: 76900/100000, Smooth Loss: 1.6064, Min Smooth Loss: 1.6033 (at iter 76842), Time: 380.11s\n",
            "Iter: 77000/100000, Smooth Loss: 1.6133, Min Smooth Loss: 1.6033 (at iter 76842), Time: 380.53s\n",
            "Iter: 77100/100000, Smooth Loss: 1.6261, Min Smooth Loss: 1.6033 (at iter 76842), Time: 380.96s\n",
            "Iter: 77200/100000, Smooth Loss: 1.6287, Min Smooth Loss: 1.6033 (at iter 76842), Time: 381.39s\n",
            "Iter: 77300/100000, Smooth Loss: 1.6301, Min Smooth Loss: 1.6033 (at iter 76842), Time: 381.81s\n",
            "Iter: 77400/100000, Smooth Loss: 1.6340, Min Smooth Loss: 1.6033 (at iter 76842), Time: 382.30s\n",
            "Iter: 77500/100000, Smooth Loss: 1.6344, Min Smooth Loss: 1.6033 (at iter 76842), Time: 382.73s\n",
            "Iter: 77600/100000, Smooth Loss: 1.6322, Min Smooth Loss: 1.6033 (at iter 76842), Time: 383.15s\n",
            "Iter: 77700/100000, Smooth Loss: 1.6278, Min Smooth Loss: 1.6033 (at iter 76842), Time: 383.58s\n",
            "Iter: 77800/100000, Smooth Loss: 1.6215, Min Smooth Loss: 1.6033 (at iter 76842), Time: 384.01s\n",
            "Iter: 77900/100000, Smooth Loss: 1.6093, Min Smooth Loss: 1.6033 (at iter 76842), Time: 384.47s\n",
            "  ** New min smooth loss: 1.6033 at iter 77990 **\n",
            "  ** New min smooth loss: 1.6033 at iter 77992 **\n",
            "  ** New min smooth loss: 1.6027 at iter 77993 **\n",
            "  ** New min smooth loss: 1.6023 at iter 77994 **\n",
            "  ** New min smooth loss: 1.6019 at iter 77998 **\n",
            "  ** New min smooth loss: 1.6016 at iter 77999 **\n",
            "  ** New min smooth loss: 1.6013 at iter 78000 **\n",
            "Iter: 78000/100000, Smooth Loss: 1.6013, Min Smooth Loss: 1.6013 (at iter 78000), Time: 384.89s\n",
            "  ** New min smooth loss: 1.6012 at iter 78002 **\n",
            "  ** New min smooth loss: 1.6011 at iter 78003 **\n",
            "  ** New min smooth loss: 1.6009 at iter 78004 **\n",
            "  ** New min smooth loss: 1.6007 at iter 78007 **\n",
            "  ** New min smooth loss: 1.6006 at iter 78008 **\n",
            "  ** New min smooth loss: 1.5999 at iter 78009 **\n",
            "  ** New min smooth loss: 1.5995 at iter 78011 **\n",
            "  ** New min smooth loss: 1.5994 at iter 78016 **\n",
            "  ** New min smooth loss: 1.5994 at iter 78017 **\n",
            "  ** New min smooth loss: 1.5993 at iter 78019 **\n",
            "  ** New min smooth loss: 1.5992 at iter 78020 **\n",
            "  ** New min smooth loss: 1.5991 at iter 78021 **\n",
            "  ** New min smooth loss: 1.5991 at iter 78022 **\n",
            "  ** New min smooth loss: 1.5983 at iter 78023 **\n",
            "  ** New min smooth loss: 1.5982 at iter 78024 **\n",
            "  ** New min smooth loss: 1.5982 at iter 78027 **\n",
            "  ** New min smooth loss: 1.5977 at iter 78029 **\n",
            "  ** New min smooth loss: 1.5974 at iter 78030 **\n",
            "  ** New min smooth loss: 1.5968 at iter 78031 **\n",
            "  ** New min smooth loss: 1.5966 at iter 78032 **\n",
            "  ** New min smooth loss: 1.5964 at iter 78034 **\n",
            "  ** New min smooth loss: 1.5959 at iter 78035 **\n",
            "  ** New min smooth loss: 1.5955 at iter 78036 **\n",
            "  ** New min smooth loss: 1.5947 at iter 78037 **\n",
            "  ** New min smooth loss: 1.5945 at iter 78038 **\n",
            "  ** New min smooth loss: 1.5941 at iter 78040 **\n",
            "  ** New min smooth loss: 1.5936 at iter 78041 **\n",
            "  ** New min smooth loss: 1.5929 at iter 78042 **\n",
            "  ** New min smooth loss: 1.5923 at iter 78043 **\n",
            "  ** New min smooth loss: 1.5920 at iter 78044 **\n",
            "  ** New min smooth loss: 1.5919 at iter 78045 **\n",
            "  ** New min smooth loss: 1.5916 at iter 78046 **\n",
            "  ** New min smooth loss: 1.5915 at iter 78047 **\n",
            "  ** New min smooth loss: 1.5914 at iter 78048 **\n",
            "  ** New min smooth loss: 1.5911 at iter 78049 **\n",
            "  ** New min smooth loss: 1.5911 at iter 78050 **\n",
            "  ** New min smooth loss: 1.5906 at iter 78052 **\n",
            "  ** New min smooth loss: 1.5897 at iter 78053 **\n",
            "  ** New min smooth loss: 1.5896 at iter 78054 **\n",
            "  ** New min smooth loss: 1.5893 at iter 78055 **\n",
            "  ** New min smooth loss: 1.5889 at iter 78056 **\n",
            "  ** New min smooth loss: 1.5887 at iter 78057 **\n",
            "  ** New min smooth loss: 1.5885 at iter 78084 **\n",
            "  ** New min smooth loss: 1.5883 at iter 78089 **\n",
            "  ** New min smooth loss: 1.5879 at iter 78090 **\n",
            "  ** New min smooth loss: 1.5874 at iter 78094 **\n",
            "  ** New min smooth loss: 1.5869 at iter 78095 **\n",
            "  ** New min smooth loss: 1.5865 at iter 78096 **\n",
            "Iter: 78100/100000, Smooth Loss: 1.5867, Min Smooth Loss: 1.5865 (at iter 78096), Time: 385.34s\n",
            "  ** New min smooth loss: 1.5865 at iter 78103 **\n",
            "  ** New min smooth loss: 1.5863 at iter 78105 **\n",
            "  ** New min smooth loss: 1.5858 at iter 78106 **\n",
            "  ** New min smooth loss: 1.5854 at iter 78112 **\n",
            "  ** New min smooth loss: 1.5851 at iter 78113 **\n",
            "  ** New min smooth loss: 1.5846 at iter 78117 **\n",
            "  ** New min smooth loss: 1.5844 at iter 78118 **\n",
            "  ** New min smooth loss: 1.5840 at iter 78119 **\n",
            "  ** New min smooth loss: 1.5836 at iter 78131 **\n",
            "  ** New min smooth loss: 1.5835 at iter 78132 **\n",
            "  ** New min smooth loss: 1.5827 at iter 78133 **\n",
            "  ** New min smooth loss: 1.5826 at iter 78136 **\n",
            "  ** New min smooth loss: 1.5825 at iter 78137 **\n",
            "  ** New min smooth loss: 1.5823 at iter 78138 **\n",
            "  ** New min smooth loss: 1.5823 at iter 78139 **\n",
            "  ** New min smooth loss: 1.5820 at iter 78140 **\n",
            "  ** New min smooth loss: 1.5816 at iter 78141 **\n",
            "  ** New min smooth loss: 1.5814 at iter 78147 **\n",
            "  ** New min smooth loss: 1.5812 at iter 78148 **\n",
            "  ** New min smooth loss: 1.5810 at iter 78152 **\n",
            "  ** New min smooth loss: 1.5805 at iter 78153 **\n",
            "  ** New min smooth loss: 1.5805 at iter 78155 **\n",
            "  ** New min smooth loss: 1.5800 at iter 78158 **\n",
            "  ** New min smooth loss: 1.5793 at iter 78159 **\n",
            "  ** New min smooth loss: 1.5789 at iter 78160 **\n",
            "  ** New min smooth loss: 1.5788 at iter 78161 **\n",
            "  ** New min smooth loss: 1.5787 at iter 78162 **\n",
            "  ** New min smooth loss: 1.5780 at iter 78163 **\n",
            "  ** New min smooth loss: 1.5777 at iter 78166 **\n",
            "  ** New min smooth loss: 1.5776 at iter 78168 **\n",
            "  ** New min smooth loss: 1.5774 at iter 78169 **\n",
            "  ** New min smooth loss: 1.5771 at iter 78171 **\n",
            "  ** New min smooth loss: 1.5769 at iter 78173 **\n",
            "  ** New min smooth loss: 1.5768 at iter 78175 **\n",
            "Iter: 78200/100000, Smooth Loss: 1.5790, Min Smooth Loss: 1.5768 (at iter 78175), Time: 385.78s\n",
            "Iter: 78300/100000, Smooth Loss: 1.5839, Min Smooth Loss: 1.5768 (at iter 78175), Time: 386.20s\n",
            "  ** New min smooth loss: 1.5767 at iter 78374 **\n",
            "  ** New min smooth loss: 1.5765 at iter 78375 **\n",
            "  ** New min smooth loss: 1.5765 at iter 78376 **\n",
            "  ** New min smooth loss: 1.5762 at iter 78378 **\n",
            "  ** New min smooth loss: 1.5762 at iter 78379 **\n",
            "  ** New min smooth loss: 1.5760 at iter 78380 **\n",
            "  ** New min smooth loss: 1.5756 at iter 78381 **\n",
            "  ** New min smooth loss: 1.5756 at iter 78387 **\n",
            "  ** New min smooth loss: 1.5753 at iter 78394 **\n",
            "  ** New min smooth loss: 1.5748 at iter 78395 **\n",
            "  ** New min smooth loss: 1.5746 at iter 78396 **\n",
            "  ** New min smooth loss: 1.5743 at iter 78397 **\n",
            "  ** New min smooth loss: 1.5743 at iter 78398 **\n",
            "  ** New min smooth loss: 1.5742 at iter 78399 **\n",
            "  ** New min smooth loss: 1.5736 at iter 78400 **\n",
            "Iter: 78400/100000, Smooth Loss: 1.5736, Min Smooth Loss: 1.5736 (at iter 78400), Time: 386.64s\n",
            "  ** New min smooth loss: 1.5730 at iter 78401 **\n",
            "  ** New min smooth loss: 1.5725 at iter 78402 **\n",
            "  ** New min smooth loss: 1.5723 at iter 78403 **\n",
            "  ** New min smooth loss: 1.5718 at iter 78404 **\n",
            "  ** New min smooth loss: 1.5715 at iter 78405 **\n",
            "  ** New min smooth loss: 1.5713 at iter 78406 **\n",
            "  ** New min smooth loss: 1.5709 at iter 78432 **\n",
            "  ** New min smooth loss: 1.5708 at iter 78433 **\n",
            "  ** New min smooth loss: 1.5703 at iter 78434 **\n",
            "  ** New min smooth loss: 1.5696 at iter 78435 **\n",
            "  ** New min smooth loss: 1.5689 at iter 78436 **\n",
            "  ** New min smooth loss: 1.5688 at iter 78437 **\n",
            "  ** New min smooth loss: 1.5684 at iter 78438 **\n",
            "  ** New min smooth loss: 1.5682 at iter 78443 **\n",
            "  ** New min smooth loss: 1.5682 at iter 78444 **\n",
            "  ** New min smooth loss: 1.5680 at iter 78449 **\n",
            "Iter: 78500/100000, Smooth Loss: 1.5687, Min Smooth Loss: 1.5680 (at iter 78449), Time: 387.07s\n",
            "  ** New min smooth loss: 1.5678 at iter 78503 **\n",
            "  ** New min smooth loss: 1.5677 at iter 78510 **\n",
            "  ** New min smooth loss: 1.5674 at iter 78512 **\n",
            "  ** New min smooth loss: 1.5672 at iter 78513 **\n",
            "  ** New min smooth loss: 1.5668 at iter 78514 **\n",
            "  ** New min smooth loss: 1.5665 at iter 78515 **\n",
            "  ** New min smooth loss: 1.5659 at iter 78516 **\n",
            "  ** New min smooth loss: 1.5653 at iter 78533 **\n",
            "  ** New min smooth loss: 1.5650 at iter 78534 **\n",
            "  ** New min smooth loss: 1.5647 at iter 78535 **\n",
            "  ** New min smooth loss: 1.5641 at iter 78536 **\n",
            "  ** New min smooth loss: 1.5639 at iter 78537 **\n",
            "  ** New min smooth loss: 1.5638 at iter 78538 **\n",
            "  ** New min smooth loss: 1.5632 at iter 78539 **\n",
            "  ** New min smooth loss: 1.5626 at iter 78540 **\n",
            "  ** New min smooth loss: 1.5623 at iter 78541 **\n",
            "  ** New min smooth loss: 1.5620 at iter 78543 **\n",
            "  ** New min smooth loss: 1.5619 at iter 78544 **\n",
            "  ** New min smooth loss: 1.5616 at iter 78545 **\n",
            "  ** New min smooth loss: 1.5611 at iter 78546 **\n",
            "  ** New min smooth loss: 1.5607 at iter 78547 **\n",
            "  ** New min smooth loss: 1.5606 at iter 78555 **\n",
            "  ** New min smooth loss: 1.5606 at iter 78556 **\n",
            "  ** New min smooth loss: 1.5606 at iter 78557 **\n",
            "  ** New min smooth loss: 1.5601 at iter 78559 **\n",
            "  ** New min smooth loss: 1.5597 at iter 78560 **\n",
            "  ** New min smooth loss: 1.5597 at iter 78567 **\n",
            "  ** New min smooth loss: 1.5594 at iter 78572 **\n",
            "  ** New min smooth loss: 1.5590 at iter 78573 **\n",
            "  ** New min smooth loss: 1.5582 at iter 78575 **\n",
            "  ** New min smooth loss: 1.5580 at iter 78580 **\n",
            "  ** New min smooth loss: 1.5577 at iter 78581 **\n",
            "  ** New min smooth loss: 1.5577 at iter 78598 **\n",
            "  ** New min smooth loss: 1.5576 at iter 78599 **\n",
            "  ** New min smooth loss: 1.5571 at iter 78600 **\n",
            "Iter: 78600/100000, Smooth Loss: 1.5571, Min Smooth Loss: 1.5571 (at iter 78600), Time: 387.65s\n",
            "  ** New min smooth loss: 1.5568 at iter 78653 **\n",
            "  ** New min smooth loss: 1.5568 at iter 78654 **\n",
            "  ** New min smooth loss: 1.5560 at iter 78655 **\n",
            "  ** New min smooth loss: 1.5557 at iter 78662 **\n",
            "  ** New min smooth loss: 1.5556 at iter 78672 **\n",
            "  ** New min smooth loss: 1.5555 at iter 78674 **\n",
            "  ** New min smooth loss: 1.5552 at iter 78675 **\n",
            "  ** New min smooth loss: 1.5551 at iter 78679 **\n",
            "  ** New min smooth loss: 1.5549 at iter 78685 **\n",
            "  ** New min smooth loss: 1.5548 at iter 78688 **\n",
            "  ** New min smooth loss: 1.5545 at iter 78689 **\n",
            "  ** New min smooth loss: 1.5543 at iter 78694 **\n",
            "  ** New min smooth loss: 1.5537 at iter 78695 **\n",
            "  ** New min smooth loss: 1.5536 at iter 78696 **\n",
            "Iter: 78700/100000, Smooth Loss: 1.5542, Min Smooth Loss: 1.5536 (at iter 78696), Time: 388.33s\n",
            "  ** New min smooth loss: 1.5536 at iter 78705 **\n",
            "  ** New min smooth loss: 1.5536 at iter 78723 **\n",
            "  ** New min smooth loss: 1.5531 at iter 78724 **\n",
            "  ** New min smooth loss: 1.5529 at iter 78729 **\n",
            "  ** New min smooth loss: 1.5522 at iter 78730 **\n",
            "  ** New min smooth loss: 1.5520 at iter 78731 **\n",
            "Iter: 78800/100000, Smooth Loss: 1.5555, Min Smooth Loss: 1.5520 (at iter 78731), Time: 389.00s\n",
            "  ** New min smooth loss: 1.5520 at iter 78868 **\n",
            "  ** New min smooth loss: 1.5519 at iter 78871 **\n",
            "  ** New min smooth loss: 1.5517 at iter 78872 **\n",
            "  ** New min smooth loss: 1.5515 at iter 78873 **\n",
            "  ** New min smooth loss: 1.5513 at iter 78874 **\n",
            "  ** New min smooth loss: 1.5511 at iter 78875 **\n",
            "  ** New min smooth loss: 1.5510 at iter 78877 **\n",
            "  ** New min smooth loss: 1.5509 at iter 78878 **\n",
            "  ** New min smooth loss: 1.5507 at iter 78882 **\n",
            "Iter: 78900/100000, Smooth Loss: 1.5543, Min Smooth Loss: 1.5507 (at iter 78882), Time: 389.70s\n",
            "Iter: 79000/100000, Smooth Loss: 1.5654, Min Smooth Loss: 1.5507 (at iter 78882), Time: 390.39s\n",
            "Iter: 79100/100000, Smooth Loss: 1.5772, Min Smooth Loss: 1.5507 (at iter 78882), Time: 391.06s\n",
            "Iter: 79200/100000, Smooth Loss: 1.5717, Min Smooth Loss: 1.5507 (at iter 78882), Time: 391.73s\n",
            "Iter: 79300/100000, Smooth Loss: 1.5914, Min Smooth Loss: 1.5507 (at iter 78882), Time: 392.22s\n",
            "Iter: 79400/100000, Smooth Loss: 1.5913, Min Smooth Loss: 1.5507 (at iter 78882), Time: 392.63s\n",
            "Iter: 79500/100000, Smooth Loss: 1.5916, Min Smooth Loss: 1.5507 (at iter 78882), Time: 393.08s\n",
            "Iter: 79600/100000, Smooth Loss: 1.5860, Min Smooth Loss: 1.5507 (at iter 78882), Time: 393.49s\n",
            "Iter: 79700/100000, Smooth Loss: 1.5777, Min Smooth Loss: 1.5507 (at iter 78882), Time: 393.93s\n",
            "Iter: 79800/100000, Smooth Loss: 1.5786, Min Smooth Loss: 1.5507 (at iter 78882), Time: 394.36s\n",
            "Iter: 79900/100000, Smooth Loss: 1.5848, Min Smooth Loss: 1.5507 (at iter 78882), Time: 394.77s\n",
            "Iter: 80000/100000, Smooth Loss: 1.5867, Min Smooth Loss: 1.5507 (at iter 78882), Time: 395.21s\n",
            "--- Synthesized text at iter 80000 ---\n",
            " Veement he ham apound the would at are and wlowing his have who whe maded, and alonged threr's geing to said of his spotcer; quinting the ropestrrowing he was see way at an I fall, Krifther fows into\n",
            "---\n",
            "Iter: 80100/100000, Smooth Loss: 1.5891, Min Smooth Loss: 1.5507 (at iter 78882), Time: 395.65s\n",
            "Iter: 80200/100000, Smooth Loss: 1.5870, Min Smooth Loss: 1.5507 (at iter 78882), Time: 396.10s\n",
            "Iter: 80300/100000, Smooth Loss: 1.5942, Min Smooth Loss: 1.5507 (at iter 78882), Time: 396.51s\n",
            "Iter: 80400/100000, Smooth Loss: 1.5881, Min Smooth Loss: 1.5507 (at iter 78882), Time: 396.94s\n",
            "Iter: 80500/100000, Smooth Loss: 1.5909, Min Smooth Loss: 1.5507 (at iter 78882), Time: 397.38s\n",
            "Iter: 80600/100000, Smooth Loss: 1.5809, Min Smooth Loss: 1.5507 (at iter 78882), Time: 397.79s\n",
            "Iter: 80700/100000, Smooth Loss: 1.5762, Min Smooth Loss: 1.5507 (at iter 78882), Time: 398.23s\n",
            "Iter: 80800/100000, Smooth Loss: 1.5652, Min Smooth Loss: 1.5507 (at iter 78882), Time: 398.66s\n",
            "Iter: 80900/100000, Smooth Loss: 1.5613, Min Smooth Loss: 1.5507 (at iter 78882), Time: 399.09s\n",
            "Iter: 81000/100000, Smooth Loss: 1.5656, Min Smooth Loss: 1.5507 (at iter 78882), Time: 399.52s\n",
            "Iter: 81100/100000, Smooth Loss: 1.5733, Min Smooth Loss: 1.5507 (at iter 78882), Time: 399.95s\n",
            "Iter: 81200/100000, Smooth Loss: 1.5872, Min Smooth Loss: 1.5507 (at iter 78882), Time: 400.39s\n",
            "Iter: 81300/100000, Smooth Loss: 1.6024, Min Smooth Loss: 1.5507 (at iter 78882), Time: 400.81s\n",
            "Iter: 81400/100000, Smooth Loss: 1.6026, Min Smooth Loss: 1.5507 (at iter 78882), Time: 401.23s\n",
            "Iter: 81500/100000, Smooth Loss: 1.5995, Min Smooth Loss: 1.5507 (at iter 78882), Time: 401.67s\n",
            "Iter: 81600/100000, Smooth Loss: 1.5914, Min Smooth Loss: 1.5507 (at iter 78882), Time: 402.25s\n",
            "Iter: 81700/100000, Smooth Loss: 1.5864, Min Smooth Loss: 1.5507 (at iter 78882), Time: 402.91s\n",
            "Iter: 81800/100000, Smooth Loss: 1.5884, Min Smooth Loss: 1.5507 (at iter 78882), Time: 403.55s\n",
            "Iter: 81900/100000, Smooth Loss: 1.5870, Min Smooth Loss: 1.5507 (at iter 78882), Time: 404.18s\n",
            "Iter: 82000/100000, Smooth Loss: 1.5937, Min Smooth Loss: 1.5507 (at iter 78882), Time: 404.82s\n",
            "Iter: 82100/100000, Smooth Loss: 1.5984, Min Smooth Loss: 1.5507 (at iter 78882), Time: 405.45s\n",
            "Iter: 82200/100000, Smooth Loss: 1.5924, Min Smooth Loss: 1.5507 (at iter 78882), Time: 406.14s\n",
            "Iter: 82300/100000, Smooth Loss: 1.6069, Min Smooth Loss: 1.5507 (at iter 78882), Time: 406.70s\n",
            "Iter: 82400/100000, Smooth Loss: 1.6111, Min Smooth Loss: 1.5507 (at iter 78882), Time: 407.11s\n",
            "Iter: 82500/100000, Smooth Loss: 1.6096, Min Smooth Loss: 1.5507 (at iter 78882), Time: 407.52s\n",
            "Iter: 82600/100000, Smooth Loss: 1.5977, Min Smooth Loss: 1.5507 (at iter 78882), Time: 407.97s\n",
            "Iter: 82700/100000, Smooth Loss: 1.5931, Min Smooth Loss: 1.5507 (at iter 78882), Time: 408.38s\n",
            "Iter: 82800/100000, Smooth Loss: 1.5911, Min Smooth Loss: 1.5507 (at iter 78882), Time: 408.83s\n",
            "Iter: 82900/100000, Smooth Loss: 1.5842, Min Smooth Loss: 1.5507 (at iter 78882), Time: 409.25s\n",
            "Iter: 83000/100000, Smooth Loss: 1.5825, Min Smooth Loss: 1.5507 (at iter 78882), Time: 409.68s\n",
            "Iter: 83100/100000, Smooth Loss: 1.5830, Min Smooth Loss: 1.5507 (at iter 78882), Time: 410.12s\n",
            "Iter: 83200/100000, Smooth Loss: 1.5871, Min Smooth Loss: 1.5507 (at iter 78882), Time: 410.53s\n",
            "Iter: 83300/100000, Smooth Loss: 1.5939, Min Smooth Loss: 1.5507 (at iter 78882), Time: 410.97s\n",
            "Iter: 83400/100000, Smooth Loss: 1.5984, Min Smooth Loss: 1.5507 (at iter 78882), Time: 411.38s\n",
            "Iter: 83500/100000, Smooth Loss: 1.6033, Min Smooth Loss: 1.5507 (at iter 78882), Time: 411.80s\n",
            "Iter: 83600/100000, Smooth Loss: 1.6023, Min Smooth Loss: 1.5507 (at iter 78882), Time: 412.27s\n",
            "Iter: 83700/100000, Smooth Loss: 1.6082, Min Smooth Loss: 1.5507 (at iter 78882), Time: 412.69s\n",
            "Iter: 83800/100000, Smooth Loss: 1.6073, Min Smooth Loss: 1.5507 (at iter 78882), Time: 413.14s\n",
            "Iter: 83900/100000, Smooth Loss: 1.6132, Min Smooth Loss: 1.5507 (at iter 78882), Time: 413.57s\n",
            "Iter: 84000/100000, Smooth Loss: 1.6039, Min Smooth Loss: 1.5507 (at iter 78882), Time: 414.02s\n",
            "Iter: 84100/100000, Smooth Loss: 1.5960, Min Smooth Loss: 1.5507 (at iter 78882), Time: 414.45s\n",
            "Iter: 84200/100000, Smooth Loss: 1.5892, Min Smooth Loss: 1.5507 (at iter 78882), Time: 414.88s\n",
            "Iter: 84300/100000, Smooth Loss: 1.5835, Min Smooth Loss: 1.5507 (at iter 78882), Time: 415.32s\n",
            "Iter: 84400/100000, Smooth Loss: 1.5741, Min Smooth Loss: 1.5507 (at iter 78882), Time: 415.74s\n",
            "Iter: 84500/100000, Smooth Loss: 1.5643, Min Smooth Loss: 1.5507 (at iter 78882), Time: 416.18s\n",
            "Iter: 84600/100000, Smooth Loss: 1.5562, Min Smooth Loss: 1.5507 (at iter 78882), Time: 416.71s\n",
            "Iter: 84700/100000, Smooth Loss: 1.5571, Min Smooth Loss: 1.5507 (at iter 78882), Time: 417.37s\n",
            "Iter: 84800/100000, Smooth Loss: 1.5524, Min Smooth Loss: 1.5507 (at iter 78882), Time: 418.00s\n",
            "  ** New min smooth loss: 1.5506 at iter 84875 **\n",
            "  ** New min smooth loss: 1.5500 at iter 84876 **\n",
            "  ** New min smooth loss: 1.5498 at iter 84883 **\n",
            "  ** New min smooth loss: 1.5496 at iter 84884 **\n",
            "  ** New min smooth loss: 1.5491 at iter 84885 **\n",
            "  ** New min smooth loss: 1.5490 at iter 84886 **\n",
            "  ** New min smooth loss: 1.5489 at iter 84887 **\n",
            "Iter: 84900/100000, Smooth Loss: 1.5519, Min Smooth Loss: 1.5489 (at iter 84887), Time: 418.66s\n",
            "  ** New min smooth loss: 1.5488 at iter 84927 **\n",
            "  ** New min smooth loss: 1.5485 at iter 84928 **\n",
            "  ** New min smooth loss: 1.5481 at iter 84929 **\n",
            "  ** New min smooth loss: 1.5479 at iter 84930 **\n",
            "  ** New min smooth loss: 1.5476 at iter 84933 **\n",
            "  ** New min smooth loss: 1.5474 at iter 84934 **\n",
            "  ** New min smooth loss: 1.5469 at iter 84935 **\n",
            "  ** New min smooth loss: 1.5466 at iter 84936 **\n",
            "  ** New min smooth loss: 1.5463 at iter 84938 **\n",
            "  ** New min smooth loss: 1.5460 at iter 84939 **\n",
            "  ** New min smooth loss: 1.5459 at iter 84943 **\n",
            "  ** New min smooth loss: 1.5457 at iter 84944 **\n",
            "  ** New min smooth loss: 1.5456 at iter 84945 **\n",
            "  ** New min smooth loss: 1.5449 at iter 84946 **\n",
            "  ** New min smooth loss: 1.5447 at iter 84947 **\n",
            "  ** New min smooth loss: 1.5444 at iter 84948 **\n",
            "  ** New min smooth loss: 1.5442 at iter 84970 **\n",
            "  ** New min smooth loss: 1.5441 at iter 84971 **\n",
            "  ** New min smooth loss: 1.5440 at iter 84972 **\n",
            "  ** New min smooth loss: 1.5438 at iter 84974 **\n",
            "  ** New min smooth loss: 1.5437 at iter 84976 **\n",
            "  ** New min smooth loss: 1.5434 at iter 84977 **\n",
            "  ** New min smooth loss: 1.5433 at iter 84979 **\n",
            "  ** New min smooth loss: 1.5429 at iter 84980 **\n",
            "  ** New min smooth loss: 1.5427 at iter 84981 **\n",
            "  ** New min smooth loss: 1.5424 at iter 84982 **\n",
            "  ** New min smooth loss: 1.5416 at iter 84983 **\n",
            "  ** New min smooth loss: 1.5415 at iter 84987 **\n",
            "  ** New min smooth loss: 1.5409 at iter 84988 **\n",
            "  ** New min smooth loss: 1.5406 at iter 84991 **\n",
            "  ** New min smooth loss: 1.5405 at iter 84992 **\n",
            "Iter: 85000/100000, Smooth Loss: 1.5427, Min Smooth Loss: 1.5405 (at iter 84992), Time: 419.29s\n",
            "Iter: 85100/100000, Smooth Loss: 1.5488, Min Smooth Loss: 1.5405 (at iter 84992), Time: 419.93s\n",
            "Iter: 85200/100000, Smooth Loss: 1.5514, Min Smooth Loss: 1.5405 (at iter 84992), Time: 420.60s\n",
            "Iter: 85300/100000, Smooth Loss: 1.5468, Min Smooth Loss: 1.5405 (at iter 84992), Time: 421.20s\n",
            "Iter: 85400/100000, Smooth Loss: 1.5529, Min Smooth Loss: 1.5405 (at iter 84992), Time: 421.65s\n",
            "Iter: 85500/100000, Smooth Loss: 1.5546, Min Smooth Loss: 1.5405 (at iter 84992), Time: 422.08s\n",
            "Iter: 85600/100000, Smooth Loss: 1.5621, Min Smooth Loss: 1.5405 (at iter 84992), Time: 422.50s\n",
            "Iter: 85700/100000, Smooth Loss: 1.5621, Min Smooth Loss: 1.5405 (at iter 84992), Time: 422.96s\n",
            "Iter: 85800/100000, Smooth Loss: 1.5558, Min Smooth Loss: 1.5405 (at iter 84992), Time: 423.39s\n",
            "Iter: 85900/100000, Smooth Loss: 1.5560, Min Smooth Loss: 1.5405 (at iter 84992), Time: 423.84s\n",
            "Iter: 86000/100000, Smooth Loss: 1.5422, Min Smooth Loss: 1.5405 (at iter 84992), Time: 424.27s\n",
            "  ** New min smooth loss: 1.5404 at iter 86005 **\n",
            "  ** New min smooth loss: 1.5399 at iter 86006 **\n",
            "  ** New min smooth loss: 1.5395 at iter 86007 **\n",
            "  ** New min smooth loss: 1.5389 at iter 86008 **\n",
            "  ** New min smooth loss: 1.5386 at iter 86009 **\n",
            "  ** New min smooth loss: 1.5380 at iter 86010 **\n",
            "  ** New min smooth loss: 1.5379 at iter 86011 **\n",
            "  ** New min smooth loss: 1.5376 at iter 86012 **\n",
            "Iter: 86100/100000, Smooth Loss: 1.5414, Min Smooth Loss: 1.5376 (at iter 86012), Time: 424.72s\n",
            "  ** New min smooth loss: 1.5375 at iter 86154 **\n",
            "Iter: 86200/100000, Smooth Loss: 1.5393, Min Smooth Loss: 1.5375 (at iter 86154), Time: 425.15s\n",
            "  ** New min smooth loss: 1.5370 at iter 86268 **\n",
            "  ** New min smooth loss: 1.5366 at iter 86269 **\n",
            "  ** New min smooth loss: 1.5366 at iter 86270 **\n",
            "  ** New min smooth loss: 1.5362 at iter 86271 **\n",
            "  ** New min smooth loss: 1.5361 at iter 86272 **\n",
            "  ** New min smooth loss: 1.5359 at iter 86275 **\n",
            "  ** New min smooth loss: 1.5357 at iter 86278 **\n",
            "  ** New min smooth loss: 1.5355 at iter 86285 **\n",
            "  ** New min smooth loss: 1.5354 at iter 86286 **\n",
            "  ** New min smooth loss: 1.5352 at iter 86289 **\n",
            "  ** New min smooth loss: 1.5346 at iter 86291 **\n",
            "  ** New min smooth loss: 1.5343 at iter 86292 **\n",
            "  ** New min smooth loss: 1.5343 at iter 86293 **\n",
            "  ** New min smooth loss: 1.5341 at iter 86294 **\n",
            "  ** New min smooth loss: 1.5339 at iter 86296 **\n",
            "Iter: 86300/100000, Smooth Loss: 1.5347, Min Smooth Loss: 1.5339 (at iter 86296), Time: 425.58s\n",
            "  ** New min smooth loss: 1.5339 at iter 86303 **\n",
            "  ** New min smooth loss: 1.5335 at iter 86304 **\n",
            "  ** New min smooth loss: 1.5332 at iter 86305 **\n",
            "  ** New min smooth loss: 1.5328 at iter 86306 **\n",
            "  ** New min smooth loss: 1.5324 at iter 86307 **\n",
            "  ** New min smooth loss: 1.5317 at iter 86308 **\n",
            "  ** New min smooth loss: 1.5317 at iter 86311 **\n",
            "  ** New min smooth loss: 1.5314 at iter 86312 **\n",
            "  ** New min smooth loss: 1.5312 at iter 86313 **\n",
            "  ** New min smooth loss: 1.5311 at iter 86317 **\n",
            "  ** New min smooth loss: 1.5307 at iter 86318 **\n",
            "  ** New min smooth loss: 1.5304 at iter 86319 **\n",
            "  ** New min smooth loss: 1.5304 at iter 86320 **\n",
            "  ** New min smooth loss: 1.5301 at iter 86323 **\n",
            "  ** New min smooth loss: 1.5300 at iter 86324 **\n",
            "  ** New min smooth loss: 1.5297 at iter 86326 **\n",
            "  ** New min smooth loss: 1.5290 at iter 86327 **\n",
            "  ** New min smooth loss: 1.5286 at iter 86328 **\n",
            "  ** New min smooth loss: 1.5280 at iter 86329 **\n",
            "  ** New min smooth loss: 1.5280 at iter 86330 **\n",
            "Iter: 86400/100000, Smooth Loss: 1.5309, Min Smooth Loss: 1.5280 (at iter 86330), Time: 426.04s\n",
            "  ** New min smooth loss: 1.5277 at iter 86435 **\n",
            "Iter: 86500/100000, Smooth Loss: 1.5285, Min Smooth Loss: 1.5277 (at iter 86435), Time: 426.47s\n",
            "  ** New min smooth loss: 1.5275 at iter 86509 **\n",
            "  ** New min smooth loss: 1.5267 at iter 86546 **\n",
            "  ** New min smooth loss: 1.5264 at iter 86547 **\n",
            "Iter: 86600/100000, Smooth Loss: 1.5283, Min Smooth Loss: 1.5264 (at iter 86547), Time: 426.95s\n",
            "  ** New min smooth loss: 1.5261 at iter 86668 **\n",
            "Iter: 86700/100000, Smooth Loss: 1.5291, Min Smooth Loss: 1.5261 (at iter 86668), Time: 427.37s\n",
            "Iter: 86800/100000, Smooth Loss: 1.5343, Min Smooth Loss: 1.5261 (at iter 86668), Time: 427.78s\n",
            "Iter: 86900/100000, Smooth Loss: 1.5335, Min Smooth Loss: 1.5261 (at iter 86668), Time: 428.22s\n",
            "Iter: 87000/100000, Smooth Loss: 1.5438, Min Smooth Loss: 1.5261 (at iter 86668), Time: 428.64s\n",
            "Iter: 87100/100000, Smooth Loss: 1.5489, Min Smooth Loss: 1.5261 (at iter 86668), Time: 429.10s\n",
            "Iter: 87200/100000, Smooth Loss: 1.5498, Min Smooth Loss: 1.5261 (at iter 86668), Time: 429.52s\n",
            "Iter: 87300/100000, Smooth Loss: 1.5471, Min Smooth Loss: 1.5261 (at iter 86668), Time: 429.98s\n",
            "Iter: 87400/100000, Smooth Loss: 1.5445, Min Smooth Loss: 1.5261 (at iter 86668), Time: 430.39s\n",
            "Iter: 87500/100000, Smooth Loss: 1.5407, Min Smooth Loss: 1.5261 (at iter 86668), Time: 430.81s\n",
            "Iter: 87600/100000, Smooth Loss: 1.5366, Min Smooth Loss: 1.5261 (at iter 86668), Time: 431.36s\n",
            "Iter: 87700/100000, Smooth Loss: 1.5512, Min Smooth Loss: 1.5261 (at iter 86668), Time: 432.00s\n",
            "Iter: 87800/100000, Smooth Loss: 1.5446, Min Smooth Loss: 1.5261 (at iter 86668), Time: 432.63s\n",
            "Iter: 87900/100000, Smooth Loss: 1.5439, Min Smooth Loss: 1.5261 (at iter 86668), Time: 433.29s\n",
            "Iter: 88000/100000, Smooth Loss: 1.5484, Min Smooth Loss: 1.5261 (at iter 86668), Time: 433.93s\n",
            "Iter: 88100/100000, Smooth Loss: 1.5580, Min Smooth Loss: 1.5261 (at iter 86668), Time: 434.56s\n",
            "Iter: 88200/100000, Smooth Loss: 1.5587, Min Smooth Loss: 1.5261 (at iter 86668), Time: 435.23s\n",
            "Iter: 88300/100000, Smooth Loss: 1.5558, Min Smooth Loss: 1.5261 (at iter 86668), Time: 435.78s\n",
            "Iter: 88400/100000, Smooth Loss: 1.5583, Min Smooth Loss: 1.5261 (at iter 86668), Time: 436.21s\n",
            "Iter: 88500/100000, Smooth Loss: 1.5625, Min Smooth Loss: 1.5261 (at iter 86668), Time: 436.66s\n",
            "Iter: 88600/100000, Smooth Loss: 1.5619, Min Smooth Loss: 1.5261 (at iter 86668), Time: 437.09s\n",
            "Iter: 88700/100000, Smooth Loss: 1.5882, Min Smooth Loss: 1.5261 (at iter 86668), Time: 437.53s\n",
            "Iter: 88800/100000, Smooth Loss: 1.5996, Min Smooth Loss: 1.5261 (at iter 86668), Time: 437.96s\n",
            "Iter: 88900/100000, Smooth Loss: 1.6050, Min Smooth Loss: 1.5261 (at iter 86668), Time: 438.37s\n",
            "Iter: 89000/100000, Smooth Loss: 1.6027, Min Smooth Loss: 1.5261 (at iter 86668), Time: 438.82s\n",
            "Iter: 89100/100000, Smooth Loss: 1.6059, Min Smooth Loss: 1.5261 (at iter 86668), Time: 439.26s\n",
            "Iter: 89200/100000, Smooth Loss: 1.6108, Min Smooth Loss: 1.5261 (at iter 86668), Time: 439.70s\n",
            "Iter: 89300/100000, Smooth Loss: 1.6087, Min Smooth Loss: 1.5261 (at iter 86668), Time: 440.13s\n",
            "Iter: 89400/100000, Smooth Loss: 1.6078, Min Smooth Loss: 1.5261 (at iter 86668), Time: 440.55s\n",
            "Iter: 89500/100000, Smooth Loss: 1.6028, Min Smooth Loss: 1.5261 (at iter 86668), Time: 441.00s\n",
            "Iter: 89600/100000, Smooth Loss: 1.6006, Min Smooth Loss: 1.5261 (at iter 86668), Time: 441.42s\n",
            "Iter: 89700/100000, Smooth Loss: 1.5990, Min Smooth Loss: 1.5261 (at iter 86668), Time: 441.85s\n",
            "Iter: 89800/100000, Smooth Loss: 1.5966, Min Smooth Loss: 1.5261 (at iter 86668), Time: 442.30s\n",
            "Iter: 89900/100000, Smooth Loss: 1.5917, Min Smooth Loss: 1.5261 (at iter 86668), Time: 442.74s\n",
            "Iter: 90000/100000, Smooth Loss: 1.5990, Min Smooth Loss: 1.5261 (at iter 86668), Time: 443.17s\n",
            "--- Synthesized text at iter 90000 ---\n",
            "ext outhentt to house eation.  And then?\"  said Harry kild, by intiheped the fiside.  Ix would fame plaruety wizardiment, at the dark know that would had been hop pothed with's like that one, looked i\n",
            "---\n",
            "Iter: 90100/100000, Smooth Loss: 1.5950, Min Smooth Loss: 1.5261 (at iter 86668), Time: 443.61s\n",
            "Iter: 90200/100000, Smooth Loss: 1.6131, Min Smooth Loss: 1.5261 (at iter 86668), Time: 444.06s\n",
            "Iter: 90300/100000, Smooth Loss: 1.6268, Min Smooth Loss: 1.5261 (at iter 86668), Time: 444.48s\n",
            "Iter: 90400/100000, Smooth Loss: 1.6284, Min Smooth Loss: 1.5261 (at iter 86668), Time: 444.91s\n",
            "Iter: 90500/100000, Smooth Loss: 1.6330, Min Smooth Loss: 1.5261 (at iter 86668), Time: 445.32s\n",
            "Iter: 90600/100000, Smooth Loss: 1.6254, Min Smooth Loss: 1.5261 (at iter 86668), Time: 445.90s\n",
            "Iter: 90700/100000, Smooth Loss: 1.6184, Min Smooth Loss: 1.5261 (at iter 86668), Time: 446.58s\n",
            "Iter: 90800/100000, Smooth Loss: 1.6158, Min Smooth Loss: 1.5261 (at iter 86668), Time: 447.24s\n",
            "Iter: 90900/100000, Smooth Loss: 1.6264, Min Smooth Loss: 1.5261 (at iter 86668), Time: 447.91s\n",
            "Iter: 91000/100000, Smooth Loss: 1.6242, Min Smooth Loss: 1.5261 (at iter 86668), Time: 448.57s\n",
            "Iter: 91100/100000, Smooth Loss: 1.6226, Min Smooth Loss: 1.5261 (at iter 86668), Time: 449.21s\n",
            "Iter: 91200/100000, Smooth Loss: 1.6277, Min Smooth Loss: 1.5261 (at iter 86668), Time: 449.89s\n",
            "Iter: 91300/100000, Smooth Loss: 1.6221, Min Smooth Loss: 1.5261 (at iter 86668), Time: 450.41s\n",
            "Iter: 91400/100000, Smooth Loss: 1.6128, Min Smooth Loss: 1.5261 (at iter 86668), Time: 450.84s\n",
            "Iter: 91500/100000, Smooth Loss: 1.6003, Min Smooth Loss: 1.5261 (at iter 86668), Time: 451.28s\n",
            "Iter: 91600/100000, Smooth Loss: 1.5960, Min Smooth Loss: 1.5261 (at iter 86668), Time: 451.71s\n",
            "Iter: 91700/100000, Smooth Loss: 1.6019, Min Smooth Loss: 1.5261 (at iter 86668), Time: 452.13s\n",
            "Iter: 91800/100000, Smooth Loss: 1.5954, Min Smooth Loss: 1.5261 (at iter 86668), Time: 452.56s\n",
            "Iter: 91900/100000, Smooth Loss: 1.6013, Min Smooth Loss: 1.5261 (at iter 86668), Time: 452.99s\n",
            "Iter: 92000/100000, Smooth Loss: 1.6082, Min Smooth Loss: 1.5261 (at iter 86668), Time: 453.42s\n",
            "Iter: 92100/100000, Smooth Loss: 1.6064, Min Smooth Loss: 1.5261 (at iter 86668), Time: 453.85s\n",
            "Iter: 92200/100000, Smooth Loss: 1.6084, Min Smooth Loss: 1.5261 (at iter 86668), Time: 454.26s\n",
            "Iter: 92300/100000, Smooth Loss: 1.6159, Min Smooth Loss: 1.5261 (at iter 86668), Time: 454.70s\n",
            "Iter: 92400/100000, Smooth Loss: 1.6223, Min Smooth Loss: 1.5261 (at iter 86668), Time: 455.11s\n",
            "Iter: 92500/100000, Smooth Loss: 1.6245, Min Smooth Loss: 1.5261 (at iter 86668), Time: 455.55s\n",
            "Iter: 92600/100000, Smooth Loss: 1.6221, Min Smooth Loss: 1.5261 (at iter 86668), Time: 455.96s\n",
            "Iter: 92700/100000, Smooth Loss: 1.6223, Min Smooth Loss: 1.5261 (at iter 86668), Time: 456.38s\n",
            "Iter: 92800/100000, Smooth Loss: 1.6238, Min Smooth Loss: 1.5261 (at iter 86668), Time: 456.81s\n",
            "Iter: 92900/100000, Smooth Loss: 1.6252, Min Smooth Loss: 1.5261 (at iter 86668), Time: 457.24s\n",
            "Iter: 93000/100000, Smooth Loss: 1.6273, Min Smooth Loss: 1.5261 (at iter 86668), Time: 457.67s\n",
            "Iter: 93100/100000, Smooth Loss: 1.6344, Min Smooth Loss: 1.5261 (at iter 86668), Time: 458.09s\n",
            "Iter: 93200/100000, Smooth Loss: 1.6290, Min Smooth Loss: 1.5261 (at iter 86668), Time: 458.51s\n",
            "Iter: 93300/100000, Smooth Loss: 1.6328, Min Smooth Loss: 1.5261 (at iter 86668), Time: 458.95s\n",
            "Iter: 93400/100000, Smooth Loss: 1.6336, Min Smooth Loss: 1.5261 (at iter 86668), Time: 459.37s\n",
            "Iter: 93500/100000, Smooth Loss: 1.6425, Min Smooth Loss: 1.5261 (at iter 86668), Time: 459.81s\n",
            "Iter: 93600/100000, Smooth Loss: 1.6414, Min Smooth Loss: 1.5261 (at iter 86668), Time: 460.34s\n",
            "Iter: 93700/100000, Smooth Loss: 1.6434, Min Smooth Loss: 1.5261 (at iter 86668), Time: 461.00s\n",
            "Iter: 93800/100000, Smooth Loss: 1.6524, Min Smooth Loss: 1.5261 (at iter 86668), Time: 461.60s\n",
            "Iter: 93900/100000, Smooth Loss: 1.6588, Min Smooth Loss: 1.5261 (at iter 86668), Time: 462.28s\n",
            "Iter: 94000/100000, Smooth Loss: 1.6614, Min Smooth Loss: 1.5261 (at iter 86668), Time: 462.94s\n",
            "Iter: 94100/100000, Smooth Loss: 1.6615, Min Smooth Loss: 1.5261 (at iter 86668), Time: 463.56s\n",
            "Iter: 94200/100000, Smooth Loss: 1.6686, Min Smooth Loss: 1.5261 (at iter 86668), Time: 464.23s\n",
            "Iter: 94300/100000, Smooth Loss: 1.6670, Min Smooth Loss: 1.5261 (at iter 86668), Time: 464.80s\n",
            "Iter: 94400/100000, Smooth Loss: 1.6755, Min Smooth Loss: 1.5261 (at iter 86668), Time: 465.24s\n",
            "Iter: 94500/100000, Smooth Loss: 1.6714, Min Smooth Loss: 1.5261 (at iter 86668), Time: 465.66s\n",
            "Iter: 94600/100000, Smooth Loss: 1.6682, Min Smooth Loss: 1.5261 (at iter 86668), Time: 466.10s\n",
            "Iter: 94700/100000, Smooth Loss: 1.6669, Min Smooth Loss: 1.5261 (at iter 86668), Time: 466.51s\n",
            "Iter: 94800/100000, Smooth Loss: 1.6741, Min Smooth Loss: 1.5261 (at iter 86668), Time: 466.94s\n",
            "Iter: 94900/100000, Smooth Loss: 1.6806, Min Smooth Loss: 1.5261 (at iter 86668), Time: 467.38s\n",
            "Iter: 95000/100000, Smooth Loss: 1.7032, Min Smooth Loss: 1.5261 (at iter 86668), Time: 467.80s\n",
            "Iter: 95100/100000, Smooth Loss: 1.7095, Min Smooth Loss: 1.5261 (at iter 86668), Time: 468.23s\n",
            "Iter: 95200/100000, Smooth Loss: 1.7093, Min Smooth Loss: 1.5261 (at iter 86668), Time: 468.64s\n",
            "Iter: 95300/100000, Smooth Loss: 1.7035, Min Smooth Loss: 1.5261 (at iter 86668), Time: 469.07s\n",
            "Iter: 95400/100000, Smooth Loss: 1.7032, Min Smooth Loss: 1.5261 (at iter 86668), Time: 469.51s\n",
            "Iter: 95500/100000, Smooth Loss: 1.7003, Min Smooth Loss: 1.5261 (at iter 86668), Time: 469.93s\n",
            "Iter: 95600/100000, Smooth Loss: 1.6918, Min Smooth Loss: 1.5261 (at iter 86668), Time: 470.36s\n",
            "Iter: 95700/100000, Smooth Loss: 1.6873, Min Smooth Loss: 1.5261 (at iter 86668), Time: 470.78s\n",
            "Iter: 95800/100000, Smooth Loss: 1.6777, Min Smooth Loss: 1.5261 (at iter 86668), Time: 471.20s\n",
            "Iter: 95900/100000, Smooth Loss: 1.6620, Min Smooth Loss: 1.5261 (at iter 86668), Time: 471.64s\n",
            "Iter: 96000/100000, Smooth Loss: 1.6555, Min Smooth Loss: 1.5261 (at iter 86668), Time: 472.07s\n",
            "Iter: 96100/100000, Smooth Loss: 1.6470, Min Smooth Loss: 1.5261 (at iter 86668), Time: 472.53s\n",
            "Iter: 96200/100000, Smooth Loss: 1.6472, Min Smooth Loss: 1.5261 (at iter 86668), Time: 472.95s\n",
            "Iter: 96300/100000, Smooth Loss: 1.6337, Min Smooth Loss: 1.5261 (at iter 86668), Time: 473.38s\n",
            "Iter: 96400/100000, Smooth Loss: 1.6307, Min Smooth Loss: 1.5261 (at iter 86668), Time: 473.80s\n",
            "Iter: 96500/100000, Smooth Loss: 1.6176, Min Smooth Loss: 1.5261 (at iter 86668), Time: 474.22s\n",
            "Iter: 96600/100000, Smooth Loss: 1.6133, Min Smooth Loss: 1.5261 (at iter 86668), Time: 474.66s\n",
            "Iter: 96700/100000, Smooth Loss: 1.6091, Min Smooth Loss: 1.5261 (at iter 86668), Time: 475.32s\n",
            "Iter: 96800/100000, Smooth Loss: 1.6007, Min Smooth Loss: 1.5261 (at iter 86668), Time: 475.97s\n",
            "Iter: 96900/100000, Smooth Loss: 1.5941, Min Smooth Loss: 1.5261 (at iter 86668), Time: 476.65s\n",
            "Iter: 97000/100000, Smooth Loss: 1.5882, Min Smooth Loss: 1.5261 (at iter 86668), Time: 477.28s\n",
            "Iter: 97100/100000, Smooth Loss: 1.5736, Min Smooth Loss: 1.5261 (at iter 86668), Time: 477.91s\n",
            "Iter: 97200/100000, Smooth Loss: 1.5682, Min Smooth Loss: 1.5261 (at iter 86668), Time: 478.54s\n",
            "Iter: 97300/100000, Smooth Loss: 1.5688, Min Smooth Loss: 1.5261 (at iter 86668), Time: 479.22s\n",
            "Iter: 97400/100000, Smooth Loss: 1.5823, Min Smooth Loss: 1.5261 (at iter 86668), Time: 479.64s\n",
            "Iter: 97500/100000, Smooth Loss: 1.5914, Min Smooth Loss: 1.5261 (at iter 86668), Time: 480.10s\n",
            "Iter: 97600/100000, Smooth Loss: 1.5875, Min Smooth Loss: 1.5261 (at iter 86668), Time: 480.54s\n",
            "Iter: 97700/100000, Smooth Loss: 1.5933, Min Smooth Loss: 1.5261 (at iter 86668), Time: 481.01s\n",
            "Iter: 97800/100000, Smooth Loss: 1.6064, Min Smooth Loss: 1.5261 (at iter 86668), Time: 481.47s\n",
            "Iter: 97900/100000, Smooth Loss: 1.6045, Min Smooth Loss: 1.5261 (at iter 86668), Time: 481.90s\n",
            "Iter: 98000/100000, Smooth Loss: 1.6099, Min Smooth Loss: 1.5261 (at iter 86668), Time: 482.35s\n",
            "Iter: 98100/100000, Smooth Loss: 1.6080, Min Smooth Loss: 1.5261 (at iter 86668), Time: 482.79s\n",
            "Iter: 98200/100000, Smooth Loss: 1.6059, Min Smooth Loss: 1.5261 (at iter 86668), Time: 483.28s\n",
            "Iter: 98300/100000, Smooth Loss: 1.6010, Min Smooth Loss: 1.5261 (at iter 86668), Time: 483.77s\n",
            "Iter: 98400/100000, Smooth Loss: 1.6135, Min Smooth Loss: 1.5261 (at iter 86668), Time: 484.24s\n",
            "Iter: 98500/100000, Smooth Loss: 1.6012, Min Smooth Loss: 1.5261 (at iter 86668), Time: 484.67s\n",
            "Iter: 98600/100000, Smooth Loss: 1.6095, Min Smooth Loss: 1.5261 (at iter 86668), Time: 485.13s\n",
            "Iter: 98700/100000, Smooth Loss: 1.6228, Min Smooth Loss: 1.5261 (at iter 86668), Time: 485.57s\n",
            "Iter: 98800/100000, Smooth Loss: 1.6256, Min Smooth Loss: 1.5261 (at iter 86668), Time: 486.01s\n",
            "Iter: 98900/100000, Smooth Loss: 1.6333, Min Smooth Loss: 1.5261 (at iter 86668), Time: 486.44s\n",
            "Iter: 99000/100000, Smooth Loss: 1.6398, Min Smooth Loss: 1.5261 (at iter 86668), Time: 486.89s\n",
            "Iter: 99100/100000, Smooth Loss: 1.6372, Min Smooth Loss: 1.5261 (at iter 86668), Time: 487.34s\n",
            "Iter: 99200/100000, Smooth Loss: 1.6534, Min Smooth Loss: 1.5261 (at iter 86668), Time: 487.77s\n",
            "Iter: 99300/100000, Smooth Loss: 1.6682, Min Smooth Loss: 1.5261 (at iter 86668), Time: 488.21s\n",
            "Iter: 99400/100000, Smooth Loss: 1.6766, Min Smooth Loss: 1.5261 (at iter 86668), Time: 488.66s\n",
            "Iter: 99500/100000, Smooth Loss: 1.6781, Min Smooth Loss: 1.5261 (at iter 86668), Time: 489.10s\n",
            "Iter: 99600/100000, Smooth Loss: 1.6821, Min Smooth Loss: 1.5261 (at iter 86668), Time: 489.72s\n",
            "Iter: 99700/100000, Smooth Loss: 1.6707, Min Smooth Loss: 1.5261 (at iter 86668), Time: 490.37s\n",
            "Iter: 99800/100000, Smooth Loss: 1.6687, Min Smooth Loss: 1.5261 (at iter 86668), Time: 491.03s\n",
            "Iter: 99900/100000, Smooth Loss: 1.6652, Min Smooth Loss: 1.5261 (at iter 86668), Time: 491.69s\n",
            "Iter: 100000/100000, Smooth Loss: 1.6559, Min Smooth Loss: 1.5261 (at iter 86668), Time: 492.33s\n",
            "--- Synthesized text at iter 100000 ---\n",
            "uring it's Awliton on onnoass ask Froghten and who's though, into yourselently malins who are-lew great lame to she savent of fate umoather with Birry hunder as the days of on'll and Goom, they he?  H\n",
            "---\n",
            "Training finished.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Synthesized Texts for Report (includes before training and every 10k updates) ---\n",
            "\n",
            "Iteration: 0\n",
            "fWd-jzu(,STrHS4y.vt\"lD7IxJ9TK\n",
            "3\n",
            "T_g4nK\"xeV!\n",
            "Wtb\t_E-nXg1-RTXjbc1KZSCy0}lY.V)x?J_\n",
            "Vf40QTMBJB9PIL\te!4nü•Y:pbüUT!3c•O I-z/-NVgGvY! PLN\"\"\"hZ.Ly(3H71}lhx3uüXuoCEdVH!\n",
            ",J /}Nx3Fc•ev!BtT12j0R24I,c1jSEZJ(BT,iF4\n",
            "\n",
            "Iteration: 10000\n",
            "\"\n",
            "\"I levere hers ao sly chat, ance, s arr armaar momberuus, burn nlf to slet serom afire comming the ffleeps, cemale surd rou maly.\n",
            "\"D ster lyoof yea larevery, the forthar al D\"\n",
            "UStmy gor core ond.  \"\n",
            "\n",
            "Iteration: 20000\n",
            "\n",
            "\"Yem of rage cor chernionr. . .\"\n",
            "\"Lowe he you ghuin. Ske beared. S\" horsed fizards klfted enf tull me forn leak, buck.\n",
            "\"At - they, about Tiscous was on milady got rigrtered, whinngring's ffriep!\"  sh\n",
            "\n",
            "Iteration: 30000\n",
            "op stle baring shrione, goted wat live of it? nuter wates. . ....\n",
            "Aly of reothee stoposs the coill scavisel chaupingen o show It heartef wereiny hadpes.\n",
            "\"What you to cand ans of the gamer the rount yo\n",
            "\n",
            "Iteration: 40000\n",
            "he gand, whithand-inrierlyed ave, Harry becand fliend, thit, thought thes nook it Snough lith and have lookend - they wan the thavegs.  Hf wint Lapble doing to gazing and sheter.  And he retereded, wh\n",
            "\n",
            "Iteration: 50000\n",
            "and coulding, and weat chought.  \"You?\"\n",
            "\"Judd.\n",
            "Ag in fill,\" said Ron,\" said Rentals the Mrmimbs Cus forthan inthe the sappover of that wizlot'v it them wairseasly wizard his righted their arment you p\n",
            "\n",
            "Iteration: 60000\n",
            "r student, Lodd the gottel, cone, veal outt back fred grinnishin; you they can't as And expite called once,\" said Maxable.\n",
            "\"With,\" said Heree Joyffet?\"  \"I was right, her Harry, like said steds takned\n",
            "\n",
            "Iteration: 70000\n",
            "'me, by her a didine conce to then the Back in fance bessurastred Valins.  Hen.  She welling in his, they let!\n",
            "\n",
            "Then whetly wintly - and Ron, I wisned he came in whon, I woube've in all now much he wi\n",
            "\n",
            "Iteration: 80000\n",
            " Veement he ham apound the would at are and wlowing his have who whe maded, and alonged threr's geing to said of his spotcer; quinting the ropestrrowing he was see way at an I fall, Krifther fows into\n",
            "\n",
            "Iteration: 90000\n",
            "ext outhentt to house eation.  And then?\"  said Harry kild, by intiheped the fiside.  Ix would fame plaruety wizardiment, at the dark know that would had been hop pothed with's like that one, looked i\n",
            "\n",
            "Iteration: 100000\n",
            "uring it's Awliton on onnoass ask Froghten and who's though, into yourselently malins who are-lew great lame to she savent of fate umoather with Birry hunder as the days of on'll and Goom, they he?  H\n",
            "\n",
            "--- Final Synthesized Text (1000 chars) from best model (achieved at iter 86668 with loss 1.5261) ---\n",
            "  Winked wizarns, pawill the poodered as who fattes tusion him, here deaped eymard them scare at her havi!  Kriuch\" Azard agancg.  Dumbledose.  Harry thes hesses, and - you, Harry's it bed his firsh his a Snarka Flee when murd-azmobiever him.  He pullon, very the horescus.  Mrsing clousself appone wan's had Harry was voice again, sheff inster, and as yas put the fecting at Potion will, sheve it, his was with actulded up nearad scaulddly ade bed, fryan lest his bandgail?\"\n",
            "\"Youn't one be ampe whiens in the leff.\"\n",
            "\"On reatiple spely he well oning the happear Mrs. Weacle detsing and and Mrs. Weasley befull parking onto that from the seizieverm, he's very be pelled. . . Of there, mumften.  Harry had not,  bached his sconting the Madiccull the fach, in a Pertho the hasper, and asfuc it wort had happently noves the costinaked.\"\n",
            "\"Weltis.  \"Unliking?\"\n",
            "\"What to still from stopersing bedorgg!\n",
            "\"I scartenly, shoop?\"\n",
            "\"He's drefuedly and the ound hast and vis not. . he scared in shitilld actake bleor\n",
            "\n",
            "Saved final 1000-char synthesized text (from best model) to final_synthesized_text_best_model.txt\n",
            "Saved loss plot to smooth_loss_plot.png (if history was recorded)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch # Make sure torch is imported for ComputeGradsWithTorch\n",
        "\n",
        "# --- PyTorch Gradient Checker Function (Integrated) ---\n",
        "def ComputeGradsWithTorch(X_np, Y_inds_np, h0_np, RNN_np):\n",
        "    # assumes X_np has size K x tau (d x tau), Y_inds_np has length tau, h0_np has size m x 1, etc\n",
        "    tau = X_np.shape[1] # seq_length\n",
        "    K_dim = X_np.shape[0]   # vocab_size (d)\n",
        "    m_dim = h0_np.shape[0] # hidden_size\n",
        "\n",
        "    # Convert numpy inputs to PyTorch tensors\n",
        "    Xt = torch.from_numpy(X_np).double()\n",
        "    ht_prev = torch.from_numpy(h0_np).double()\n",
        "\n",
        "    torch_network = {}\n",
        "    for kk in RNN_np.keys():\n",
        "        torch_network[kk] = torch.tensor(RNN_np[kk], requires_grad=True, dtype=torch.double)\n",
        "\n",
        "    apply_tanh = torch.nn.Tanh()\n",
        "    apply_softmax = torch.nn.Softmax(dim=0) # Softmax over K characters for each time step\n",
        "\n",
        "    Hs = torch.empty(m_dim, tau, dtype=torch.float64)\n",
        "    Os_torch = torch.empty(K_dim, tau, dtype=torch.float64)\n",
        "\n",
        "    for t in range(tau):\n",
        "        xt_curr = Xt[:, t:t+1]\n",
        "        at = torch.matmul(torch_network['W'], ht_prev) + \\\n",
        "             torch.matmul(torch_network['U'], xt_curr) + \\\n",
        "             torch_network['b']\n",
        "        ht_curr = apply_tanh(at)\n",
        "        Hs[:, t:t+1] = ht_curr\n",
        "        ot = torch.matmul(torch_network['V'], ht_curr) + torch_network['c']\n",
        "        Os_torch[:, t:t+1] = ot\n",
        "        ht_prev = ht_curr\n",
        "\n",
        "    P_torch = apply_softmax(Os_torch)\n",
        "\n",
        "    # Add epsilon for numerical stability before log\n",
        "    log_probs_correct_char = torch.log(P_torch[Y_inds_np, torch.arange(tau)] + 1e-9)\n",
        "    loss = -torch.mean(log_probs_correct_char)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    grads = {}\n",
        "    for kk in RNN_np.keys():\n",
        "        if torch_network[kk].grad is not None:\n",
        "            grads[kk] = torch_network[kk].grad.numpy()\n",
        "        else:\n",
        "            print(f\"Warning: No gradient for {kk} in PyTorch.\")\n",
        "            grads[kk] = np.zeros_like(RNN_np[kk])\n",
        "    return grads\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "m = 100  # Dimensionality of hidden state\n",
        "eta = 0.001  # Learning rate\n",
        "seq_length = 25  # Length of input sequences\n",
        "K = 0 # Vocabulary size, will be set after loading data\n",
        "rng_seed = 42 # For reproducibility\n",
        "rng = np.random.default_rng(rng_seed)\n",
        "\n",
        "# Adam parameters\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "def load_data(filename=\"goblet_book.txt\", use_dummy_data_if_not_found=True):\n",
        "    global K # Allow modification of global K\n",
        "    try:\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            book_data = f.read()\n",
        "        print(f\"Successfully loaded {filename}. Length: {len(book_data)} characters.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {filename} not found.\")\n",
        "        if use_dummy_data_if_not_found:\n",
        "            print(\"Using dummy data instead.\")\n",
        "            book_data = \"The quick brown fox jumps over the lazy dog. \" * 200 # A bit longer dummy data\n",
        "            book_data += \"Harry Potter and the Goblet of Fire. Hermione Granger. Ron Weasley. Albus Dumbledore. \" * 50\n",
        "            book_data += \"The Dark Lord will rise again. Voldemort. Magic is might. \" * 50\n",
        "            print(f\"Dummy data length: {len(book_data)} characters.\")\n",
        "        else:\n",
        "            return None, None, None, None\n",
        "\n",
        "    unique_chars = sorted(list(set(book_data)))\n",
        "    K_val = len(unique_chars)\n",
        "    K = K_val # Set global K here after it's determined\n",
        "    char_to_ind = {char: i for i, char in enumerate(unique_chars)}\n",
        "    ind_to_char = {i: char for i, char in enumerate(unique_chars)}\n",
        "    return book_data, char_to_ind, ind_to_char, K_val\n",
        "\n",
        "def one_hot_encode(index, vocab_size):\n",
        "    vec = np.zeros((vocab_size, 1))\n",
        "    vec[index, 0] = 1\n",
        "    return vec\n",
        "\n",
        "# --- RNN Parameters Initialization ---\n",
        "def initialize_rnn_parameters(m_dim, K_dim):\n",
        "    RNN = {}\n",
        "    RNN['b'] = np.zeros((m_dim, 1))\n",
        "    RNN['c'] = np.zeros((K_dim, 1))\n",
        "    RNN['U'] = (1/np.sqrt(K_dim)) * rng.standard_normal(size = (m_dim, K_dim)) # Assignment page 4: (1/np.sqrt(2*K_dim))\n",
        "    RNN['W'] = (1/np.sqrt(m_dim)) * rng.standard_normal(size = (m_dim, m_dim)) # Assignment page 4: (1/np.sqrt(2*m_dim))\n",
        "    RNN['V'] = (1/np.sqrt(m_dim)) * rng.standard_normal(size = (K_dim, m_dim)) # Assignment page 4: (1/np.sqrt(m_dim))\n",
        "\n",
        "    # Correcting to strictly follow page 4 of assignment for initialization\n",
        "    RNN['U'] = (1/np.sqrt(2*K_dim)) * rng.standard_normal(size = (m_dim, K_dim))\n",
        "    RNN['W'] = (1/np.sqrt(2*m_dim)) * rng.standard_normal(size = (m_dim, m_dim))\n",
        "    # RNN['V'] initialization was correct as per page 4 already.\n",
        "\n",
        "    return RNN\n",
        "\n",
        "# --- Softmax ---\n",
        "def softmax(x):\n",
        "    # Subtract max for numerical stability (prevents overflow with large exponents)\n",
        "    e_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=0, keepdims=True)\n",
        "\n",
        "# --- Forward Pass ---\n",
        "def forward_pass(X_seq, h_prev, RNN_params):\n",
        "    W, U, V, b, c = RNN_params['W'], RNN_params['U'], RNN_params['V'], RNN_params['b'], RNN_params['c']\n",
        "    seq_len = X_seq.shape[1]\n",
        "    m_dim = W.shape[0]\n",
        "    K_dim = V.shape[0]\n",
        "\n",
        "    a_vals = np.zeros((m_dim, seq_len))\n",
        "    h_vals = np.zeros((m_dim, seq_len + 1))\n",
        "    o_vals = np.zeros((K_dim, seq_len))\n",
        "    p_vals = np.zeros((K_dim, seq_len))\n",
        "\n",
        "    h_vals[:, 0:1] = h_prev\n",
        "\n",
        "    for t in range(seq_len):\n",
        "        xt = X_seq[:, t:t+1]\n",
        "        a_vals[:, t:t+1] = W @ h_vals[:, t:t+1] + U @ xt + b\n",
        "        h_vals[:, t+1:t+2] = np.tanh(a_vals[:, t:t+1])\n",
        "        o_vals[:, t:t+1] = V @ h_vals[:, t+1:t+2] + c\n",
        "        p_vals[:, t:t+1] = softmax(o_vals[:, t:t+1])\n",
        "\n",
        "    return a_vals, h_vals, o_vals, p_vals, h_vals[:, -1:]\n",
        "\n",
        "def calculate_loss(p_vals, Y_inds):\n",
        "    seq_len = Y_inds.shape[0]\n",
        "    loss = 0\n",
        "    for t in range(seq_len):\n",
        "        # Add small epsilon for log(0) stability\n",
        "        loss -= np.log(p_vals[Y_inds[t], t] + 1e-9)\n",
        "    return loss / seq_len # Average loss\n",
        "\n",
        "# --- Backward Pass ---\n",
        "def backward_pass(X_seq, Y_inds, RNN_params, a_vals, h_vals, o_vals, p_vals):\n",
        "    W, U, V, b, c = RNN_params['W'], RNN_params['U'], RNN_params['V'], RNN_params['b'], RNN_params['c']\n",
        "    seq_len = X_seq.shape[1]\n",
        "    m_dim = W.shape[0]\n",
        "\n",
        "    grads = {key: np.zeros_like(val) for key, val in RNN_params.items()}\n",
        "    grad_o = np.copy(p_vals)\n",
        "    for t in range(seq_len):\n",
        "        grad_o[Y_inds[t], t] -= 1\n",
        "    grad_o /= seq_len # Average gradient because loss was averaged\n",
        "\n",
        "    for t in range(seq_len): # This loop should be outside grad_h_next loop\n",
        "        grads['V'] += np.outer(grad_o[:, t], h_vals[:, t+1])\n",
        "        grads['c'] += grad_o[:, t:t+1]\n",
        "\n",
        "    grad_h_next = np.zeros((m_dim, 1))\n",
        "    for t in reversed(range(seq_len)):\n",
        "        grad_h = V.T @ grad_o[:, t:t+1] + grad_h_next\n",
        "        grad_a = grad_h * (1 - h_vals[:, t+1:t+2]**2)\n",
        "\n",
        "        grads['W'] += np.outer(grad_a, h_vals[:, t])\n",
        "        grads['U'] += np.outer(grad_a, X_seq[:, t])\n",
        "        grads['b'] += grad_a\n",
        "\n",
        "        grad_h_next = W.T @ grad_a\n",
        "\n",
        "    for key in grads: # Clip gradients\n",
        "        grads[key] = np.clip(grads[key], -5.0, 5.0)\n",
        "    return grads\n",
        "\n",
        "# --- Adam Optimizer ---\n",
        "def initialize_adam(params):\n",
        "    m_adam = {}\n",
        "    v_adam = {}\n",
        "    for key in params:\n",
        "        m_adam[key] = np.zeros_like(params[key])\n",
        "        v_adam[key] = np.zeros_like(params[key])\n",
        "    return m_adam, v_adam\n",
        "\n",
        "def update_parameters_adam(params, grads, m_adam, v_adam, t_adam, learning_rate, beta1_adam, beta2_adam, epsilon_adam):\n",
        "    for key in params:\n",
        "        m_adam[key] = beta1_adam * m_adam[key] + (1 - beta1_adam) * grads[key]\n",
        "        v_adam[key] = beta2_adam * v_adam[key] + (1 - beta2_adam) * (grads[key]**2)\n",
        "        m_hat = m_adam[key] / (1 - beta1_adam**t_adam)\n",
        "        v_hat = v_adam[key] / (1 - beta2_adam**t_adam)\n",
        "        params[key] -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon_adam)\n",
        "    return params, m_adam, v_adam\n",
        "\n",
        "# --- Text Synthesis ---\n",
        "def synthesize_text(RNN_params, h0_syn, x0_ind_syn, n_chars_syn, K_dim_syn, ind_to_char_map, char_to_ind_map):\n",
        "    W, U, V, b, c = RNN_params['W'], RNN_params['U'], RNN_params['V'], RNN_params['b'], RNN_params['c']\n",
        "    x_curr = one_hot_encode(x0_ind_syn, K_dim_syn)\n",
        "    h_curr = np.copy(h0_syn)\n",
        "    generated_indices = []\n",
        "\n",
        "    for _ in range(n_chars_syn):\n",
        "        a = W @ h_curr + U @ x_curr + b\n",
        "        h_curr = np.tanh(a)\n",
        "        o = V @ h_curr + c\n",
        "        p = softmax(o)\n",
        "\n",
        "        cp = np.cumsum(p.ravel())\n",
        "        rand_val = rng.uniform()\n",
        "        ix = np.argmax(cp - rand_val > 0)\n",
        "        # Safety check, though with correct cumsum and rand_val in [0,1), ix should be valid\n",
        "        if ix >= K_dim_syn: ix = K_dim_syn -1\n",
        "\n",
        "        generated_indices.append(ix)\n",
        "        x_curr = one_hot_encode(ix, K_dim_syn)\n",
        "\n",
        "    return \"\".join([ind_to_char_map[ix_char] for ix_char in generated_indices])\n",
        "\n",
        "\n",
        "import copy # Needed for deepcopying the best model parameters\n",
        "\n",
        "# ... (之前的 ComputeGradsWithTorch, Hyperparameters, Data Loading, etc. 保持不变) ...\n",
        "# ... (initialize_rnn_parameters, softmax, forward_pass, calculate_loss, backward_pass, initialize_adam, update_parameters_adam, synthesize_text 保持不变) ...\n",
        "\n",
        "# --- Gradient Checking ---\n",
        "# 修改点 1: 将 seq_len_gc 的默认值和调用时的值改为25\n",
        "def check_gradients(char_to_ind_map_check, K_dim_check, m_dim_check=10, seq_len_gc=25): # MODIFIED: seq_len_gc default to 25\n",
        "    print(\"\\n--- Starting Gradient Check ---\")\n",
        "    # ... (内部逻辑不变，除了 seq_len_gc 会使用新的值) ...\n",
        "\n",
        "    available_chars = list(char_to_ind_map_check.keys())\n",
        "    if len(available_chars) < seq_len_gc + 1:\n",
        "        print(f\"Warning: Not enough unique characters ({len(available_chars)}) for gradient check sequence length ({seq_len_gc}+1).\")\n",
        "        if len(available_chars) < 2:\n",
        "            print(\"Error: Need at least 2 unique characters for gradient checking. Skipping.\")\n",
        "            return False\n",
        "        # If still not enough after trying to use all, it might fail or use a shorter seq.\n",
        "        # For strictness, we might want to ensure seq_len_gc can actually be met.\n",
        "        # However, the assignment implies K is from the book, so it should be large enough.\n",
        "        # If K is small from dummy data, this part might need more robust handling or a skip.\n",
        "        # For now, assume K from actual book data is sufficient for seq_len_gc=25.\n",
        "        if len(available_chars) <= seq_len_gc : # if we only have exactly seq_len_gc chars, we can't make Y\n",
        "             print(f\"Error: Not enough unique characters to form X and Y for seq_len_gc={seq_len_gc}. Skipping gradient check.\")\n",
        "             return False # Cannot proceed if not enough data for X and Y\n",
        "\n",
        "\n",
        "    dummy_chars_X = available_chars[:seq_len_gc]\n",
        "    dummy_chars_Y = available_chars[1:seq_len_gc+1]\n",
        "\n",
        "    X_check = np.zeros((K_dim_check, seq_len_gc))\n",
        "    Y_inds_check = np.zeros(seq_len_gc, dtype=int)\n",
        "\n",
        "    for t, char_val in enumerate(dummy_chars_X):\n",
        "        X_check[:, t] = one_hot_encode(char_to_ind_map_check[char_val], K_dim_check)[:,0]\n",
        "    for t, char_val in enumerate(dummy_chars_Y):\n",
        "        Y_inds_check[t] = char_to_ind_map_check[char_val]\n",
        "\n",
        "    h0_check = np.zeros((m_dim_check, 1))\n",
        "    RNN_check = initialize_rnn_parameters(m_dim_check, K_dim_check)\n",
        "\n",
        "    a_vals_c, h_vals_c, o_vals_c, p_vals_c, _ = forward_pass(X_check, h0_check, RNN_check)\n",
        "    grads_analytical = backward_pass(X_check, Y_inds_check, RNN_check, a_vals_c, h_vals_c, o_vals_c, p_vals_c)\n",
        "    grads_pytorch = ComputeGradsWithTorch(X_check, Y_inds_check, h0_check, RNN_check)\n",
        "\n",
        "    print(\"Comparing gradients (Max absolute difference & Avg Relative Error):\")\n",
        "    all_passed = True\n",
        "    for key in RNN_check:\n",
        "        abs_diff = np.abs(grads_analytical[key] - grads_pytorch[key])\n",
        "        max_abs_diff_val = np.max(abs_diff)\n",
        "        rel_error_num = np.sum(abs_diff)\n",
        "        rel_error_den = np.sum(np.abs(grads_analytical[key]) + np.abs(grads_pytorch[key])) + 1e-9\n",
        "        avg_rel_error = rel_error_num / rel_error_den\n",
        "\n",
        "        print(f\"Param: {key}, Max Abs Diff: {max_abs_diff_val:.2e}, Avg Rel Error: {avg_rel_error:.2e}\")\n",
        "        if max_abs_diff_val > 1e-6 or avg_rel_error > 1e-5 :\n",
        "            print(f\"  WARNING: Gradients for {key} might differ significantly.\")\n",
        "            all_passed = False\n",
        "\n",
        "    if all_passed:\n",
        "        print(\"Gradient check PASSED (within tolerance).\")\n",
        "    else:\n",
        "        print(\"Gradient check FAILED for some parameters.\")\n",
        "    print(\"--- Gradient Check Finished ---\")\n",
        "    return all_passed\n",
        "\n",
        "\n",
        "# --- Main Training Loop ---\n",
        "def train_rnn():\n",
        "    global K\n",
        "    book_file = \"goblet_book.txt\"\n",
        "    book_data, char_to_ind, ind_to_char, K_val_loaded = load_data(book_file, use_dummy_data_if_not_found=True)\n",
        "\n",
        "    if K_val_loaded is None:\n",
        "        print(f\"Could not load data from {book_file} and dummy data was not used. Exiting.\")\n",
        "        return\n",
        "    K = K_val_loaded\n",
        "\n",
        "    print(f\"Vocabulary size (K): {K}\")\n",
        "    print(f\"Hidden state size (m): {m}\")\n",
        "    print(f\"Sequence length: {seq_length}\")\n",
        "\n",
        "    # 修改点 1 (调用): Gradient Check first (m=10, seq_length=25 (as per assignment page 5, last paragraph))\n",
        "    if not check_gradients(char_to_ind, K, m_dim_check=10, seq_len_gc=25): # MODIFIED: seq_len_gc to 25\n",
        "         print(\"Gradient check issues. Please review before extensive training.\")\n",
        "         return\n",
        "\n",
        "    RNN_params = initialize_rnn_parameters(m, K)\n",
        "    adam_m_params, adam_v_params = initialize_adam(RNN_params)\n",
        "\n",
        "    e_ptr = 0\n",
        "    h_prev_state = np.zeros((m, 1))\n",
        "    smooth_loss_val = -np.log(1.0/K) * seq_length\n",
        "    adam_iter_count = 0\n",
        "\n",
        "    # 修改点 2: 初始化用于保存最佳模型和最低损失的变量\n",
        "    min_smooth_loss = float('inf')\n",
        "    best_RNN_params = None\n",
        "    iteration_of_best_loss = 0\n",
        "\n",
        "    num_epochs_train = 3\n",
        "    iterations_per_epoch = len(book_data) // seq_length\n",
        "    total_iterations_train = 100000\n",
        "\n",
        "    losses_history = []\n",
        "    iteration_points_history = []\n",
        "\n",
        "    print(f\"\\nStarting training for {total_iterations_train} iterations...\")\n",
        "    start_time_total_train = time.time()\n",
        "\n",
        "    synthesized_texts_for_report = []\n",
        "    if K > 0 and len(char_to_ind)>0:\n",
        "        first_char_of_book = book_data[0] if book_data else list(char_to_ind.keys())[0]\n",
        "        first_char_idx_init = char_to_ind.get(first_char_of_book, 0)\n",
        "        initial_h0_for_synth = np.zeros((m,1))\n",
        "        text_before_train = synthesize_text(RNN_params, initial_h0_for_synth, first_char_idx_init, 200, K, ind_to_char, char_to_ind)\n",
        "        synthesized_texts_for_report.append((0, text_before_train))\n",
        "        print(f\"Iter: 0 (Before training), Synthesized:\\n{text_before_train}\\n---\")\n",
        "\n",
        "\n",
        "    for iteration in range(1, total_iterations_train + 1):\n",
        "        adam_iter_count += 1\n",
        "        if e_ptr + seq_length + 1 >= len(book_data):\n",
        "            e_ptr = 0\n",
        "            h_prev_state = np.zeros((m, 1))\n",
        "            # current_epoch_approx = iteration // iterations_per_epoch if iterations_per_epoch > 0 else 0\n",
        "            # print(f\"--- Completed Epoch (approx) {current_epoch_approx} ---\")\n",
        "\n",
        "        X_chars_batch = book_data[e_ptr : e_ptr + seq_length]\n",
        "        Y_chars_batch = book_data[e_ptr + 1 : e_ptr + seq_length + 1]\n",
        "\n",
        "        X_one_hot_batch = np.zeros((K, seq_length))\n",
        "        Y_inds_batch = np.zeros(seq_length, dtype=int)\n",
        "\n",
        "        for t_idx, char_val in enumerate(X_chars_batch):\n",
        "            X_one_hot_batch[:, t_idx] = one_hot_encode(char_to_ind[char_val], K)[:,0]\n",
        "        for t_idx, char_val in enumerate(Y_chars_batch):\n",
        "            Y_inds_batch[t_idx] = char_to_ind[char_val]\n",
        "\n",
        "        a_vals_fwd, h_vals_fwd, o_vals_fwd, p_vals_fwd, h_next_state = forward_pass(X_one_hot_batch, h_prev_state, RNN_params)\n",
        "        current_loss_val = calculate_loss(p_vals_fwd, Y_inds_batch)\n",
        "        grads_bwd = backward_pass(X_one_hot_batch, Y_inds_batch, RNN_params, a_vals_fwd, h_vals_fwd, o_vals_fwd, p_vals_fwd)\n",
        "\n",
        "        RNN_params, adam_m_params, adam_v_params = update_parameters_adam(\n",
        "            RNN_params, grads_bwd, adam_m_params, adam_v_params, adam_iter_count, eta, beta1, beta2, epsilon\n",
        "        )\n",
        "\n",
        "        smooth_loss_val = 0.999 * smooth_loss_val + 0.001 * current_loss_val\n",
        "        h_prev_state = h_next_state\n",
        "\n",
        "        # 修改点 2: 跟踪并保存最佳模型\n",
        "        if smooth_loss_val < min_smooth_loss:\n",
        "            min_smooth_loss = smooth_loss_val\n",
        "            best_RNN_params = copy.deepcopy(RNN_params) # Use deepcopy for dictionaries of numpy arrays\n",
        "            iteration_of_best_loss = iteration\n",
        "            print(f\"  ** New min smooth loss: {min_smooth_loss:.4f} at iter {iteration} **\")\n",
        "\n",
        "\n",
        "        if iteration % 100 == 0:\n",
        "            elapsed_time_iter = time.time() - start_time_total_train\n",
        "            print(f\"Iter: {iteration}/{total_iterations_train}, Smooth Loss: {smooth_loss_val:.4f}, Min Smooth Loss: {min_smooth_loss:.4f} (at iter {iteration_of_best_loss}), Time: {elapsed_time_iter:.2f}s\")\n",
        "            losses_history.append(smooth_loss_val)\n",
        "            iteration_points_history.append(iteration)\n",
        "\n",
        "        if iteration % 10000 == 0:\n",
        "            print(f\"--- Synthesized text at iter {iteration} ---\")\n",
        "            first_char_current_seq_idx = char_to_ind[X_chars_batch[0]]\n",
        "            synthesized_iter = synthesize_text(RNN_params, h_prev_state, first_char_current_seq_idx, 200, K, ind_to_char, char_to_ind)\n",
        "            print(synthesized_iter)\n",
        "            synthesized_texts_for_report.append((iteration, synthesized_iter))\n",
        "            print(\"---\")\n",
        "\n",
        "        e_ptr += seq_length\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    plt.figure(figsize=(12,7))\n",
        "    if iteration_points_history and losses_history:\n",
        "        plt.plot(iteration_points_history, losses_history, label=\"Smooth Loss\")\n",
        "        # Optionally, mark the point of minimum smooth loss\n",
        "        if iteration_of_best_loss in iteration_points_history:\n",
        "             idx_best = iteration_points_history.index(iteration_of_best_loss)\n",
        "             plt.scatter([iteration_of_best_loss], [losses_history[idx_best]], color='red', s=100, label=f'Min Loss ({min_smooth_loss:.4f})', zorder=5)\n",
        "        elif best_RNN_params is not None : # If best iter was not exactly on a 100th step\n",
        "             plt.axhline(y=min_smooth_loss, color='r', linestyle='--', label=f'Min Loss ({min_smooth_loss:.4f}) at iter {iteration_of_best_loss}')\n",
        "\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Smooth Loss\")\n",
        "        plt.title(\"Smooth Loss during RNN Training\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(\"smooth_loss_plot.png\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No loss history to plot.\")\n",
        "\n",
        "    print(\"\\n--- Synthesized Texts for Report (includes before training and every 10k updates) ---\")\n",
        "    for iter_num, text_sample in synthesized_texts_for_report:\n",
        "        print(f\"\\nIteration: {iter_num}\\n{text_sample}\")\n",
        "\n",
        "    # 修改点 2: 使用最佳模型进行最终文本合成\n",
        "    print(f\"\\n--- Final Synthesized Text (1000 chars) from best model (achieved at iter {iteration_of_best_loss} with loss {min_smooth_loss:.4f}) ---\")\n",
        "    if best_RNN_params is not None and K > 0 and len(char_to_ind)>0:\n",
        "        start_char_final_synth = '.' if '.' in char_to_ind else list(char_to_ind.keys())[0]\n",
        "        start_idx_final_synth = char_to_ind.get(start_char_final_synth,0)\n",
        "        # For final synthesis, it's good practice to start with a fresh h0, or the h_prev from when best_RNN_params was saved.\n",
        "        # Here, we'll use a fresh h0 for consistency, as h_prev from that specific iteration wasn't saved.\n",
        "        final_h0_for_best_model = np.zeros((m,1))\n",
        "        final_long_text = synthesize_text(best_RNN_params, final_h0_for_best_model, start_idx_final_synth, 1000, K, ind_to_char, char_to_ind)\n",
        "        print(final_long_text)\n",
        "        with open(\"final_synthesized_text_best_model.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(final_long_text)\n",
        "        print(f\"\\nSaved final 1000-char synthesized text (from best model) to final_synthesized_text_best_model.txt\")\n",
        "    elif K == 0 or len(char_to_ind) == 0:\n",
        "        print(\"Cannot synthesize final text, K or char_to_ind is not valid.\")\n",
        "    else:\n",
        "        print(\"No best model was saved (perhaps training was too short or an error occurred). Synthesizing with final model instead.\")\n",
        "        # Fallback to final model if best_RNN_params is None (shouldn't happen if training runs)\n",
        "        if K > 0 and len(char_to_ind)>0:\n",
        "            start_char_final_synth = '.' if '.' in char_to_ind else list(char_to_ind.keys())[0]\n",
        "            start_idx_final_synth = char_to_ind.get(start_char_final_synth,0)\n",
        "            final_h0_for_final_model = np.zeros((m,1))\n",
        "            final_long_text_fallback = synthesize_text(RNN_params, final_h0_for_final_model, start_idx_final_synth, 1000, K, ind_to_char, char_to_ind)\n",
        "            print(final_long_text_fallback)\n",
        "            with open(\"final_synthesized_text_final_model.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(final_long_text_fallback)\n",
        "            print(f\"\\nSaved final 1000-char synthesized text (from final model as fallback) to final_synthesized_text_final_model.txt\")\n",
        "\n",
        "\n",
        "    print(\"Saved loss plot to smooth_loss_plot.png (if history was recorded)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ... (Colab instructions remain the same) ...\n",
        "    train_rnn()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-XahxAaLXOyzVxZ5n6ZOqxp15BziyDU2",
      "authorship_tag": "ABX9TyPdtwsmx1InbZppqRDF+tLl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}