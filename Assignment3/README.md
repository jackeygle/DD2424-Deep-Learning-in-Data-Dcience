# Assignment 3: Recurrent Neural Networks & Text Generation

## ğŸ“– Overview
This assignment explores sequence modeling using Recurrent Neural Networks (RNNs). The main focus is on implementing vanilla RNNs and LSTMs from scratch for character-level text generation, providing deep insights into sequential data processing and the challenges of training recurrent architectures.

## ğŸ¯ Learning Objectives
- Understand recurrent neural network architectures
- Implement vanilla RNN and LSTM from scratch
- Master sequence-to-sequence modeling
- Apply RNNs to character-level text generation
- Address vanishing gradient problems in RNNs

## ğŸ“Š Dataset
- **Type:** Text corpus (e.g., classic literature, poems, or custom text)
- **Task:** Character-level language modeling
- **Input:** Sequence of characters
- **Output:** Next character prediction
- **Sequence Length:** Variable (typically 25-100 characters)

## ğŸ§  Network Architecture

### Vanilla RNN Implementation
```
Input: x_t (one-hot encoded character)
Hidden: h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)
Output: y_t = W_hy * h_t + b_y
```

### LSTM Implementation (Bonus)
```
Forget Gate: f_t = Ïƒ(W_f * [h_{t-1}, x_t] + b_f)
Input Gate: i_t = Ïƒ(W_i * [h_{t-1}, x_t] + b_i)
Candidate: CÌƒ_t = tanh(W_C * [h_{t-1}, x_t] + b_C)
Cell State: C_t = f_t * C_{t-1} + i_t * CÌƒ_t
Output Gate: o_t = Ïƒ(W_o * [h_{t-1}, x_t] + b_o)
Hidden: h_t = o_t * tanh(C_t)
```

## ğŸ“ˆ Key Results
| Model | Perplexity | Training Time | Generated Quality |
|-------|------------|---------------|-------------------|
| **Vanilla RNN** | ~XX | ~XX min | Coherent short sequences |
| **LSTM** | ~XX | ~XX min | Longer coherent text |

## ğŸ” Implementation Details

### Vanilla RNN Forward Pass
```python
def rnn_forward(X, h_prev, Wxh, Whh, bh):
    """
    Forward pass for vanilla RNN
    X: input sequence (seq_len, vocab_size)
    h_prev: previous hidden state
    """
    h = {}
    h[-1] = np.copy(h_prev)
    
    for t in range(len(X)):
        h[t] = np.tanh(Wxh @ X[t] + Whh @ h[t-1] + bh)
    
    return h
```

### Backpropagation Through Time (BPTT)
```python
def rnn_backward(X, Y, h, Wyh, by):
    """
    Backward pass with BPTT
    Compute gradients for all time steps
    """
    dWxh = np.zeros_like(Wxh)
    dWhh = np.zeros_like(Whh)
    dbh = np.zeros_like(bh)
    dh_next = np.zeros_like(h[0])
    
    for t in reversed(range(len(X))):
        # Output layer gradients
        dy = np.copy(Y[t])
        dy[targets[t]] -= 1
        
        # Hidden layer gradients
        dh = Wyh.T @ dy + dh_next
        dh_raw = (1 - h[t] * h[t]) * dh  # tanh derivative
        
        # Parameter gradients
        dWxh += dh_raw @ X[t].T
        dWhh += dh_raw @ h[t-1].T
        dbh += dh_raw
        
        dh_next = Whh.T @ dh_raw
    
    return dWxh, dWhh, dbh
```

### Text Generation
```python
def generate_text(seed_char, length, temperature=1.0):
    """
    Generate text using trained RNN
    """
    x = char_to_ix[seed_char]
    h = np.zeros((hidden_size, 1))
    generated = []
    
    for t in range(length):
        h = np.tanh(Wxh @ x + Whh @ h + bh)
        y = Wyh @ h + by
        
        # Apply temperature for sampling diversity
        p = np.exp(y / temperature) / np.sum(np.exp(y / temperature))
        ix = np.random.choice(range(vocab_size), p=p.ravel())
        
        x = np.zeros((vocab_size, 1))
        x[ix] = 1
        generated.append(ix_to_char[ix])
    
    return ''.join(generated)
```

## ğŸ“ Files Description

### assignment3bonus.ipynb
**Comprehensive RNN implementation including:**

1. **Data Preprocessing**
   - Text loading and character encoding
   - Sequence generation for training
   - Vocabulary building

2. **Model Implementation**
   - Vanilla RNN from scratch
   - LSTM implementation (bonus)
   - Forward propagation
   - Backpropagation through time

3. **Training Procedures**
   - Mini-batch training
   - Gradient clipping
   - Learning rate scheduling
   - Loss monitoring

4. **Text Generation**
   - Sampling strategies
   - Temperature control
   - Seed-based generation
   - Quality evaluation

5. **Advanced Experiments**
   - Different activation functions
   - Various sequence lengths
   - Architecture comparisons
   - Hyperparameter sensitivity

## ğŸš€ How to Run

### Prerequisites
```bash
pip install numpy matplotlib jupyter
```

### Execution
```bash
# Navigate to Assignment3 directory
cd Assignment3/

# Launch Jupyter notebook
jupyter notebook assignment3bonus.ipynb
```

### Expected Runtime
- Basic RNN training: ~20-30 minutes
- LSTM experiments: ~45-60 minutes
- Text generation: ~1-2 minutes

## ğŸ“Š Hyperparameters Explored

| Parameter | Values Tested | Best Value |
|-----------|---------------|------------|
| Hidden Size | [50, 100, 200] | 100 |
| Sequence Length | [25, 50, 100] | 50 |
| Learning Rate | [0.1, 0.01, 0.001] | 0.01 |
| Gradient Clip | [1, 5, 10] | 5 |

## ğŸ“ Key Insights Learned

### RNN Fundamentals
- **Sequential Processing:** Understanding how RNNs maintain memory through hidden states
- **Vanishing Gradients:** Why vanilla RNNs struggle with long sequences
- **BPTT Complexity:** Computational challenges of training recurrent networks

### Training Challenges
- **Gradient Explosion:** Need for gradient clipping in RNN training
- **Memory Limitations:** Processing long sequences efficiently
- **Convergence Issues:** RNNs can be sensitive to initialization and learning rates

### Text Generation Insights
- **Temperature Sampling:** Balancing creativity vs. coherence in generation
- **Sequence Dependencies:** How RNNs capture short and long-term patterns
- **Quality Metrics:** Evaluating generated text beyond perplexity

## ğŸ› Common Issues & Solutions

### Issue 1: Vanishing Gradients
**Problem:** RNN fails to learn long-term dependencies
**Solutions:**
- Use LSTM/GRU architectures
- Implement gradient clipping
- Better initialization strategies

### Issue 2: Exploding Gradients
**Problem:** Gradients become extremely large during training
**Solutions:**
- Gradient clipping (norm-based)
- Lower learning rates
- Proper weight initialization

### Issue 3: Poor Text Quality
**Problem:** Generated text is incoherent or repetitive
**Solutions:**
- Increase model capacity (hidden size)
- Train for more epochs
- Experiment with temperature sampling

## ğŸ”¬ Advanced Experiments

### Architecture Comparisons
```python
# Compare different RNN variants
models = {
    'Vanilla RNN': VanillaRNN(hidden_size=100),
    'LSTM': LSTM(hidden_size=100),
    'GRU': GRU(hidden_size=100)  # if implemented
}

for name, model in models.items():
    train_loss = train_model(model, data)
    generated_text = generate_text(model, seed="The")
    print(f"{name}: Loss={train_loss:.3f}")
```

### Sampling Strategies
```python
# Different text generation approaches
def nucleus_sampling(probabilities, p=0.9):
    """Top-p sampling for better diversity"""
    sorted_probs = np.sort(probabilities)[::-1]
    cumsum_probs = np.cumsum(sorted_probs)
    cutoff = sorted_probs[np.argmax(cumsum_probs >= p)]
    probabilities[probabilities < cutoff] = 0
    return probabilities / np.sum(probabilities)
```

## ğŸ“š Theory References

### Core Concepts
1. **Recurrent Neural Networks**
   - Hidden state dynamics
   - Sequence modeling theory

2. **Backpropagation Through Time**
   - Gradient computation in sequential models
   - Computational complexity analysis

3. **LSTM Architecture**
   - Gate mechanisms
   - Cell state vs. hidden state

### Recommended Reading
- "Understanding LSTM Networks" by Christopher Olah
- Deep Learning Book, Chapter 10 (Sequence Modeling)
- "The Unreasonable Effectiveness of RNNs" by Andrej Karpathy

## ğŸ† Assessment Criteria

### Implementation Quality (40%)
- Correct RNN forward pass
- Proper BPTT implementation
- Code efficiency and clarity

### Experimental Design (35%)
- Systematic hyperparameter exploration
- Meaningful architecture comparisons
- Creative text generation experiments

### Analysis & Discussion (25%)
- Understanding of RNN limitations
- Insightful interpretation of results
- Connection to theoretical concepts

## ğŸ¨ Example Generated Text

### After 100 epochs (Vanilla RNN):
```
"The king was in the castle when the..."
```

### After 200 epochs (LSTM):
```
"The king was in the castle when the great war began. 
His knights gathered in the hall, their armor gleaming..."
```

---

**Note:** This assignment demonstrates the power and limitations of recurrent architectures, setting the foundation for understanding more advanced sequence models like Transformers in modern NLP. 